diff --git a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Adjacency.java b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Adjacency.java
index 37edf2a..7cce8f3 100644
--- a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Adjacency.java
+++ b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Adjacency.java
@@ -52,7 +52,14 @@
   private List<String> children; // required
   private AdjacencyType adjacencyType; // required
 
-  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
+    public void addToAdjacencyList(Graph graph) {
+        if (graph.getAdjacencyList() == null) {
+        graph.setAdjacencyList(new ArrayList<Adjacency>());
+      }
+      graph.getAdjacencyList().add(this);
+    }
+
+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
   public enum _Fields implements org.apache.thrift.TFieldIdEnum {
     NODE((short)1, "node"),
     CHILDREN((short)2, "children"),
diff --git a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Graph.java b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Graph.java
index 0b454df..1ff6e4c 100644
--- a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Graph.java
+++ b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Graph.java
@@ -12,26 +12,16 @@
 
 import org.apache.thrift.scheme.TupleScheme;
 import org.apache.thrift.protocol.TTupleProtocol;
-import org.apache.thrift.protocol.TProtocolException;
-import org.apache.thrift.EncodingUtils;
-import org.apache.thrift.TException;
-import org.apache.thrift.async.AsyncMethodCallback;
-import org.apache.thrift.server.AbstractNonblockingServer.*;
+
 import java.util.List;
 import java.util.ArrayList;
 import java.util.Map;
 import java.util.HashMap;
 import java.util.EnumMap;
-import java.util.Set;
-import java.util.HashSet;
 import java.util.EnumSet;
 import java.util.Collections;
 import java.util.BitSet;
-import java.nio.ByteBuffer;
-import java.util.Arrays;
 import javax.annotation.Generated;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
 @Generated(value = "Autogenerated by Thrift Compiler (0.9.3)")
@@ -258,14 +248,7 @@ public int getAdjacencyListSize() {
     return (this.adjacencyList == null) ? null : this.adjacencyList.iterator();
   }
 
-  public void addToAdjacencyList(Adjacency elem) {
-    if (this.adjacencyList == null) {
-      this.adjacencyList = new ArrayList<Adjacency>();
-    }
-    this.adjacencyList.add(elem);
-  }
-
-  public List<Adjacency> getAdjacencyList() {
+    public List<Adjacency> getAdjacencyList() {
     return this.adjacencyList;
   }
 
diff --git a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Operator.java b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Operator.java
index 05dbb2d..2d065a1 100644
--- a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Operator.java
+++ b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Operator.java
@@ -58,7 +58,14 @@
   private boolean done; // required
   private boolean started; // required
 
-  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
+    public void addToOperatorList(Task task) {
+        if (task.getOperatorList() == null) {
+        task.setOperatorList(new ArrayList<Operator>());
+      }
+      task.getOperatorList().add(this);
+    }
+
+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
   public enum _Fields implements org.apache.thrift.TFieldIdEnum {
     OPERATOR_ID((short)1, "operatorId"),
     /**
diff --git a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Query.java b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Query.java
index de553e1..ec2e990 100644
--- a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Query.java
+++ b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Query.java
@@ -12,26 +12,17 @@
 
 import org.apache.thrift.scheme.TupleScheme;
 import org.apache.thrift.protocol.TTupleProtocol;
-import org.apache.thrift.protocol.TProtocolException;
 import org.apache.thrift.EncodingUtils;
-import org.apache.thrift.TException;
-import org.apache.thrift.async.AsyncMethodCallback;
-import org.apache.thrift.server.AbstractNonblockingServer.*;
+
 import java.util.List;
 import java.util.ArrayList;
 import java.util.Map;
 import java.util.HashMap;
 import java.util.EnumMap;
-import java.util.Set;
-import java.util.HashSet;
 import java.util.EnumSet;
 import java.util.Collections;
 import java.util.BitSet;
-import java.nio.ByteBuffer;
-import java.util.Arrays;
 import javax.annotation.Generated;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
 @Generated(value = "Autogenerated by Thrift Compiler (0.9.3)")
@@ -395,14 +386,7 @@ public int getStageListSize() {
     return (this.stageList == null) ? null : this.stageList.iterator();
   }
 
-  public void addToStageList(Stage elem) {
-    if (this.stageList == null) {
-      this.stageList = new ArrayList<Stage>();
-    }
-    this.stageList.add(elem);
-  }
-
-  public List<Stage> getStageList() {
+    public List<Stage> getStageList() {
     return this.stageList;
   }
 
diff --git a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Stage.java b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Stage.java
index 7f86eeb..abbdf60 100644
--- a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Stage.java
+++ b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Stage.java
@@ -60,7 +60,14 @@
   private boolean done; // required
   private boolean started; // required
 
-  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
+    public void addToStageList(Query query) {
+        if (query.getStageList() == null) {
+        query.setStageList(new ArrayList<Stage>());
+      }
+      query.getStageList().add(this);
+    }
+
+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
   public enum _Fields implements org.apache.thrift.TFieldIdEnum {
     STAGE_ID((short)1, "stageId"),
     /**
diff --git a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Task.java b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Task.java
index f978e42..fe48b85 100644
--- a/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Task.java
+++ b/ql/src/gen/thrift/gen-javabean/org/apache/hadoop/hive/ql/plan/api/Task.java
@@ -12,26 +12,17 @@
 
 import org.apache.thrift.scheme.TupleScheme;
 import org.apache.thrift.protocol.TTupleProtocol;
-import org.apache.thrift.protocol.TProtocolException;
 import org.apache.thrift.EncodingUtils;
-import org.apache.thrift.TException;
-import org.apache.thrift.async.AsyncMethodCallback;
-import org.apache.thrift.server.AbstractNonblockingServer.*;
+
 import java.util.List;
 import java.util.ArrayList;
 import java.util.Map;
 import java.util.HashMap;
 import java.util.EnumMap;
-import java.util.Set;
-import java.util.HashSet;
 import java.util.EnumSet;
 import java.util.Collections;
 import java.util.BitSet;
-import java.nio.ByteBuffer;
-import java.util.Arrays;
 import javax.annotation.Generated;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 @SuppressWarnings({"cast", "rawtypes", "serial", "unchecked"})
 @Generated(value = "Autogenerated by Thrift Compiler (0.9.3)")
@@ -404,14 +395,7 @@ public int getOperatorListSize() {
     return (this.operatorList == null) ? null : this.operatorList.iterator();
   }
 
-  public void addToOperatorList(Operator elem) {
-    if (this.operatorList == null) {
-      this.operatorList = new ArrayList<Operator>();
-    }
-    this.operatorList.add(elem);
-  }
-
-  public List<Operator> getOperatorList() {
+    public List<Operator> getOperatorList() {
     return this.operatorList;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java b/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java
index 7636019..85fea42 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/QueryPlan.java
@@ -35,7 +35,6 @@
 import java.util.UUID;
 import java.util.concurrent.ConcurrentHashMap;
 
-import org.apache.curator.shaded.com.google.common.collect.Lists;
 import org.apache.hadoop.hive.metastore.api.Schema;
 import org.apache.hadoop.hive.ql.exec.ConditionalTask;
 import org.apache.hadoop.hive.ql.exec.ExplainTask;
@@ -238,7 +237,7 @@ private void populateOperatorGraph(
         new org.apache.hadoop.hive.ql.plan.api.Operator();
       operator.setOperatorId(op.getOperatorId());
       operator.setOperatorType(op.getType());
-      task.addToOperatorList(operator);
+      operator.addToOperatorList(task);
       // done processing the operator
       if (op.getChildOperators() != null) {
         org.apache.hadoop.hive.ql.plan.api.Adjacency entry =
@@ -251,7 +250,7 @@ private void populateOperatorGraph(
             opsToVisit.add(childOp);
           }
         }
-        task.getOperatorGraph().addToAdjacencyList(entry);
+        entry.addToAdjacencyList(task.getOperatorGraph());
       }
     }
   }
@@ -278,7 +277,7 @@ private void populateQueryPlan() throws IOException {
         new org.apache.hadoop.hive.ql.plan.api.Stage();
       stage.setStageId(task.getId());
       stage.setStageType(task.getType());
-      query.addToStageList(stage);
+      stage.addToStageList(query);
 
       if (task instanceof ExecDriver) {
         // populate map task
@@ -330,7 +329,7 @@ private void populateQueryPlan() throws IOException {
                 tasksToVisit.add(childTask);
               }
             }
-            query.getStageGraph().addToAdjacencyList(childEntry);
+            childEntry.addToAdjacencyList(query.getStageGraph());
           }
 
           listEntry.addToChildren(listTask.getId());
@@ -338,7 +337,7 @@ private void populateQueryPlan() throws IOException {
             tasksToVisit.add(listTask);
           }
         }
-        query.getStageGraph().addToAdjacencyList(listEntry);
+        listEntry.addToAdjacencyList(query.getStageGraph());
       } else if (task.getChildTasks() != null) {
         org.apache.hadoop.hive.ql.plan.api.Adjacency entry =
           new org.apache.hadoop.hive.ql.plan.api.Adjacency();
@@ -351,7 +350,7 @@ private void populateQueryPlan() throws IOException {
             tasksToVisit.add(childTask);
           }
         }
-        query.getStageGraph().addToAdjacencyList(entry);
+        entry.addToAdjacencyList(query.getStageGraph());
       }
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapOperator.java
index c7af991..0e6ecbc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractMapOperator.java
@@ -19,20 +19,21 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import java.io.Serializable;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.TreeMap;
+import java.util.*;
 
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.ql.CompilationOpContext;
+import org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor;
+import org.apache.hadoop.hive.ql.exec.tez.tools.KeyValueInputMerger;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.serde2.Deserializer;
 import org.apache.hadoop.hive.serde2.SerDeException;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Writable;
+import org.apache.tez.runtime.library.api.KeyValueReader;
 
 
 /**
@@ -44,7 +45,22 @@
 
   private static final long serialVersionUID = 1L;
 
-  /**
+    @SuppressWarnings("deprecation")
+    public KeyValueReader getKeyValueReader(Collection<KeyValueReader> keyValueReaders, MapRecordProcessor mapRecordProcessor)
+      throws Exception {
+        List<KeyValueReader> kvReaderList = new ArrayList<KeyValueReader>(keyValueReaders);
+      // this sets up the map operator contexts correctly
+      initializeContexts();
+      Deserializer deserializer = getCurrentDeserializer();
+      // deserializer is null in case of VectorMapOperator
+      KeyValueReader reader =
+        new KeyValueInputMerger(kvReaderList, deserializer,
+            new ObjectInspector[] { deserializer == null ? null : deserializer.getObjectInspector() }, getConf()
+            .getSortCols());
+      return reader;
+    }
+
+    /**
    * Initialization call sequence:
    *
    *   (Operator)                     Operator.setConf(MapWork conf);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java
index e17068e..ad866fd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/PTFPartition.java
@@ -21,7 +21,9 @@
 import java.util.ConcurrentModificationException;
 import java.util.Iterator;
 import java.util.List;
+import java.util.Map;
 
+import org.apache.hadoop.hive.ql.udf.ptf.ValueBoundaryScanner;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -133,7 +135,34 @@ public void close() {
     }
   }
 
-  class PItr implements PTFPartitionIterator<Object> {
+    /**
+     * Retrieves the range for rowIdx, then removes all previous range entries before it.
+     * @param rowIdx row index.
+     * @param willScanFwd false: removal is started only from the previous previous range.
+     * @param valueBoundaryScanner
+     */
+    public void checkIfCacheCanEvict(int rowIdx, boolean willScanFwd, ValueBoundaryScanner valueBoundaryScanner) {
+        BoundaryCache cache = getBoundaryCache();
+      if (cache == null) {
+        return;
+      }
+      Map.Entry<Integer, Object> floorEntry = cache.floorEntry(rowIdx);
+      if (floorEntry != null) {
+        floorEntry = cache.floorEntry(floorEntry.getKey() - 1);
+        if (floorEntry != null) {
+          if (willScanFwd) {
+            cache.evictThisAndAllBefore(floorEntry.getKey());
+          } else {
+            floorEntry = cache.floorEntry(floorEntry.getKey() - 1);
+            if (floorEntry != null) {
+              cache.evictThisAndAllBefore(floorEntry.getKey());
+            }
+          }
+        }
+      }
+    }
+
+    class PItr implements PTFPartitionIterator<Object> {
     int idx;
     final int start;
     final int end;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/RowSchema.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/RowSchema.java
index 72dc631..f6e5248 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/RowSchema.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/RowSchema.java
@@ -18,6 +18,10 @@
 
 package org.apache.hadoop.hive.ql.exec;
 
+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;
+import org.apache.hadoop.hive.ql.plan.FilterDesc;
+import org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate;
+
 import java.io.Serializable;
 import java.util.ArrayList;
 import java.util.HashSet;
@@ -176,4 +180,16 @@ public String toString() {
     sb.append(')');
     return sb.toString();
   }
+
+    // insert filter operator between target(child) and input(parent)
+    public Operator<FilterDesc> createFilter(Operator<?> target, Operator<?> parent,
+                                             ExprNodeDesc filterExpr, PredicateTransitivePropagate predicateTransitivePropagate) {
+        Operator<FilterDesc> filter = OperatorFactory.get(parent.getCompilationOpContext(),
+          new FilterDesc(filterExpr, false), new RowSchema(getSignature()));
+      filter.getParentOperators().add(parent);
+      filter.getChildOperators().add(target);
+      parent.replaceChild(target, filter);
+      target.replaceParent(parent, filter);
+      return filter;
+    }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
index ae84d2d..6116beb 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java
@@ -32,7 +32,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.JoinUtil;
-import org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.NonMatchedSmallTableIterator;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
 import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
@@ -182,7 +181,7 @@ public GetAdaptor(MapJoinKey key) {
         }
       }
       for (int i = 0; i < keyOutputWriters.length; i++) {
-        currentKey[i] = keyWrapperBatch.getWritableKeyValue(kw, i, keyOutputWriters[i]);
+        currentKey[i] = kw.getWritableKeyValue(i, keyOutputWriters[i], keyWrapperBatch);
       }
       key =  MapJoinKey.readFromVector(output, key, currentKey, vectorKeyOIs, !isFirstKey);
       isFirstKey = false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
index 545a729..920d958 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HybridHashTableContainer.java
@@ -39,7 +39,6 @@
 import org.apache.hadoop.hive.ql.exec.JoinUtil.JoinResult;
 import org.apache.hadoop.hive.ql.exec.SerializationUtilities;
 import org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.KeyValueHelper;
-import org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.NonMatchedSmallTableIterator;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.exec.vector.rowbytescontainer.VectorRowBytesContainer;
 import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
@@ -837,7 +836,7 @@ public GetAdaptor() {
         assert nulls.length == keyOutputWriters.length;
       }
       for (int i = 0; i < keyOutputWriters.length; i++) {
-        currentKey[i] = keyWrapperBatch.getWritableKeyValue(kw, i, keyOutputWriters[i]);
+        currentKey[i] = kw.getWritableKeyValue(i, keyOutputWriters[i], keyWrapperBatch);
         nulls[i] = currentKey[i] == null;
       }
       return currentValue.setFromOutput(
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
index 464edf6..b6ac904 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinBytesTableContainer.java
@@ -30,7 +30,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
 import org.apache.hadoop.hive.ql.exec.JoinUtil;
-import org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainer.NonMatchedSmallTableIterator;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
 import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
@@ -564,7 +563,7 @@ public GetAdaptor() {
         assert nulls.length == keyOutputWriters.length;
       }
       for (int i = 0; i < keyOutputWriters.length; i++) {
-        currentKey[i] = keyWrapperBatch.getWritableKeyValue(kw, i, keyOutputWriters[i]);
+        currentKey[i] = kw.getWritableKeyValue(i, keyOutputWriters[i], keyWrapperBatch);
         nulls[i] = currentKey[i] == null;
       }
       return currentValue.setFromOutput(
@@ -593,7 +592,7 @@ public GetAdaptor() {
       }
       boolean hasNulls = false;
       for (int i = 0; i < keyOutputWriters.length; i++) {
-        currentKey[i] = keyWrapperBatch.getWritableKeyValue(kw, i, keyOutputWriters[i]);
+        currentKey[i] = kw.getWritableKeyValue(i, keyOutputWriters[i], keyWrapperBatch);
         if (currentKey[i] == null) {
           nulls[i] = true;
           hasNulls = true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
index c44315f..cc58e44 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKey.java
@@ -130,7 +130,7 @@ public static Output serializeVector(Output byteStream, VectorHashKeyWrapperBase
       fieldOis.add(writer.getObjectInspector());
       // This is rather convoluted... to simplify for perf, we could call getRawKeyValue
       // instead of writable, and serialize based on Java type as opposed to OI.
-      fieldData[i] = keyWrapperBatch.getWritableKeyValue(kw, i, writer);
+      fieldData[i] = kw.getWritableKeyValue(i, writer, keyWrapperBatch);
       if (nulls != null) {
         nulls[i] = (fieldData[i] == null);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
index 555ccdf..47645be 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/MapJoinKeyObject.java
@@ -24,8 +24,6 @@
 import java.util.Arrays;
 import java.util.List;
 
-import org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBase;
 import org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch;
@@ -155,8 +153,8 @@ public void readFromVector(VectorHashKeyWrapperBase kw, VectorExpressionWriter[]
       key = new Object[keyOutputWriters.length];
     }
     for (int keyIndex = 0; keyIndex < keyOutputWriters.length; ++keyIndex) {
-      key[keyIndex] = keyWrapperBatch.getWritableKeyValue(
-          kw, keyIndex, keyOutputWriters[keyIndex]);
+      key[keyIndex] = kw.getWritableKeyValue(
+              keyIndex, keyOutputWriters[keyIndex], keyWrapperBatch);
     }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
index 84bdcd7..475a9c8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplLoadTask.java
@@ -180,7 +180,7 @@ a database ( directory )
           LoadTable loadTable = new LoadTable(tableEvent, context, iterator.replLogger(),
                                               tableContext, loadTaskTracker);
           tableTracker = loadTable.tasks();
-          setUpDependencies(dbTracker, tableTracker);
+          dbTracker.setUpDependencies(tableTracker, this);
           if (!scope.database && tableTracker.hasTasks()) {
             scope.rootTasks.addAll(tableTracker.tasks());
             scope.table = true;
@@ -237,7 +237,7 @@ a database ( directory )
           if (!scope.database) {
             scope.rootTasks.addAll(functionsTracker.tasks());
           } else {
-            setUpDependencies(dbTracker, functionsTracker);
+            dbTracker.setUpDependencies(functionsTracker, this);
           }
           loadTaskTracker.update(functionsTracker);
           functionsTracker.debugLog("functions");
@@ -418,7 +418,7 @@ private TaskTracker updateDatabaseLastReplID(int maxTasks, Context context, Scop
   private void partitionsPostProcessing(BootstrapEventsIterator iterator,
       Scope scope, TaskTracker loadTaskTracker, TaskTracker tableTracker,
       TaskTracker partitionsTracker) {
-    setUpDependencies(tableTracker, partitionsTracker);
+    tableTracker.setUpDependencies(partitionsTracker, this);
     if (!scope.database && !scope.table) {
       scope.rootTasks.addAll(partitionsTracker.tasks());
       scope.partition = true;
@@ -430,24 +430,7 @@ private void partitionsPostProcessing(BootstrapEventsIterator iterator,
     }
   }
 
-  /*
-      This sets up dependencies such that a child task is dependant on the parent to be complete.
-   */
-  private void setUpDependencies(TaskTracker parentTasks, TaskTracker childTasks) {
-    if (parentTasks.hasTasks()) {
-      for (Task<? extends Serializable> parentTask : parentTasks.tasks()) {
-        for (Task<? extends Serializable> childTask : childTasks.tasks()) {
-          parentTask.addDependentTask(childTask);
-        }
-      }
-    } else {
-      for (Task<? extends Serializable> childTask : childTasks.tasks()) {
-        parentTasks.addTask(childTask);
-      }
-    }
-  }
-
-  private void createBuilderTask(List<Task<? extends Serializable>> rootTasks) {
+    private void createBuilderTask(List<Task<? extends Serializable>> rootTasks) {
     // Use loadTask as dependencyCollection
     Task<ReplLoadWork> loadTask = TaskFactory.get(work, conf);
     DAGTraversal.traverse(rootTasks, new AddDependencyToLeaves(loadTask));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/FSTableEvent.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/FSTableEvent.java
index 64f9af3..83193ad 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/FSTableEvent.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/events/filesystem/FSTableEvent.java
@@ -23,8 +23,6 @@
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.TableType;
 import org.apache.hadoop.hive.metastore.Warehouse;
-import org.apache.hadoop.hive.metastore.api.ColumnStatistics;
-import org.apache.hadoop.hive.metastore.api.ColumnStatisticsDesc;
 import org.apache.hadoop.hive.metastore.api.MetaException;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.ql.ddl.table.partition.AlterTableAddPartitionDesc;
@@ -146,7 +144,7 @@ public ImportTableDesc tableDesc(String dbName) throws SemanticException {
     //TODO: if partitions are loaded lazily via the iterator then we will have to avoid conversion of everything here as it defeats the purpose.
     for (Partition partition : metadata.getPartitions()) {
       // TODO: this should ideally not create AddPartitionDesc per partition
-      AlterTableAddPartitionDesc partsDesc = partitionDesc(fromPath, tblDesc, partition);
+      AlterTableAddPartitionDesc partsDesc = tblDesc.partitionDesc(fromPath, partition, this);
       descs.add(partsDesc);
     }
     return descs;
@@ -167,52 +165,7 @@ public ImportTableDesc tableDesc(String dbName) throws SemanticException {
     return partitions;
   }
 
-  private AlterTableAddPartitionDesc partitionDesc(Path fromPath,
-      ImportTableDesc tblDesc, Partition partition) throws SemanticException {
-    try {
-      AlterTableAddPartitionDesc partsDesc =
-          new AlterTableAddPartitionDesc(tblDesc.getDatabaseName(), tblDesc.getTableName(),
-              EximUtil.makePartSpec(tblDesc.getPartCols(), partition.getValues()),
-              partition.getSd().getLocation(), partition.getParameters());
-      AlterTableAddPartitionDesc.PartitionDesc partDesc = partsDesc.getPartition(0);
-      partDesc.setInputFormat(partition.getSd().getInputFormat());
-      partDesc.setOutputFormat(partition.getSd().getOutputFormat());
-      partDesc.setNumBuckets(partition.getSd().getNumBuckets());
-      partDesc.setCols(partition.getSd().getCols());
-      partDesc.setSerializationLib(partition.getSd().getSerdeInfo().getSerializationLib());
-      partDesc.setSerdeParams(partition.getSd().getSerdeInfo().getParameters());
-      partDesc.setBucketCols(partition.getSd().getBucketCols());
-      partDesc.setSortCols(partition.getSd().getSortCols());
-      if (tblDesc.isExternal() && !replicationSpec().isMigratingToExternalTable()) {
-        // we have to provide the source location so target location can be derived.
-        partDesc.setLocation(partition.getSd().getLocation());
-      } else {
-        /**
-         * this is required for file listing of all files in a partition for managed table as described in
-         * {@link org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator}
-         */
-        partDesc.setLocation(new Path(fromPath,
-            Warehouse.makePartName(tblDesc.getPartCols(), partition.getValues())).toString());
-      }
-      partsDesc.setReplicationSpec(replicationSpec());
-
-      if (partition.isSetColStats()) {
-        ColumnStatistics colStats = partition.getColStats();
-        ColumnStatisticsDesc colStatsDesc = new ColumnStatisticsDesc(colStats.getStatsDesc());
-        colStatsDesc.setTableName(tblDesc.getTableName());
-        colStatsDesc.setDbName(tblDesc.getDatabaseName());
-        partDesc.setColStats(new ColumnStatistics(colStatsDesc, colStats.getStatsObj()));
-        long writeId = replicationSpec().isMigratingToTxnTable() ?
-                ReplUtils.REPL_BOOTSTRAP_MIGRATION_BASE_WRITE_ID : partition.getWriteId();
-        partDesc.setWriteId(writeId);
-      }
-      return partsDesc;
-    } catch (Exception e) {
-      throw new SemanticException(e);
-    }
-  }
-
-  @Override
+    @Override
   public ReplicationSpec replicationSpec() {
     return metadata.getReplicationSpec();
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/TaskTracker.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/TaskTracker.java
index 20ede9c..b4dc56a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/TaskTracker.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/TaskTracker.java
@@ -18,6 +18,7 @@
 package org.apache.hadoop.hive.ql.exec.repl.util;
 
 import org.apache.hadoop.hive.ql.exec.Task;
+import org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask;
 import org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.ReplicationState;
 import org.apache.hadoop.hive.ql.exec.util.DAGTraversal;
 import org.slf4j.Logger;
@@ -142,4 +143,21 @@ public void debugLog(String forEventType) {
   public int numberOfTasks() {
     return numberOfTasks;
   }
+
+    /*
+            This sets up dependencies such that a child task is dependant on the parent to be complete.
+         */
+    public void setUpDependencies(TaskTracker childTasks, ReplLoadTask replLoadTask) {
+        if (hasTasks()) {
+        for (Task<? extends Serializable> parentTask : tasks()) {
+          for (Task<? extends Serializable> childTask : childTasks.tasks()) {
+            parentTask.addDependentTask(childTask);
+          }
+        }
+      } else {
+        for (Task<? extends Serializable> childTask : childTasks.tasks()) {
+          addTask(childTask);
+        }
+      }
+    }
 }
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
index f06ac37..6561e3c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
@@ -72,17 +72,12 @@
 import org.apache.hadoop.hive.ql.exec.FileSinkOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.Utilities;
-import org.apache.hadoop.hive.ql.exec.mr.ExecMapper;
 import org.apache.hadoop.hive.ql.exec.mr.ExecReducer;
 import org.apache.hadoop.hive.ql.exec.tez.tools.TezMergedLogicalInput;
-import org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
 import org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.NullOutputCommitter;
 import org.apache.hadoop.hive.ql.io.HiveInputFormat;
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.hive.ql.io.HiveOutputFormatImpl;
-import org.apache.hadoop.hive.ql.io.merge.MergeFileMapper;
-import org.apache.hadoop.hive.ql.io.merge.MergeFileOutputFormat;
 import org.apache.hadoop.hive.ql.io.merge.MergeFileWork;
 import org.apache.hadoop.hive.ql.lib.DefaultGraphWalker;
 import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
@@ -112,7 +107,6 @@
 import org.apache.hadoop.hive.shims.Utils;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.mapred.FileOutputFormat;
 import org.apache.hadoop.mapred.InputFormat;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.OutputFormat;
@@ -291,72 +285,7 @@ private void addCredentials(ReduceWork reduceWork, DAG dag) {
     dag.addURIsForCredentials(fileSinkUris);
   }
 
-  /*
-   * Creates the configuration object necessary to run a specific vertex from
-   * map work. This includes input formats, input processor, etc.
-   */
-  private JobConf initializeVertexConf(JobConf baseConf, Context context, MapWork mapWork) {
-    JobConf conf = new JobConf(baseConf);
-
-    conf.set(Operator.CONTEXT_NAME_KEY, mapWork.getName());
-
-    if (mapWork.getNumMapTasks() != null) {
-      // Is this required ?
-      conf.setInt(MRJobConfig.NUM_MAPS, mapWork.getNumMapTasks().intValue());
-    }
-
-    if (mapWork.getMaxSplitSize() != null) {
-      HiveConf.setLongVar(conf, HiveConf.ConfVars.MAPREDMAXSPLITSIZE,
-          mapWork.getMaxSplitSize().longValue());
-    }
-
-    if (mapWork.getMinSplitSize() != null) {
-      HiveConf.setLongVar(conf, HiveConf.ConfVars.MAPREDMINSPLITSIZE,
-          mapWork.getMinSplitSize().longValue());
-    }
-
-    if (mapWork.getMinSplitSizePerNode() != null) {
-      HiveConf.setLongVar(conf, HiveConf.ConfVars.MAPREDMINSPLITSIZEPERNODE,
-          mapWork.getMinSplitSizePerNode().longValue());
-    }
-
-    if (mapWork.getMinSplitSizePerRack() != null) {
-      HiveConf.setLongVar(conf, HiveConf.ConfVars.MAPREDMINSPLITSIZEPERRACK,
-          mapWork.getMinSplitSizePerRack().longValue());
-    }
-
-    Utilities.setInputAttributes(conf, mapWork);
-
-    String inpFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVETEZINPUTFORMAT);
-
-    if (mapWork.isUseBucketizedHiveInputFormat()) {
-      inpFormat = BucketizedHiveInputFormat.class.getName();
-    }
-
-    if (mapWork.getDummyTableScan()) {
-      // hive input format doesn't handle the special condition of no paths + 1
-      // split correctly.
-      inpFormat = CombineHiveInputFormat.class.getName();
-    }
-
-    conf.set(TEZ_TMP_DIR_KEY, context.getMRTmpPath().toUri().toString());
-    conf.set("mapred.mapper.class", ExecMapper.class.getName());
-    conf.set("mapred.input.format.class", inpFormat);
-
-    if (mapWork instanceof MergeFileWork) {
-      MergeFileWork mfWork = (MergeFileWork) mapWork;
-      // This mapper class is used for serializaiton/deserializaiton of merge
-      // file work.
-      conf.set("mapred.mapper.class", MergeFileMapper.class.getName());
-      conf.set("mapred.input.format.class", mfWork.getInputformat());
-      conf.setClass("mapred.output.format.class", MergeFileOutputFormat.class,
-          FileOutputFormat.class);
-    }
-
-    return conf;
-  }
-
-  /**
+    /**
    * Given a Vertex group and a vertex createEdge will create an
    * Edge between them.
    *
@@ -1380,7 +1309,7 @@ public JobConf initializeVertexConf(JobConf conf, Context context, BaseWork work
     // simply dispatch the call to the right method for the actual (sub-) type of
     // BaseWork.
     if (work instanceof MapWork) {
-      return initializeVertexConf(conf, context, (MapWork)work);
+      return ((MapWork)work).initializeVertexConf(conf, context, this);
     } else if (work instanceof ReduceWork) {
       return initializeVertexConf(conf, context, (ReduceWork)work);
     } else if (work instanceof MergeJoinWork) {
@@ -1393,7 +1322,7 @@ public JobConf initializeVertexConf(JobConf conf, Context context, BaseWork work
 
   private JobConf initializeVertexConf(JobConf conf, Context context, MergeJoinWork work) {
     if (work.getMainWork() instanceof MapWork) {
-      return initializeVertexConf(conf, context, (MapWork) (work.getMainWork()));
+      return ((MapWork) (work.getMainWork())).initializeVertexConf(conf, context, this);
     } else {
       return initializeVertexConf(conf, context, (ReduceWork) (work.getMainWork()));
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
index ea2e1fd..83645c6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java
@@ -55,15 +55,12 @@
 import org.apache.hadoop.hive.ql.exec.mr.ExecMapperContext;
 import org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.RegistryConfTez;
 import org.apache.hadoop.hive.ql.exec.tez.TezProcessor.TezKVOutputCollector;
-import org.apache.hadoop.hive.ql.exec.tez.tools.KeyValueInputMerger;
 import org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator;
 import org.apache.hadoop.hive.ql.log.PerfLogger;
 import org.apache.hadoop.hive.ql.plan.BaseWork;
 import org.apache.hadoop.hive.ql.plan.DynamicValue;
 import org.apache.hadoop.hive.ql.plan.MapWork;
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
-import org.apache.hadoop.hive.serde2.Deserializer;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.tez.mapreduce.input.MRInputLegacy;
 import org.apache.tez.mapreduce.input.MultiMRInput;
@@ -372,7 +369,7 @@ private void initializeMapRecordSources() throws Exception {
     sources[position] = new MapRecordSource();
     KeyValueReader reader = null;
     if (mainWorkMultiMRInput != null) {
-      reader = getKeyValueReader(mainWorkMultiMRInput.getKeyValueReaders(), mapOp);
+      reader = mapOp.getKeyValueReader(mainWorkMultiMRInput.getKeyValueReaders(), this);
     } else {
       reader = legacyMRInput.getReader();
     }
@@ -385,31 +382,14 @@ private void initializeMapRecordSources() throws Exception {
       Collection<KeyValueReader> kvReaders = multiMRInput.getKeyValueReaders();
       l4j.debug("There are " + kvReaders.size() + " key-value readers for input " + inputName);
       if (kvReaders.size() > 0) {
-        reader = getKeyValueReader(kvReaders, mapOp);
+        reader = mapOp.getKeyValueReader(kvReaders, this);
         sources[tag].init(jconf, mapOp, reader);
       }
     }
     ((TezContext) MapredContext.get()).setRecordSources(sources);
   }
 
-  @SuppressWarnings("deprecation")
-  private KeyValueReader getKeyValueReader(Collection<KeyValueReader> keyValueReaders,
-      AbstractMapOperator mapOp)
-    throws Exception {
-    List<KeyValueReader> kvReaderList = new ArrayList<KeyValueReader>(keyValueReaders);
-    // this sets up the map operator contexts correctly
-    mapOp.initializeContexts();
-    Deserializer deserializer = mapOp.getCurrentDeserializer();
-    // deserializer is null in case of VectorMapOperator
-    KeyValueReader reader =
-      new KeyValueInputMerger(kvReaderList, deserializer,
-          new ObjectInspector[] { deserializer == null ? null : deserializer.getObjectInspector() }, mapOp
-          .getConf()
-          .getSortCols());
-    return reader;
-  }
-
-  private Operator<? extends OperatorDesc> getFinalOp(Operator<? extends OperatorDesc> mergeMapOp) {
+    private Operator<? extends OperatorDesc> getFinalOp(Operator<? extends OperatorDesc> mergeMapOp) {
     for (Operator<? extends OperatorDesc> childOp : mergeMapOp.getChildOperators()) {
       if ((childOp.getChildOperators() == null) || (childOp.getChildOperators().isEmpty())) {
         return childOp;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
index f2ed07a..47a0ca9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java
@@ -159,7 +159,7 @@ public int execute(DriverContext driverContext) {
       // We only need a username for UGI to use for groups; getGroups will fetch the groups
       // based on Hadoop configuration, as documented at
       // https://hadoop.apache.org/docs/r2.8.0/hadoop-project-dist/hadoop-common/GroupsMapping.html
-      String userName = getUserNameForGroups(ss);
+      String userName = ss.getUserNameForGroups(this);
       List<String> groups = null;
       if (userName == null) {
         userName = "anonymous";
@@ -323,16 +323,7 @@ public int execute(DriverContext driverContext) {
     return rc;
   }
 
-  private String getUserNameForGroups(SessionState ss) {
-    // This should be removed when authenticator and the 2-username mess is cleaned up.
-    if (ss.getAuthenticator() != null) {
-      String userName = ss.getAuthenticator().getUserName();
-      if (userName != null) return userName;
-    }
-    return ss.getUserName();
-  }
-
-  private void closeDagClientOnCancellation(DAGClient dagClient) {
+    private void closeDagClientOnCancellation(DAGClient dagClient) {
     try {
       dagClient.tryKillDAG();
       LOG.info("Waiting for Tez task to shut down: " + this);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WmTezSession.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WmTezSession.java
index fa2b02e..20cbbda 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WmTezSession.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WmTezSession.java
@@ -27,6 +27,7 @@
 import java.util.concurrent.ScheduledFuture;
 import java.util.concurrent.TimeUnit;
 import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.registry.impl.TezAmInstance;
 import org.apache.hive.common.util.Ref;
 import org.codehaus.jackson.annotate.JsonIgnore;
@@ -58,7 +59,16 @@
 
   private final WorkloadManager wmParent;
 
-  /** The actual state of the guaranteed task, and the update state, for the session. */
+    public void resetGlobalTezSession(WorkloadManager workloadManager) {
+        // This has to be done synchronously to avoid the caller getting this session again.
+      // Ideally we'd get rid of this thread-local nonsense.
+      SessionState sessionState = SessionState.get();
+      if (sessionState != null && sessionState.getTezSession() == this) {
+        sessionState.setTezSession(null);
+      }
+    }
+
+    /** The actual state of the guaranteed task, and the update state, for the session. */
   // Note: hypothetically, a generic WM-aware-session should not know about guaranteed tasks.
   //       We should have another subclass for a WM-aware-session-implemented-using-ducks.
   //       However, since this is the only type of WM for now, this can live here.
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
index 7ede4c8..6e4f0cd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/WorkloadManager.java
@@ -1451,7 +1451,7 @@ public WmTezSession getSession(TezSessionState session, MappingInput input, Hive
   @Override
   public void destroy(TezSessionState session) throws Exception {
     WmTezSession wmTezSession = ensureOwnedSession(session);
-    resetGlobalTezSession(wmTezSession);
+    wmTezSession.resetGlobalTezSession(this);
     currentLock.lock();
     try {
       current.toDestroy.add(wmTezSession);
@@ -1461,19 +1461,10 @@ public void destroy(TezSessionState session) throws Exception {
     }
   }
 
-  private void resetGlobalTezSession(WmTezSession wmTezSession) {
-    // This has to be done synchronously to avoid the caller getting this session again.
-    // Ideally we'd get rid of this thread-local nonsense.
-    SessionState sessionState = SessionState.get();
-    if (sessionState != null && sessionState.getTezSession() == wmTezSession) {
-      sessionState.setTezSession(null);
-    }
-  }
-
-  @Override
+    @Override
   public void returnAfterUse(TezSessionPoolSession session) throws Exception {
     WmTezSession wmTezSession = ensureOwnedSession(session);
-    resetGlobalTezSession(wmTezSession);
+    wmTezSession.resetGlobalTezSession(this);
     currentLock.lock();
     try {
       wmTezSession.createAndSetReturnFuture();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/TezProgressMonitor.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/TezProgressMonitor.java
index 735442d..e97fd11 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/TezProgressMonitor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/monitoring/TezProgressMonitor.java
@@ -96,7 +96,7 @@
           results.add(
             Arrays.asList(
               getNameWithProgress(vertexName, progress.succeededTaskCount, progress.totalTaskCount),
-              getMode(baseWork),
+              baseWork.getMode(this),
               progress.vertexStatus(vertexStatusMap.get(vertexName)),
               progress.total(),
               progress.completed(),
@@ -194,22 +194,7 @@ private String getNameWithProgress(String s, int complete, int total) {
     return result;
   }
 
-  private String getMode(BaseWork work) {
-    String mode = "container";
-    if (work != null) {
-      // uber > llap > container
-      if (work.getUberMode()) {
-        mode = "uber";
-      } else if (work.getLlapMode()) {
-        mode = "llap";
-      } else {
-        mode = "container";
-      }
-    }
-    return mode;
-  }
-
-  public static class VertexProgress {
+    public static class VertexProgress {
     private final int totalTaskCount;
     private final int succeededTaskCount;
     private final int failedTaskAttemptCount;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java
index 5a903d3..6b55062 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorMapOperator.java
@@ -384,8 +384,9 @@ public void init(Configuration hconf)
   public VectorPartitionContext createAndInitPartitionContext(PartitionDesc partDesc,
       Configuration hconf)
           throws SerDeException, Exception {
+      vectorMapOperator.getClass();
 
-    VectorPartitionDesc vectorPartDesc = partDesc.getVectorPartitionDesc();
+      VectorPartitionDesc vectorPartDesc = getVectorPartitionDesc();
     if (vectorPartDesc == null) {
       return null;
     }
@@ -398,13 +399,13 @@ public VectorPartitionContext createAndInitPartitionContext(PartitionDesc partDe
       // Verify hive.exec.schema.evolution is true or we have an ACID table so we are producing
       // the table schema from ORC.  The Vectorizer class assures this.
       boolean isAcid =
-          AcidUtils.isTablePropertyTransactional(partDesc.getTableDesc().getProperties());
+          AcidUtils.isTablePropertyTransactional(getTableDesc().getProperties());
       Preconditions.checkState(Utilities.isSchemaEvolutionEnabled(hconf, isAcid));
     }
 
     switch (vectorMapOperatorReadType) {
     case VECTORIZED_INPUT_FILE_FORMAT:
-      vectorPartitionContext = new VectorizedInputFileFormatPartitionContext(partDesc);
+      vectorPartitionContext = vectorMapOperator.new VectorizedInputFileFormatPartitionContext(partDesc);
       break;
 
     case VECTOR_DESERIALIZE:
@@ -412,7 +413,7 @@ public VectorPartitionContext createAndInitPartitionContext(PartitionDesc partDe
       break;
 
     case ROW_DESERIALIZE:
-      vectorPartitionContext = new RowDeserializePartitionContext(partDesc);
+      vectorPartitionContext = new RowDeserializePartitionContext(this);
       break;
 
     default:
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
index 267d0dc..08969a9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorSMBMapJoinOperator.java
@@ -37,7 +37,6 @@
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.SMBJoinDesc;
 import org.apache.hadoop.hive.ql.plan.VectorDesc;
-import org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc;
 import org.apache.hadoop.hive.ql.plan.VectorSMBJoinDesc;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -197,7 +196,7 @@ public SMBJoinKeyEvaluator init() {
       @Override
       public List<Object> evaluate(VectorHashKeyWrapperBase kw) throws HiveException {
         for(int i = 0; i < keyExpressions.length; ++i) {
-          key.set(i, keyWrapperBatch.getWritableKeyValue(kw, i, keyOutputWriters[i]));
+          key.set(i, kw.getWritableKeyValue(i, keyOutputWriters[i], keyWrapperBatch));
         }
         return key;
       };
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprColumnCondExpr.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprColumnCondExpr.java
index 0a1fa17..5e6ba6c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprColumnCondExpr.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprColumnCondExpr.java
@@ -110,7 +110,7 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
       outputColVector.setElement(batchIndex, batchIndex, thenColVector);
     }
 
-    conditionalEvaluate(batch, childExpressions[2], elseSelected, elseCount);
+    childExpressions[2].conditionalEvaluate(batch, elseSelected, elseCount, IfExprColumnCondExpr.this);
     for (int i = 0; i < elseCount; i++) {
       final int batchIndex = elseSelected[i];
       outputIsNull[batchIndex] = false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprBase.java
index 04b54a2..d05ccc3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprBase.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprBase.java
@@ -54,25 +54,7 @@ public IfExprCondExprBase() {
     arg1Column = -1;
   }
 
-  public void conditionalEvaluate(VectorizedRowBatch batch, VectorExpression condVecExpr,
-      int[] condSelected, int condSize) throws HiveException {
-
-    int saveSize = batch.size;
-    boolean saveSelectedInUse = batch.selectedInUse;
-    int[] saveSelected = batch.selected;
-
-    batch.size = condSize;
-    batch.selectedInUse = true;
-    batch.selected = condSelected;
-
-    condVecExpr.evaluate(batch);
-
-    batch.size = saveSize;
-    batch.selectedInUse = saveSelectedInUse;
-    batch.selected = saveSelected;
-  }
-
-  @Override
+    @Override
   public void evaluate(VectorizedRowBatch batch) throws HiveException {
 
     // NOTE: We do conditional vector expression so we do not call super.evaluateChildren(batch).
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprColumn.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprColumn.java
index f2f4d3e..9f78bc3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprColumn.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprColumn.java
@@ -101,7 +101,7 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
 
     // NOTE: We cannot use copySelected below since it is a whole column operation.
 
-    conditionalEvaluate(batch, childExpressions[1], thenSelected, thenCount);
+    childExpressions[1].conditionalEvaluate(batch, thenSelected, thenCount, IfExprCondExprColumn.this);
     for (int i = 0; i < thenCount; i++) {
       final int batchIndex = thenSelected[i];
       outputIsNull[batchIndex] = false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprCondExpr.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprCondExpr.java
index 59db6b7..16ba56b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprCondExpr.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprCondExpr.java
@@ -100,14 +100,14 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
 
     // NOTE: We cannot use copySelected below since it is a whole column operation.
 
-    conditionalEvaluate(batch, childExpressions[1], thenSelected, thenCount);
+    childExpressions[1].conditionalEvaluate(batch, thenSelected, thenCount, IfExprCondExprCondExpr.this);
     for (int i = 0; i < thenCount; i++) {
       final int batchIndex = thenSelected[i];
       outputIsNull[batchIndex] = false;
       outputColVector.setElement(batchIndex, batchIndex, thenColVector);
     }
 
-    conditionalEvaluate(batch, childExpressions[2], elseSelected, elseCount);
+    childExpressions[2].conditionalEvaluate(batch, elseSelected, elseCount, IfExprCondExprCondExpr.this);
     for (int i = 0; i < elseCount; i++) {
       final int batchIndex = elseSelected[i];
       outputIsNull[batchIndex] = false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprNull.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprNull.java
index 40dbb38..fdcdbe8 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprNull.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprCondExprNull.java
@@ -95,7 +95,7 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
 
     // NOTE: We cannot use copySelected below since it is a whole column operation.
 
-    conditionalEvaluate(batch, childExpressions[1], thenSelected, thenCount);
+    childExpressions[1].conditionalEvaluate(batch, thenSelected, thenCount, IfExprCondExprNull.this);
     for (int i = 0; i < thenCount; i++) {
       final int batchIndex = thenSelected[i];
       outputIsNull[batchIndex] = false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprNullCondExpr.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprNullCondExpr.java
index aa971c3..4990e0e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprNullCondExpr.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/IfExprNullCondExpr.java
@@ -102,7 +102,7 @@ public void evaluate(VectorizedRowBatch batch) throws HiveException {
     }
 
     // Second input parameter but 3rd column.
-    conditionalEvaluate(batch, childExpressions[1], elseSelected, elseCount);
+    childExpressions[1].conditionalEvaluate(batch, elseSelected, elseCount, IfExprNullCondExpr.this);
     for (int i = 0; i < elseCount; i++) {
       final int batchIndex = elseSelected[i];
       outputIsNull[batchIndex] = false;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpression.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpression.java
index 893da15..9222a10 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpression.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpression.java
@@ -373,4 +373,22 @@ public static String displayArrayOfUtf8ByteArrays(byte[][] arrayOfByteArrays) {
     }
     return sb.toString();
   }
+
+    public void conditionalEvaluate(VectorizedRowBatch batch,
+                                    int[] condSelected, int condSize, IfExprCondExprBase ifExprCondExprBase) throws HiveException {
+
+        int saveSize = batch.size;
+      boolean saveSelectedInUse = batch.selectedInUse;
+      int[] saveSelected = batch.selected;
+
+      batch.size = condSize;
+      batch.selectedInUse = true;
+      batch.selected = condSelected;
+
+      evaluate(batch);
+
+      batch.size = saveSize;
+      batch.selectedInUse = saveSelectedInUse;
+      batch.selected = saveSelected;
+    }
 }
\ No newline at end of file
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyGenerateResultOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyGenerateResultOperator.java
index 35ddddd..db92ad2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyGenerateResultOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/VectorMapJoinInnerBigOnlyGenerateResultOperator.java
@@ -267,41 +267,7 @@ private void generateHashMultiSetResultMultiValue(VectorizedRowBatch batch,
     }
   }
 
-  /**
-   * Generate the inner big table only join output results for one vectorized row batch with
-   * a repeated key.
-   *
-   * @param batch
-   *          The big table batch with any matching and any non matching rows both as
-   *          selected in use.
-   * @param hashMultiSetResult
-   *          The hash multi-set results for the batch.
-   */
-  protected int generateHashMultiSetResultRepeatedAll(VectorizedRowBatch batch,
-          VectorMapJoinHashMultiSetResult hashMultiSetResult) throws HiveException {
-
-    long count = hashMultiSetResult.count();
-
-    if (batch.selectedInUse) {
-      // The selected array is already filled in as we want it.
-    } else {
-      int[] selected = batch.selected;
-      for (int i = 0; i < batch.size; i++) {
-        selected[i] = i;
-      }
-      batch.selectedInUse = true;
-    }
-
-    do {
-      forwardBigTableBatch(batch);
-      count--;
-    } while (count > 0);
-
-    // We forwarded the batch in this method.
-    return 0;
-  }
-
-  protected void finishInnerBigOnlyRepeated(VectorizedRowBatch batch, JoinUtil.JoinResult joinResult,
+    protected void finishInnerBigOnlyRepeated(VectorizedRowBatch batch, JoinUtil.JoinResult joinResult,
       VectorMapJoinHashMultiSetResult hashMultiSetResult) throws HiveException, IOException {
 
     switch (joinResult) {
@@ -315,7 +281,7 @@ protected void finishInnerBigOnlyRepeated(VectorizedRowBatch batch, JoinUtil.Joi
       }
 
       // Generate special repeated case.
-      int numSel = generateHashMultiSetResultRepeatedAll(batch, hashMultiSetResult);
+      int numSel = hashMultiSetResult.generateHashMultiSetResultRepeatedAll(batch, this);
       batch.size = numSel;
       batch.selectedInUse = true;
       break;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/hashtable/VectorMapJoinHashMultiSetResult.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/hashtable/VectorMapJoinHashMultiSetResult.java
index d5d7fb9..4a44820 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/hashtable/VectorMapJoinHashMultiSetResult.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/mapjoin/hashtable/VectorMapJoinHashMultiSetResult.java
@@ -18,6 +18,10 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.mapjoin.hashtable;
 
+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
+import org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyGenerateResultOperator;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+
 /*
  * Abstract class for a hash multi-set result.
  */
@@ -31,4 +35,35 @@
   public long count() {
    return count;
   }
+
+    /**
+     * Generate the inner big table only join output results for one vectorized row batch with
+     * a repeated key.
+     *  @param batch
+     *          The big table batch with any matching and any non matching rows both as
+     *          selected in use.
+     * @param vectorMapJoinInnerBigOnlyGenerateResultOperator
+     */
+    public int generateHashMultiSetResultRepeatedAll(VectorizedRowBatch batch, VectorMapJoinInnerBigOnlyGenerateResultOperator vectorMapJoinInnerBigOnlyGenerateResultOperator) throws HiveException {
+
+        long count = count();
+
+      if (batch.selectedInUse) {
+        // The selected array is already filled in as we want it.
+      } else {
+        int[] selected = batch.selected;
+        for (int i = 0; i < batch.size; i++) {
+          selected[i] = i;
+        }
+        batch.selectedInUse = true;
+      }
+
+      do {
+        vectorMapJoinInnerBigOnlyGenerateResultOperator.forwardBigTableBatch(batch);
+        count--;
+      } while (count > 0);
+
+      // We forwarded the batch in this method.
+      return 0;
+    }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBase.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBase.java
index 1bb2249..c512c13 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBase.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBase.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.vector.wrapper;
 
+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;
+import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hive.common.util.Murmur3;
 
 import java.sql.Timestamp;
@@ -40,7 +42,52 @@
  */
 public abstract class VectorHashKeyWrapperBase extends KeyWrapper {
 
-  public static final class HashContext {
+    /**
+     * Get the row-mode writable object value of a key from a key wrapper
+     * @param keyIndex
+     * @param keyOutputWriter
+     * @param vectorHashKeyWrapperBatch
+     */
+    public Object getWritableKeyValue(int keyIndex,
+                                      VectorExpressionWriter keyOutputWriter, VectorHashKeyWrapperBatch vectorHashKeyWrapperBatch)
+      throws HiveException {
+
+        if (isNull(keyIndex)) {
+        return null;
+      }
+
+      ColumnVector.Type columnVectorType = vectorHashKeyWrapperBatch.columnVectorTypes[keyIndex];
+      int columnTypeSpecificIndex = vectorHashKeyWrapperBatch.columnTypeSpecificIndices[keyIndex];
+
+      switch (columnVectorType) {
+      case LONG:
+        return keyOutputWriter.writeValue(
+            getLongValue(columnTypeSpecificIndex));
+      case DOUBLE:
+        return keyOutputWriter.writeValue(
+            getDoubleValue(columnTypeSpecificIndex));
+      case BYTES:
+        return keyOutputWriter.writeValue(
+            getBytes(columnTypeSpecificIndex),
+            getByteStart(columnTypeSpecificIndex),
+            getByteLength(columnTypeSpecificIndex));
+      case DECIMAL:
+        return keyOutputWriter.writeValue(
+            getDecimal(columnTypeSpecificIndex));
+      case DECIMAL_64:
+        throw new RuntimeException("Getting writable for DECIMAL_64 not supported");
+      case TIMESTAMP:
+        return keyOutputWriter.writeValue(
+            getTimestamp(columnTypeSpecificIndex));
+      case INTERVAL_DAY_TIME:
+        return keyOutputWriter.writeValue(
+            getIntervalDayTime(columnTypeSpecificIndex));
+      default:
+        throw new HiveException("Unexpected column vector type " + columnVectorType);
+      }
+    }
+
+    public static final class HashContext {
     private final Murmur3.IncrementalHash32 bytesHash = new Murmur3.IncrementalHash32();
 
     public static Murmur3.IncrementalHash32 getBytesHash(HashContext ctx) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBatch.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBatch.java
index dd31991..bdab84a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBatch.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/wrapper/VectorHashKeyWrapperBatch.java
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hive.ql.exec.vector.wrapper;
 
 import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;
-import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.util.JavaDataModel;
 import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;
@@ -954,50 +953,7 @@ public VectorHashKeyWrapperBase allocateKeyWrapper() {
         keyCount);
   }
 
-  /**
-   * Get the row-mode writable object value of a key from a key wrapper
-   * @param keyOutputWriter
-   */
-  public Object getWritableKeyValue(VectorHashKeyWrapperBase kw, int keyIndex,
-      VectorExpressionWriter keyOutputWriter)
-    throws HiveException {
-
-    if (kw.isNull(keyIndex)) {
-      return null;
-    }
-
-    ColumnVector.Type columnVectorType = columnVectorTypes[keyIndex];
-    int columnTypeSpecificIndex = columnTypeSpecificIndices[keyIndex];
-
-    switch (columnVectorType) {
-    case LONG:
-      return keyOutputWriter.writeValue(
-          kw.getLongValue(columnTypeSpecificIndex));
-    case DOUBLE:
-      return keyOutputWriter.writeValue(
-          kw.getDoubleValue(columnTypeSpecificIndex));
-    case BYTES:
-      return keyOutputWriter.writeValue(
-          kw.getBytes(columnTypeSpecificIndex),
-          kw.getByteStart(columnTypeSpecificIndex),
-          kw.getByteLength(columnTypeSpecificIndex));
-    case DECIMAL:
-      return keyOutputWriter.writeValue(
-          kw.getDecimal(columnTypeSpecificIndex));
-    case DECIMAL_64:
-      throw new RuntimeException("Getting writable for DECIMAL_64 not supported");
-    case TIMESTAMP:
-      return keyOutputWriter.writeValue(
-          kw.getTimestamp(columnTypeSpecificIndex));
-    case INTERVAL_DAY_TIME:
-      return keyOutputWriter.writeValue(
-          kw.getIntervalDayTime(columnTypeSpecificIndex));
-    default:
-      throw new HiveException("Unexpected column vector type " + columnVectorType);
-    }
-  }
-
-  public void setLongValue(VectorHashKeyWrapperBase kw, int keyIndex, Long value)
+    public void setLongValue(VectorHashKeyWrapperBase kw, int keyIndex, Long value)
     throws HiveException {
 
     if (columnVectorTypes[keyIndex] != Type.LONG) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java
index acc52af..eec59c7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java
@@ -42,7 +42,6 @@
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.io.NullWritable;
 import org.apache.hadoop.mapred.Counters;
-import org.apache.hadoop.mapred.FileInputFormat;
 import org.apache.hadoop.mapred.JobClient;
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapred.RunningJob;
@@ -128,7 +127,7 @@ public int execute(DriverContext driverContext) {
       }
 
       // add input path
-      addInputPaths(job, work);
+      work.addInputPaths(job, this);
 
       // serialize work
       Utilities.setMapWork(job, work, ctx.getMRTmpPath(), true);
@@ -212,13 +211,7 @@ public int execute(DriverContext driverContext) {
     return returnVal;
   }
 
-  private void addInputPaths(JobConf job, MergeFileWork work) {
-    for (Path path : work.getInputPaths()) {
-      FileInputFormat.addInputPath(job, path);
-    }
-  }
-
-  @Override
+    @Override
   public String getName() {
     return "MergeFileTask";
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java
index 3044603..fcb47df 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileWork.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.io.merge;
 
+import org.apache.hadoop.mapred.FileInputFormat;
+import org.apache.hadoop.mapred.JobConf;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.fs.FileStatus;
@@ -234,4 +236,10 @@ public String getMergeLevel() {
     }
     return null;
   }
+
+    public void addInputPaths(JobConf job, MergeFileTask mergeFileTask) {
+        for (Path path : getInputPaths()) {
+        FileInputFormat.addInputPath(job, path);
+      }
+    }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/ConvertAstToSearchArg.java b/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/ConvertAstToSearchArg.java
index 27fe828..4039884 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/ConvertAstToSearchArg.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/ConvertAstToSearchArg.java
@@ -354,30 +354,7 @@ private void createLeaf(PredicateLeaf.Operator operator,
     }
   }
 
-  /**
-   * Find the variable in the expression.
-   * @param expr the expression to look in
-   * @return the index of the variable or -1 if there is not exactly one
-   *   variable.
-   */
-  private int findVariable(ExprNodeDesc expr) {
-    int result = -1;
-    List<ExprNodeDesc> children = expr.getChildren();
-    for(int i = 0; i < children.size(); ++i) {
-      ExprNodeDesc child = children.get(i);
-      if (child instanceof ExprNodeColumnDesc) {
-        // if we already found a variable, this isn't a sarg
-        if (result != -1) {
-          return -1;
-        } else {
-          result = i;
-        }
-      }
-    }
-    return result;
-  }
-
-  /**
+    /**
    * Create a leaf expression when we aren't sure where the variable is
    * located.
    * @param operator the operator type that was found
@@ -385,7 +362,7 @@ private int findVariable(ExprNodeDesc expr) {
    */
   private void createLeaf(PredicateLeaf.Operator operator,
                           ExprNodeGenericFuncDesc expression) {
-    createLeaf(operator, expression, findVariable(expression));
+    createLeaf(operator, expression, expression.findVariable(this));
   }
 
   private void addChildren(ExprNodeGenericFuncDesc node) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java
index 31f54c3..187e5ad 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java
@@ -28,6 +28,7 @@
 import java.util.Set;
 import java.util.Stack;
 
+import org.apache.hadoop.hive.ql.exec.tez.monitoring.TezProgressMonitor;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 import org.apache.hadoop.hive.common.type.DataTypePhysicalVariation;
@@ -283,7 +284,22 @@ public boolean getIsTestVectorizationSuppressExplainExecutionMode() {
     return isTestVectorizationSuppressExplainExecutionMode;
   }
 
-  public static class BaseExplainVectorization {
+    public String getMode(TezProgressMonitor tezProgressMonitor) {
+        String mode = "container";
+      if (this != null) {
+        // uber > llap > container
+        if (getUberMode()) {
+          mode = "uber";
+        } else if (getLlapMode()) {
+          mode = "llap";
+        } else {
+          mode = "container";
+        }
+      }
+      return mode;
+    }
+
+    public static class BaseExplainVectorization {
 
     private final BaseWork baseWork;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDesc.java
index 7b8c5d1..f2d83ec 100755
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeDesc.java
@@ -23,6 +23,7 @@
 import java.util.Collection;
 import java.util.List;
 
+import org.apache.hadoop.hive.ql.io.sarg.ConvertAstToSearchArg;
 import org.apache.hadoop.hive.ql.lib.Node;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
@@ -113,7 +114,30 @@ public String getName() {
     return this.getClass().getName();
   }
 
-  // This wraps an instance of an ExprNodeDesc, and makes equals work like isSame, see comment on
+    /**
+     * Find the variable in the expression.
+     *
+     * @param convertAstToSearchArg@return the index of the variable or -1 if there is not exactly one
+     *   variable.
+     */
+    public int findVariable(ConvertAstToSearchArg convertAstToSearchArg) {
+        int result = -1;
+      List<ExprNodeDesc> children = getChildren();
+      for(int i = 0; i < children.size(); ++i) {
+        ExprNodeDesc child = children.get(i);
+        if (child instanceof ExprNodeColumnDesc) {
+          // if we already found a variable, this isn't a sarg
+          if (result != -1) {
+            return -1;
+          } else {
+            result = i;
+          }
+        }
+      }
+      return result;
+    }
+
+    // This wraps an instance of an ExprNodeDesc, and makes equals work like isSame, see comment on
   // isSame
   public final static class ExprNodeDescEqualityWrapper {
     private final ExprNodeDesc exprNodeDesc;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/ImportTableDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/ImportTableDesc.java
index 19534d1..0212189 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/ImportTableDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/ImportTableDesc.java
@@ -24,21 +24,26 @@
 import java.util.Map;
 
 import com.google.common.collect.ImmutableSet;
+import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.TableType;
-import org.apache.hadoop.hive.metastore.api.FieldSchema;
-import org.apache.hadoop.hive.metastore.api.Order;
+import org.apache.hadoop.hive.metastore.Warehouse;
+import org.apache.hadoop.hive.metastore.api.*;
 import org.apache.hadoop.hive.ql.ddl.DDLWork2;
 import org.apache.hadoop.hive.ql.ddl.table.creation.CreateTableDesc;
+import org.apache.hadoop.hive.ql.ddl.table.partition.AlterTableAddPartitionDesc;
 import org.apache.hadoop.hive.ql.ddl.view.CreateViewDesc;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.TaskFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent;
+import org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils;
 import org.apache.hadoop.hive.ql.hooks.ReadEntity;
 import org.apache.hadoop.hive.ql.hooks.WriteEntity;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer;
+import org.apache.hadoop.hive.ql.parse.EximUtil;
 import org.apache.hadoop.hive.ql.parse.ReplicationSpec;
 import org.apache.hadoop.hive.ql.parse.SemanticException;
 
@@ -52,7 +57,52 @@
   private CreateTableDesc createTblDesc = null;
   private CreateViewDesc createViewDesc = null;
 
-  public enum TYPE { TABLE, VIEW };
+    public AlterTableAddPartitionDesc partitionDesc(Path fromPath,
+                                                    Partition partition, FSTableEvent fsTableEvent) throws SemanticException {
+        try {
+        AlterTableAddPartitionDesc partsDesc =
+            new AlterTableAddPartitionDesc(getDatabaseName(), getTableName(),
+                EximUtil.makePartSpec(getPartCols(), partition.getValues()),
+                partition.getSd().getLocation(), partition.getParameters());
+        AlterTableAddPartitionDesc.PartitionDesc partDesc = partsDesc.getPartition(0);
+        partDesc.setInputFormat(partition.getSd().getInputFormat());
+        partDesc.setOutputFormat(partition.getSd().getOutputFormat());
+        partDesc.setNumBuckets(partition.getSd().getNumBuckets());
+        partDesc.setCols(partition.getSd().getCols());
+        partDesc.setSerializationLib(partition.getSd().getSerdeInfo().getSerializationLib());
+        partDesc.setSerdeParams(partition.getSd().getSerdeInfo().getParameters());
+        partDesc.setBucketCols(partition.getSd().getBucketCols());
+        partDesc.setSortCols(partition.getSd().getSortCols());
+        if (isExternal() && !fsTableEvent.replicationSpec().isMigratingToExternalTable()) {
+          // we have to provide the source location so target location can be derived.
+          partDesc.setLocation(partition.getSd().getLocation());
+        } else {
+          /**
+           * this is required for file listing of all files in a partition for managed table as described in
+           * {@link org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator}
+           */
+          partDesc.setLocation(new Path(fromPath,
+              Warehouse.makePartName(getPartCols(), partition.getValues())).toString());
+        }
+        partsDesc.setReplicationSpec(fsTableEvent.replicationSpec());
+
+        if (partition.isSetColStats()) {
+          ColumnStatistics colStats = partition.getColStats();
+          ColumnStatisticsDesc colStatsDesc = new ColumnStatisticsDesc(colStats.getStatsDesc());
+          colStatsDesc.setTableName(getTableName());
+          colStatsDesc.setDbName(getDatabaseName());
+          partDesc.setColStats(new ColumnStatistics(colStatsDesc, colStats.getStatsObj()));
+          long writeId = fsTableEvent.replicationSpec().isMigratingToTxnTable() ?
+                  ReplUtils.REPL_BOOTSTRAP_MIGRATION_BASE_WRITE_ID : partition.getWriteId();
+          partDesc.setWriteId(writeId);
+        }
+        return partsDesc;
+      } catch (Exception e) {
+        throw new SemanticException(e);
+      }
+    }
+
+    public enum TYPE { TABLE, VIEW };
 
   public ImportTableDesc(String dbName, Table table) throws Exception {
     this.dbName = dbName;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
index bb063c5..cce0897 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.plan;
 
 import org.apache.hadoop.hive.common.StringInternUtils;
+import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 
 import java.util.ArrayList;
@@ -42,20 +43,29 @@
 import org.apache.hadoop.hive.ql.exec.IConfigureJobConf;
 import org.apache.hadoop.hive.ql.exec.Operator;
 import org.apache.hadoop.hive.ql.exec.OperatorUtils;
+import org.apache.hadoop.hive.ql.exec.mr.ExecMapper;
+import org.apache.hadoop.hive.ql.exec.tez.DagUtils;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedSupport.Support;
 import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;
 import org.apache.hadoop.hive.ql.io.AcidUtils;
+import org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
+import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
 import org.apache.hadoop.hive.ql.io.HiveInputFormat;
+import org.apache.hadoop.hive.ql.io.merge.MergeFileMapper;
+import org.apache.hadoop.hive.ql.io.merge.MergeFileOutputFormat;
+import org.apache.hadoop.hive.ql.io.merge.MergeFileWork;
 import org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.BucketCol;
 import org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.SortCol;
 import org.apache.hadoop.hive.ql.optimizer.physical.VectorizerReason;
 import org.apache.hadoop.hive.ql.parse.SplitSample;
 import org.apache.hadoop.hive.ql.plan.Explain.Level;
 import org.apache.hadoop.hive.ql.plan.Explain.Vectorization;
+import org.apache.hadoop.mapred.FileOutputFormat;
 import org.apache.hadoop.mapred.JobConf;
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.Interner;
+import org.apache.tez.mapreduce.hadoop.MRJobConfig;
 
 /**
  * MapWork represents all the information used to run a map task on the cluster.
@@ -73,7 +83,72 @@
 @SuppressWarnings({"serial"})
 public class MapWork extends BaseWork {
 
-  public enum LlapIODescriptor {
+    /*
+     * Creates the configuration object necessary to run a specific vertex from
+     * map work. This includes input formats, input processor, etc.
+     */
+    public JobConf initializeVertexConf(JobConf baseConf, Context context, DagUtils dagUtils) {
+        JobConf conf = new JobConf(baseConf);
+
+      conf.set(Operator.CONTEXT_NAME_KEY, getName());
+
+      if (getNumMapTasks() != null) {
+        // Is this required ?
+        conf.setInt(MRJobConfig.NUM_MAPS, getNumMapTasks().intValue());
+      }
+
+      if (getMaxSplitSize() != null) {
+        HiveConf.setLongVar(conf, ConfVars.MAPREDMAXSPLITSIZE,
+            getMaxSplitSize().longValue());
+      }
+
+      if (getMinSplitSize() != null) {
+        HiveConf.setLongVar(conf, ConfVars.MAPREDMINSPLITSIZE,
+            getMinSplitSize().longValue());
+      }
+
+      if (getMinSplitSizePerNode() != null) {
+        HiveConf.setLongVar(conf, ConfVars.MAPREDMINSPLITSIZEPERNODE,
+            getMinSplitSizePerNode().longValue());
+      }
+
+      if (getMinSplitSizePerRack() != null) {
+        HiveConf.setLongVar(conf, ConfVars.MAPREDMINSPLITSIZEPERRACK,
+            getMinSplitSizePerRack().longValue());
+      }
+
+      Utilities.setInputAttributes(conf, this);
+
+      String inpFormat = HiveConf.getVar(conf, ConfVars.HIVETEZINPUTFORMAT);
+
+      if (isUseBucketizedHiveInputFormat()) {
+        inpFormat = BucketizedHiveInputFormat.class.getName();
+      }
+
+      if (getDummyTableScan()) {
+        // hive input format doesn't handle the special condition of no paths + 1
+        // split correctly.
+        inpFormat = CombineHiveInputFormat.class.getName();
+      }
+
+      conf.set(DagUtils.TEZ_TMP_DIR_KEY, context.getMRTmpPath().toUri().toString());
+      conf.set("mapred.mapper.class", ExecMapper.class.getName());
+      conf.set("mapred.input.format.class", inpFormat);
+
+      if (this instanceof MergeFileWork) {
+        MergeFileWork mfWork = (MergeFileWork) this;
+        // This mapper class is used for serializaiton/deserializaiton of merge
+        // file work.
+        conf.set("mapred.mapper.class", MergeFileMapper.class.getName());
+        conf.set("mapred.input.format.class", mfWork.getInputformat());
+        conf.setClass("mapred.output.format.class", MergeFileOutputFormat.class,
+            FileOutputFormat.class);
+      }
+
+      return conf;
+    }
+
+    public enum LlapIODescriptor {
     DISABLED(null, false),
     NO_INPUTS("no inputs", false),
     UNKNOWN("unknown", false),
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicateTransitivePropagate.java b/ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicateTransitivePropagate.java
index 35f9c7b..8b765c1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicateTransitivePropagate.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/ppd/PredicateTransitivePropagate.java
@@ -31,7 +31,6 @@
 import org.apache.hadoop.hive.ql.exec.FilterOperator;
 import org.apache.hadoop.hive.ql.exec.JoinOperator;
 import org.apache.hadoop.hive.ql.exec.Operator;
-import org.apache.hadoop.hive.ql.exec.OperatorFactory;
 import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;
 import org.apache.hadoop.hive.ql.exec.RowSchema;
 import org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher;
@@ -96,26 +95,14 @@ public ParseContext transform(ParseContext pctx) throws SemanticException {
       } else {
         ExprNodeDesc merged = ExprNodeDescUtils.mergePredicates(exprs);
         RowSchema parentRS = parent.getSchema();
-        Operator<FilterDesc> newFilter = createFilter(reducer, parent, parentRS, merged);
+        Operator<FilterDesc> newFilter = parentRS.createFilter(reducer, parent, merged, this);
       }
     }
 
     return pGraphContext;
   }
 
-  // insert filter operator between target(child) and input(parent)
-  private Operator<FilterDesc> createFilter(Operator<?> target, Operator<?> parent,
-      RowSchema parentRS, ExprNodeDesc filterExpr) {
-    Operator<FilterDesc> filter = OperatorFactory.get(parent.getCompilationOpContext(),
-        new FilterDesc(filterExpr, false), new RowSchema(parentRS.getSignature()));
-    filter.getParentOperators().add(parent);
-    filter.getChildOperators().add(target);
-    parent.replaceChild(target, filter);
-    target.replaceParent(parent, filter);
-    return filter;
-  }
-
-  private static class TransitiveContext implements NodeProcessorCtx {
+    private static class TransitiveContext implements NodeProcessorCtx {
 
     private final Map<CommonJoinOperator, int[][]> filterPropagates;
     private final Map<ReduceSinkOperator, List<ExprNodeDesc>> newFilters;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index 9d631ed..abb6bba 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -81,6 +81,7 @@
 import org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManagerImpl;
 import org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager;
 import org.apache.hadoop.hive.ql.exec.tez.TezSessionState;
+import org.apache.hadoop.hive.ql.exec.tez.TezTask;
 import org.apache.hadoop.hive.ql.history.HiveHistory;
 import org.apache.hadoop.hive.ql.history.HiveHistoryImpl;
 import org.apache.hadoop.hive.ql.history.HiveHistoryProxyHandler;
@@ -216,7 +217,16 @@
 
   private KillQuery killQuery;
 
-  public enum AuthorizationMode{V1, V2};
+    public String getUserNameForGroups(TezTask tezTask) {
+        // This should be removed when authenticator and the 2-username mess is cleaned up.
+      if (getAuthenticator() != null) {
+        String userName = getAuthenticator().getUserName();
+        if (userName != null) return userName;
+      }
+      return getUserName();
+    }
+
+    public enum AuthorizationMode{V1, V2};
 
   private HiveAuthenticationProvider authenticator;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/ValueBoundaryScanner.java b/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/ValueBoundaryScanner.java
index 524812f..9583b0f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/ValueBoundaryScanner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/ValueBoundaryScanner.java
@@ -107,7 +107,7 @@ public void handleCache(int rowIdx, PTFPartition p) throws HiveException {
           }
         } else {
           //Starting from current row, all previous ranges can be evicted.
-          checkIfCacheCanEvict(rowIdx, p, true);
+          p.checkIfCacheCanEvict(rowIdx, true, this);
         }
       } else {
         //We either evict when we're at the end of currently known ranges, or if not there yet and
@@ -117,51 +117,24 @@ public void handleCache(int rowIdx, PTFPartition p) throws HiveException {
           cache.evictOne();
         } else if (end.isFollowing()) {
           int startIdx = computeStart(rowIdx, p);
-          checkIfCacheCanEvict(startIdx - 1, p, true);
+          p.checkIfCacheCanEvict(startIdx - 1, true, this);
         }
       }
     }
 
     if (start.isCurrentRow()) {
       //Starting from current row, all previous ranges before the previous range can be evicted.
-      checkIfCacheCanEvict(rowIdx, p, false);
+      p.checkIfCacheCanEvict(rowIdx, false, this);
     }
     if (start.isFollowing()) {
       //Starting from current row, all previous ranges can be evicted.
-      checkIfCacheCanEvict(rowIdx, p, true);
+      p.checkIfCacheCanEvict(rowIdx, true, this);
     }
 
     fillCacheUntilEndOrFull(rowIdx, p);
   }
 
-  /**
-   * Retrieves the range for rowIdx, then removes all previous range entries before it.
-   * @param rowIdx row index.
-   * @param p partition.
-   * @param willScanFwd false: removal is started only from the previous previous range.
-   */
-  private void checkIfCacheCanEvict(int rowIdx, PTFPartition p, boolean willScanFwd) {
-    BoundaryCache cache = p.getBoundaryCache();
-    if (cache == null) {
-      return;
-    }
-    Map.Entry<Integer, Object> floorEntry = cache.floorEntry(rowIdx);
-    if (floorEntry != null) {
-      floorEntry = cache.floorEntry(floorEntry.getKey() - 1);
-      if (floorEntry != null) {
-        if (willScanFwd) {
-          cache.evictThisAndAllBefore(floorEntry.getKey());
-        } else {
-          floorEntry = cache.floorEntry(floorEntry.getKey() - 1);
-          if (floorEntry != null) {
-            cache.evictThisAndAllBefore(floorEntry.getKey());
-          }
-        }
-      }
-    }
-  }
-
-  /**
+    /**
    * Inserts values into cache starting from rowIdx in the current partition p. Stops if cache
    * reaches its maximum size or we get out of rows in p.
    * @param rowIdx
