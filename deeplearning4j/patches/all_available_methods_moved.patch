diff --git a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/MinimalSameDiffDense.java b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/MinimalSameDiffDense.java
index 1b8e7de..9db893a 100644
--- a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/MinimalSameDiffDense.java
+++ b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/MinimalSameDiffDense.java
@@ -54,7 +54,7 @@ public class MinimalSameDiffDense extends SameDiffLayer {
 
         SDVariable mmul = sd.mmul("mmul", layerInput, weights);
         SDVariable z = mmul.add("z", bias);
-        return activation.asSameDiff("out", sd, z);
+        return sd.asSameDiff("out", z, activation);
     }
 
     @Override
diff --git a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffConv.java b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffConv.java
index 778b95d..a59fb35 100644
--- a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffConv.java
+++ b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffConv.java
@@ -144,7 +144,7 @@ public class SameDiffConv extends SameDiffLayer {
 
         SDVariable conv = sameDiff.cnn().conv2d(vars, c);    //TODO can't set name
 
-        return activation.asSameDiff("out", sameDiff, conv);
+        return sameDiff.asSameDiff("out", conv, activation);
     }
 
     @Override
diff --git a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffDense.java b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffDense.java
index 3da6e8f..e9d65f3 100644
--- a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffDense.java
+++ b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffDense.java
@@ -104,7 +104,7 @@ public class SameDiffDense extends SameDiffLayer {
 
         SDVariable mmul = sd.mmul("mmul", layerInput, weights);
         SDVariable z = mmul.add("z", bias);
-        return activation.asSameDiff("out", sd, z);
+        return sd.asSameDiff("out", z, activation);
     }
 
     @Override
diff --git a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffDenseVertex.java b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffDenseVertex.java
index 3e3631d..f2f38ee 100644
--- a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffDenseVertex.java
+++ b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffDenseVertex.java
@@ -53,7 +53,7 @@ public class SameDiffDenseVertex extends SameDiffVertex {
 
         SDVariable mmul = sameDiff.mmul("mmul", layerInput.get("in"), weights);
         SDVariable z = mmul.add("z", bias);
-        return activation.asSameDiff("out", sameDiff, z);
+        return sameDiff.asSameDiff("out", z, activation);
     }
 
     @Override
diff --git a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffMSEOutputLayer.java b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffMSEOutputLayer.java
index cfead64..4d11655 100644
--- a/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffMSEOutputLayer.java
+++ b/deeplearning4j/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/layers/samediff/testlayers/SameDiffMSEOutputLayer.java
@@ -46,7 +46,7 @@ public class SameDiffMSEOutputLayer extends SameDiffOutputLayer {
     @Override
     public SDVariable defineLayer(SameDiff sameDiff, SDVariable layerInput, SDVariable labels, Map<String, SDVariable> paramTable) {
         SDVariable z = sameDiff.mmul(layerInput, paramTable.get("W")).add(paramTable.get("b"));
-        SDVariable out = activation.asSameDiff("out", sameDiff, z);
+        SDVariable out = sameDiff.asSameDiff("out", z, activation);
         //MSE: 1/nOut * (input-labels)^2
         SDVariable diff = out.sub(labels);
         return diff.mul(diff).mean(1).sum();
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/advanced/activations/KerasPReLU.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/advanced/activations/KerasPReLU.java
index 15de6fc..a8d3ccd 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/advanced/activations/KerasPReLU.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/advanced/activations/KerasPReLU.java
@@ -86,7 +86,7 @@ public class KerasPReLU extends KerasLayer {
         long[] axes = getSharedAxes(layerConfig);
 
         PReLULayer.Builder builder = new PReLULayer.Builder().sharedAxes(axes)
-        .weightInit(weightInit.getWeightInitFunction(distribution)).name(layerName);
+        .weightInit(distribution.getWeightInitFunction(weightInit)).name(layerName);
         if (weightConstraint != null){
             builder.constrainWeights(weightConstraint);
         }
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasAtrousConvolution1D.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasAtrousConvolution1D.java
index b7fa269..d4b8592 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasAtrousConvolution1D.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasAtrousConvolution1D.java
@@ -91,7 +91,7 @@ public class KerasAtrousConvolution1D extends KerasConvolution {
         Convolution1DLayer.Builder builder = new Convolution1DLayer.Builder().name(this.layerName)
                 .nOut(getNOutFromConfig(layerConfig, conf)).dropOut(this.dropout)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(weightInit.getWeightInitFunction(distribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
                 .dilation(getDilationRate(layerConfig, 1, conf, true)[0])
                 .l1(this.weightL1Regularization).l2(this.weightL2Regularization)
                 .convolutionMode(getConvolutionModeFromConfig(layerConfig, conf))
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution1D.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution1D.java
index 33512eb..53cde1d 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution1D.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution1D.java
@@ -102,7 +102,7 @@ public class KerasConvolution1D extends KerasConvolution {
         Convolution1DLayer.Builder builder = new Convolution1DLayer.Builder().name(this.layerName)
                 .nOut(getNOutFromConfig(layerConfig, conf)).dropOut(this.dropout)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(weightInit.getWeightInitFunction(distribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
                 .l1(this.weightL1Regularization).l2(this.weightL2Regularization)
                 .convolutionMode(getConvolutionModeFromConfig(layerConfig, conf))
                 .kernelSize(getKernelSizeFromConfig(layerConfig, 1,  conf, kerasMajorVersion)[0])
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution2D.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution2D.java
index 3c1d9f7..bb9d071 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution2D.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution2D.java
@@ -100,7 +100,7 @@ public class KerasConvolution2D extends KerasConvolution {
         ConvolutionLayer.Builder builder = new ConvolutionLayer.Builder().name(this.layerName)
                 .nOut(getNOutFromConfig(layerConfig, conf)).dropOut(this.dropout)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(weightInit.getWeightInitFunction(distribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
                 .l1(this.weightL1Regularization).l2(this.weightL2Regularization)
                 .convolutionMode(getConvolutionModeFromConfig(layerConfig, conf))
                 .kernelSize(getKernelSizeFromConfig(layerConfig, 2, conf, kerasMajorVersion))
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution3D.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution3D.java
index 8da12a7..3030e3e 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution3D.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasConvolution3D.java
@@ -101,7 +101,7 @@ public class KerasConvolution3D extends KerasConvolution {
         Convolution3D.Builder builder = new Convolution3D.Builder().name(this.layerName)
                 .nOut(getNOutFromConfig(layerConfig, conf)).dropOut(this.dropout)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(weightInit.getWeightInitFunction(distribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
                 .l1(this.weightL1Regularization).l2(this.weightL2Regularization)
                 .convolutionMode(getConvolutionModeFromConfig(layerConfig, conf))
                 .kernelSize(getKernelSizeFromConfig(layerConfig, 3, conf, kerasMajorVersion))
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasDeconvolution2D.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasDeconvolution2D.java
index 33e02ae..e2b16fd 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasDeconvolution2D.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasDeconvolution2D.java
@@ -99,7 +99,7 @@ public class KerasDeconvolution2D extends KerasConvolution {
         Deconvolution2D.Builder builder = new Deconvolution2D.Builder().name(this.layerName)
                 .nOut(getNOutFromConfig(layerConfig, conf)).dropOut(this.dropout)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(weightInit.getWeightInitFunction(distribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
                 .l1(this.weightL1Regularization).l2(this.weightL2Regularization)
                 .convolutionMode(getConvolutionModeFromConfig(layerConfig, conf))
                 .kernelSize(getKernelSizeFromConfig(layerConfig, 2, conf, kerasMajorVersion))
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasDepthwiseConvolution2D.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasDepthwiseConvolution2D.java
index f27d3ff..2eae8ca 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasDepthwiseConvolution2D.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasDepthwiseConvolution2D.java
@@ -152,7 +152,7 @@ public class KerasDepthwiseConvolution2D extends KerasConvolution {
                 .nIn(nIn)
                 .nOut(nIn * depthMultiplier)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(depthWeightInit.getWeightInitFunction(depthDistribution))
+                .weightInit(depthDistribution.getWeightInitFunction(depthWeightInit))
                 .depthMultiplier(depthMultiplier)
                 .l1(this.weightL1Regularization).l2(this.weightL2Regularization)
                 .convolutionMode(getConvolutionModeFromConfig(layerConfig, conf))
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasSeparableConvolution2D.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasSeparableConvolution2D.java
index 67eba9b..76f137d 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasSeparableConvolution2D.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/convolutional/KerasSeparableConvolution2D.java
@@ -126,7 +126,7 @@ public class KerasSeparableConvolution2D extends KerasConvolution {
         SeparableConvolution2D.Builder builder = new SeparableConvolution2D.Builder().name(this.layerName)
                 .nOut(getNOutFromConfig(layerConfig, conf)).dropOut(this.dropout)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(depthWeightInit.getWeightInitFunction(depthDistribution))
+                .weightInit(depthDistribution.getWeightInitFunction(depthWeightInit))
                 .depthMultiplier(depthMultiplier)
                 .l1(this.weightL1Regularization).l2(this.weightL2Regularization)
                 .convolutionMode(getConvolutionModeFromConfig(layerConfig, conf))
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/core/KerasDense.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/core/KerasDense.java
index d840370..9b4dd37 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/core/KerasDense.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/core/KerasDense.java
@@ -103,7 +103,7 @@ public class KerasDense extends KerasLayer {
         DenseLayer.Builder builder = new DenseLayer.Builder().name(this.layerName)
                 .nOut(getNOutFromConfig(layerConfig, conf))
                 .dropOut(this.dropout).activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(weightInit.getWeightInitFunction(distribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
                 .biasInit(0.0)
                 .l1(this.weightL1Regularization).l2(this.weightL2Regularization)
                 .hasBias(hasBias);
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/embeddings/KerasEmbedding.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/embeddings/KerasEmbedding.java
index 6bc1c41..4921c8a 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/embeddings/KerasEmbedding.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/embeddings/KerasEmbedding.java
@@ -121,7 +121,7 @@ public class KerasEmbedding extends KerasLayer {
                 .inferInputLength(inferInputLength)
                 .nOut(getNOutFromConfig(layerConfig, conf))
                 .dropOut(this.dropout).activation(Activation.IDENTITY)
-                .weightInit(weightInit.getWeightInitFunction(distribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
                 .biasInit(0.0)
                 .l1(this.weightL1Regularization)
                 .l2(this.weightL2Regularization)
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/recurrent/KerasLSTM.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/recurrent/KerasLSTM.java
index f047529..bb9270a 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/recurrent/KerasLSTM.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/recurrent/KerasLSTM.java
@@ -186,8 +186,8 @@ public class KerasLSTM extends KerasLayer {
                 .nOut(getNOutFromConfig(layerConfig, conf))
                 .dropOut(this.dropout)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(weightInit.getWeightInitFunction(distribution))
-                .weightInitRecurrent(recurrentWeightInit.getWeightInitFunction(recurrentDistribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
+                .weightInitRecurrent(recurrentDistribution.getWeightInitFunction(recurrentWeightInit))
                 .biasInit(0.0) // TODO: this is incorrect
                 .l1(this.weightL1Regularization)
                 .l2(this.weightL2Regularization);
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/recurrent/KerasSimpleRnn.java b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/recurrent/KerasSimpleRnn.java
index 615405f..9fb648f 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/recurrent/KerasSimpleRnn.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/main/java/org/deeplearning4j/nn/modelimport/keras/layers/recurrent/KerasSimpleRnn.java
@@ -154,8 +154,8 @@ public class KerasSimpleRnn extends KerasLayer {
                 .nOut(getNOutFromConfig(layerConfig, conf))
                 .dropOut(this.dropout)
                 .activation(getIActivationFromConfig(layerConfig, conf))
-                .weightInit(weightInit.getWeightInitFunction(distribution))
-                .weightInitRecurrent(recurrentWeightInit.getWeightInitFunction(recurrentDistribution))
+                .weightInit(distribution.getWeightInitFunction(weightInit))
+                .weightInitRecurrent(recurrentDistribution.getWeightInitFunction(recurrentWeightInit))
                 .biasInit(0.0)
                 .l1(this.weightL1Regularization)
                 .l2(this.weightL2Regularization);
diff --git a/deeplearning4j/deeplearning4j-modelimport/src/test/java/org/deeplearning4j/nn/modelimport/keras/configurations/KerasInitilizationTest.java b/deeplearning4j/deeplearning4j-modelimport/src/test/java/org/deeplearning4j/nn/modelimport/keras/configurations/KerasInitilizationTest.java
index a3f90b2..f0b6ffb 100644
--- a/deeplearning4j/deeplearning4j-modelimport/src/test/java/org/deeplearning4j/nn/modelimport/keras/configurations/KerasInitilizationTest.java
+++ b/deeplearning4j/deeplearning4j-modelimport/src/test/java/org/deeplearning4j/nn/modelimport/keras/configurations/KerasInitilizationTest.java
@@ -88,15 +88,15 @@ public class KerasInitilizationTest {
                 WeightInit.XAVIER_UNIFORM.getWeightInitFunction(),
                 WeightInit.LECUN_NORMAL.getWeightInitFunction(),
                 WeightInit.LECUN_UNIFORM.getWeightInitFunction(),
-                WeightInit.DISTRIBUTION.getWeightInitFunction(new UniformDistribution(minValue, maxValue)),
+                new UniformDistribution(minValue, maxValue).getWeightInitFunction(WeightInit.DISTRIBUTION),
                 WeightInit.RELU.getWeightInitFunction(),
                 WeightInit.RELU_UNIFORM.getWeightInitFunction(),
                 WeightInit.ONES.getWeightInitFunction(),
                 WeightInit.ZERO.getWeightInitFunction(),
                 WeightInit.IDENTITY.getWeightInitFunction(),
-                WeightInit.DISTRIBUTION.getWeightInitFunction(new NormalDistribution(mean, stdDev)),
-                WeightInit.DISTRIBUTION.getWeightInitFunction(new OrthogonalDistribution(gain)),
-                WeightInit.DISTRIBUTION.getWeightInitFunction(new ConstantDistribution(value)),
+                new NormalDistribution(mean, stdDev).getWeightInitFunction(WeightInit.DISTRIBUTION),
+                new OrthogonalDistribution(gain).getWeightInitFunction(WeightInit.DISTRIBUTION),
+                new ConstantDistribution(value).getWeightInitFunction(WeightInit.DISTRIBUTION),
                 WeightInit.VAR_SCALING_NORMAL_FAN_IN.getWeightInitFunction()};
     }
 
diff --git a/deeplearning4j/deeplearning4j-nearestneighbors-parent/nearestneighbor-core/src/main/java/org/deeplearning4j/clustering/cluster/Cluster.java b/deeplearning4j/deeplearning4j-nearestneighbors-parent/nearestneighbor-core/src/main/java/org/deeplearning4j/clustering/cluster/Cluster.java
index c9b1d80..21fd53a 100644
--- a/deeplearning4j/deeplearning4j-nearestneighbors-parent/nearestneighbor-core/src/main/java/org/deeplearning4j/clustering/cluster/Cluster.java
+++ b/deeplearning4j/deeplearning4j-nearestneighbors-parent/nearestneighbor-core/src/main/java/org/deeplearning4j/clustering/cluster/Cluster.java
@@ -148,4 +148,9 @@ public class Cluster implements Serializable {
     }
 
 
+    public boolean isPointLocationChange(Point point, ClusterSet clusterSet) {
+        if (!getPointDistribution().containsKey(point.getId()))
+            return true;
+        return !getPointDistribution().get(point.getId()).equals(getId());
+    }
 }
diff --git a/deeplearning4j/deeplearning4j-nearestneighbors-parent/nearestneighbor-core/src/main/java/org/deeplearning4j/clustering/cluster/ClusterSet.java b/deeplearning4j/deeplearning4j-nearestneighbors-parent/nearestneighbor-core/src/main/java/org/deeplearning4j/clustering/cluster/ClusterSet.java
index 1275b88..542cb29 100644
--- a/deeplearning4j/deeplearning4j-nearestneighbors-parent/nearestneighbor-core/src/main/java/org/deeplearning4j/clustering/cluster/ClusterSet.java
+++ b/deeplearning4j/deeplearning4j-nearestneighbors-parent/nearestneighbor-core/src/main/java/org/deeplearning4j/clustering/cluster/ClusterSet.java
@@ -18,7 +18,6 @@ package org.deeplearning4j.clustering.cluster;
 
 import lombok.Data;
 import org.deeplearning4j.clustering.algorithm.Distance;
-import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.factory.Nd4j;
 import org.nd4j.linalg.primitives.Pair;
 
@@ -101,17 +100,11 @@ public class ClusterSet implements Serializable {
     public PointClassification classifyPoint(Point point, boolean moveClusterCenter) {
         Pair<Cluster, Double> nearestCluster = nearestCluster(point);
         Cluster newCluster = nearestCluster.getKey();
-        boolean locationChange = isPointLocationChange(point, newCluster);
+        boolean locationChange = newCluster.isPointLocationChange(point, this);
         addPointToCluster(point, newCluster, moveClusterCenter);
         return new PointClassification(nearestCluster.getKey(), nearestCluster.getValue(), locationChange);
     }
 
-    private boolean isPointLocationChange(Point point, Cluster newCluster) {
-        if (!getPointDistribution().containsKey(point.getId()))
-            return true;
-        return !getPointDistribution().get(point.getId()).equals(newCluster.getId());
-    }
-
     private void addPointToCluster(Point point, Cluster cluster, boolean moveClusterCenter) {
         cluster.addPoint(point, moveClusterCenter);
         setPointLocation(point, cluster);
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/Model.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/Model.java
index 410b623..b490067 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/Model.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/Model.java
@@ -3,6 +3,7 @@ package org.ansj.app.crf;
 import org.ansj.app.crf.model.CRFModel;
 import org.ansj.app.crf.model.CRFppTxtModel;
 import org.ansj.app.crf.model.WapitiCRFModel;
+import org.ansj.app.crf.pojo.Element;
 import org.nlpcn.commons.lang.tire.domain.SmartForest;
 import org.nlpcn.commons.lang.util.MapCount;
 import org.nlpcn.commons.lang.util.logging.Log;
@@ -175,4 +176,30 @@ public abstract class Model {
             logger.warn("IO异常", e);
         }
     }
+
+    public void maxFrom(Element element, Element element) {
+        if (element.from == null) {
+            element.from = new int[Config.TAG_NUM];
+        }
+        float[] pTagScore = element.tagScore;
+        for (int i = 0; i < Config.TAG_NUM; i++) {
+            float maxValue = 0;
+            for (int j = 0; j < Config.TAG_NUM; j++) {
+
+                float value = (pTagScore[j] + element.tagScore[i]) + tagRate(j, i);
+
+                if (element.tagScore.length > Config.TAG_NUM) {
+                    value += element.tagScore[Config.TAG_NUM + j * Config.TAG_NUM + i];
+                }
+
+                if (value > maxValue) {
+                    maxValue = value;
+                    element.from[i] = j;
+                }
+
+            }
+
+            element.tagScore[i] = maxValue;
+        }
+    }
 }
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/SplitWord.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/SplitWord.java
index 59fc2a6..a664a90 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/SplitWord.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/SplitWord.java
@@ -88,7 +88,7 @@ public class SplitWord {
         elements.get(0).tagScore[3] = -1000;
 
         for (int i = 1; i < length; i++) {
-            elements.get(i).maxFrom(model, elements.get(i - 1));
+            model.maxFrom(elements.get(i - 1), elements.get(i));
         }
 
         // 末位置只能从S,E开始
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/pojo/Element.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/pojo/Element.java
index 98b5082..d37e52b 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/pojo/Element.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/app/crf/pojo/Element.java
@@ -1,8 +1,5 @@
 package org.ansj.app.crf.pojo;
 
-import org.ansj.app.crf.Config;
-import org.ansj.app.crf.Model;
-
 public class Element {
 
     public char name;
@@ -61,30 +58,4 @@ public class Element {
         }
     }
 
-    public void maxFrom(Model model, Element element) {
-        if (from == null) {
-            from = new int[Config.TAG_NUM];
-        }
-        float[] pTagScore = element.tagScore;
-        for (int i = 0; i < Config.TAG_NUM; i++) {
-            float maxValue = 0;
-            for (int j = 0; j < Config.TAG_NUM; j++) {
-
-                float value = (pTagScore[j] + tagScore[i]) + model.tagRate(j, i);
-
-                if (tagScore.length > Config.TAG_NUM) {
-                    value += tagScore[Config.TAG_NUM + j * Config.TAG_NUM + i];
-                }
-
-                if (value > maxValue) {
-                    maxValue = value;
-                    from[i] = j;
-                }
-
-            }
-
-            tagScore[i] = maxValue;
-        }
-    }
-
 }
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/domain/Result.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/domain/Result.java
index 097269f..dd8f237 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/domain/Result.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/domain/Result.java
@@ -1,6 +1,8 @@
 package org.ansj.domain;
 
 import org.ansj.recognition.Recognition;
+import org.ansj.recognition.impl.DicRecognition;
+import org.nlpcn.commons.lang.tire.domain.Forest;
 import org.nlpcn.commons.lang.util.StringUtil;
 
 import java.util.Iterator;
@@ -91,4 +93,8 @@ public class Result implements Iterable<Term> {
         return sb.toString();
     }
 
+    public void recognitionOther(Forest forest, DicRecognition dicRecognition) {
+        List<Term> terms = getTerms();
+
+    }
 }
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/domain/Term.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/domain/Term.java
index 6dc1253..30ffab9 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/domain/Term.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/domain/Term.java
@@ -1,6 +1,8 @@
 package org.ansj.domain;
 
+import org.ansj.util.Graph;
 import org.ansj.util.MathUtil;
+import org.ansj.util.TermUtil;
 import org.nlpcn.commons.lang.util.StringUtil;
 
 import java.io.Serializable;
@@ -297,4 +299,21 @@ public class Term implements Serializable {
         this.synonyms = synonyms;
     }
 
+    /**
+     * 增加一个词语到图中
+     *
+     * @param graph
+     */
+    public void addTerm(Graph graph) {
+        // 是否有数字
+        if (!graph.hasNum && termNatures().numAttr.numFreq > 0) {
+            graph.hasNum = true;
+        }
+        // 是否有人名
+        if (!graph.hasPerson && termNatures().personAttr.flag) {
+            graph.hasPerson = true;
+        }
+        TermUtil.insertTerm(graph.terms, this, TermUtil.InsertTermType.REPLACE);
+
+    }
 }
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/recognition/impl/DicRecognition.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/recognition/impl/DicRecognition.java
index 70d35a9..462401b 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/recognition/impl/DicRecognition.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/recognition/impl/DicRecognition.java
@@ -1,13 +1,10 @@
 package org.ansj.recognition.impl;
 
 import org.ansj.domain.Result;
-import org.ansj.domain.Term;
 import org.ansj.library.DicLibrary;
 import org.ansj.recognition.Recognition;
 import org.nlpcn.commons.lang.tire.domain.Forest;
 
-import java.util.List;
-
 /**
  * 
  * 用户自定词典识别 多本词典加入后将不再具有先后顺序,合并后统一规划.如果需要先后顺序请分别每个词典调用 Result.Recognition().Recognition() 这种方式 TODO:这种写灵活性是够了,但是速度不咋地.发愁........该不该这么写.先保留吧..也许在下一个版本中来做把
@@ -46,13 +43,8 @@ public class DicRecognition implements Recognition {
             if (forest == null) {
                 continue;
             }
-            recognition(result, forest);
+            result.recognitionOther(forest, this);
         }
     }
 
-    private void recognition(Result result, Forest forest) {
-        List<Term> terms = result.getTerms();
-
-    }
-
 }
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/splitWord/Analysis.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/splitWord/Analysis.java
index 0140b3b..c081f8a 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/splitWord/Analysis.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/splitWord/Analysis.java
@@ -139,7 +139,7 @@ public abstract class Analysis {
                 params = gw.getParams();
                 startOffe = gw.offe;
                 for (int i = 0; i < params.length; i += 2) {
-                    gp.addTerm(new Term(params[i], startOffe, new TermNatures(new TermNature(params[i + 1], 1))));
+                    new Term(params[i], startOffe, new TermNatures(new TermNature(params[i + 1], 1))).addTerm(gp);
                     startOffe += params[i].length();
                 }
             }
@@ -167,7 +167,7 @@ public abstract class Analysis {
                         end++;
                     }
                     str = WordAlert.alertEnglish(chars, start, end);
-                    gp.addTerm(new Term(str, start, TermNatures.EN));
+                    new Term(str, start, TermNatures.EN).addTerm(gp);
                     i--;
                     break;
                 case 5:
@@ -177,7 +177,7 @@ public abstract class Analysis {
                         end++;
                     }
                     str = WordAlert.alertNumber(chars, start, end);
-                    gp.addTerm(new Term(str, start, TermNatures.M));
+                    new Term(str, start, TermNatures.M).addTerm(gp);
                     i--;
                     break;
                 default:
@@ -204,18 +204,18 @@ public abstract class Analysis {
                         int len = term.getOffe() - max;
                         if (len > 0) {
                             for (; max < term.getOffe();) {
-                                gp.addTerm(new Term(String.valueOf(chars[max]), max, TermNatures.NULL));
+                                new Term(String.valueOf(chars[max]), max, TermNatures.NULL).addTerm(gp);
                                 max++;
                             }
                         }
-                        gp.addTerm(term);
+                        term.addTerm(gp);
                         max = term.toValue();
                     }
 
                     int len = end - max;
                     if (len > 0) {
                         for (; max < end;) {
-                            gp.addTerm(new Term(String.valueOf(chars[max]), max, TermNatures.NULL));
+                            new Term(String.valueOf(chars[max]), max, TermNatures.NULL).addTerm(gp);
                             max++;
                         }
                     }
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/util/Graph.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/util/Graph.java
index e90eafa..ea18529 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/util/Graph.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-chinese/src/main/java/org/ansj/util/Graph.java
@@ -5,7 +5,6 @@ import org.ansj.domain.Term;
 import org.ansj.domain.TermNatures;
 import org.ansj.library.DATDictionary;
 import org.ansj.splitWord.Analysis.Merger;
-import org.ansj.util.TermUtil.InsertTermType;
 
 import java.util.List;
 import java.util.Map;
@@ -48,24 +47,6 @@ public class Graph {
     }
 
     /**
-     * 增加一个词语到图中
-     * 
-     * @param term
-     */
-    public void addTerm(Term term) {
-        // 是否有数字
-        if (!hasNum && term.termNatures().numAttr.numFreq > 0) {
-            hasNum = true;
-        }
-        // 是否有人名
-        if (!hasPerson && term.termNatures().personAttr.flag) {
-            hasPerson = true;
-        }
-        TermUtil.insertTerm(terms, term, InsertTermType.REPLACE);
-
-    }
-
-    /**
      * 取得最优路径的root Term
      * 
      * @return
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-japanese/src/main/java/com/atilika/kuromoji/ipadic/compile/DictionaryEntry.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-japanese/src/main/java/com/atilika/kuromoji/ipadic/compile/DictionaryEntry.java
index 17ecb08..95c5453 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-japanese/src/main/java/com/atilika/kuromoji/ipadic/compile/DictionaryEntry.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-japanese/src/main/java/com/atilika/kuromoji/ipadic/compile/DictionaryEntry.java
@@ -18,6 +18,9 @@ package com.atilika.kuromoji.ipadic.compile;
 
 import com.atilika.kuromoji.dict.DictionaryEntryBase;
 
+import java.util.ArrayList;
+import java.util.List;
+
 import static com.atilika.kuromoji.dict.DictionaryField.*;
 
 public class DictionaryEntry extends DictionaryEntryBase {
@@ -99,4 +102,18 @@ public class DictionaryEntry extends DictionaryEntryBase {
     public String getPronunciation() {
         return pronunciation;
     }
+
+    public List<String> extractPosFeatures(TokenInfoDictionaryCompiler tokenInfoDictionaryCompiler) {
+        List<String> posFeatures = new ArrayList<>();
+
+        posFeatures.add(getPartOfSpeechLevel1());
+        posFeatures.add(getPartOfSpeechLevel2());
+        posFeatures.add(getPartOfSpeechLevel3());
+        posFeatures.add(getPartOfSpeechLevel4());
+
+        posFeatures.add(getConjugationType());
+        posFeatures.add(getConjugatedForm());
+
+        return posFeatures;
+    }
 }
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-japanese/src/main/java/com/atilika/kuromoji/ipadic/compile/TokenInfoDictionaryCompiler.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-japanese/src/main/java/com/atilika/kuromoji/ipadic/compile/TokenInfoDictionaryCompiler.java
index 75c031c..4cad0d2 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-japanese/src/main/java/com/atilika/kuromoji/ipadic/compile/TokenInfoDictionaryCompiler.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp-japanese/src/main/java/com/atilika/kuromoji/ipadic/compile/TokenInfoDictionaryCompiler.java
@@ -38,27 +38,13 @@ public class TokenInfoDictionaryCompiler extends TokenInfoDictionaryCompilerBase
 
     @Override
     protected GenericDictionaryEntry generateGenericDictionaryEntry(DictionaryEntry entry) {
-        List<String> pos = extractPosFeatures(entry);
+        List<String> pos = entry.extractPosFeatures(this);
         List<String> features = extractOtherFeatures(entry);
 
         return new GenericDictionaryEntry.Builder().surface(entry.getSurface()).leftId(entry.getLeftId())
                         .rightId(entry.getRightId()).wordCost(entry.getWordCost()).pos(pos).features(features).build();
     }
 
-    public List<String> extractPosFeatures(DictionaryEntry entry) {
-        List<String> posFeatures = new ArrayList<>();
-
-        posFeatures.add(entry.getPartOfSpeechLevel1());
-        posFeatures.add(entry.getPartOfSpeechLevel2());
-        posFeatures.add(entry.getPartOfSpeechLevel3());
-        posFeatures.add(entry.getPartOfSpeechLevel4());
-
-        posFeatures.add(entry.getConjugationType());
-        posFeatures.add(entry.getConjugatedForm());
-
-        return posFeatures;
-    }
-
     public List<String> extractOtherFeatures(DictionaryEntry entry) {
         List<String> otherFeatures = new ArrayList<>();
 
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectors.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectors.java
index 4c05b7c..06f2291 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectors.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectors.java
@@ -43,7 +43,6 @@ import org.deeplearning4j.models.word2vec.wordstore.VocabCache;
 import org.deeplearning4j.models.word2vec.wordstore.inmemory.AbstractCache;
 import org.deeplearning4j.text.documentiterator.*;
 import org.deeplearning4j.text.documentiterator.interoperability.DocumentIteratorConverter;
-import org.deeplearning4j.text.invertedindex.InvertedIndex;
 import org.deeplearning4j.text.sentenceiterator.SentenceIterator;
 import org.deeplearning4j.text.sentenceiterator.interoperability.SentenceIteratorConverter;
 import org.deeplearning4j.text.sentenceiterator.labelaware.LabelAwareSentenceIterator;
@@ -143,20 +142,6 @@ public class ParagraphVectors extends Word2Vec {
         this.iterator = iterator;
     }
 
-    /**
-     * This method predicts label of the document.
-     * Computes a similarity wrt the mean of the
-     * representation of words in the document
-     * @param document the document
-     * @return the word distances for each label
-     */
-    public String predict(LabelledDocument document) {
-        if (document.getReferencedContent() != null)
-            return predict(document.getReferencedContent());
-        else
-            return predict(document.getContent());
-    }
-
     public void extractLabels() {
         Collection<VocabWord> vocabWordCollection = vocab.vocabWords();
         List<VocabWord> vocabWordList = new ArrayList<>();
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/text/documentiterator/LabelledDocument.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/text/documentiterator/LabelledDocument.java
index ec583b1..0c7181bf 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/text/documentiterator/LabelledDocument.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/text/documentiterator/LabelledDocument.java
@@ -18,6 +18,7 @@ package org.deeplearning4j.text.documentiterator;
 
 import lombok.Data;
 import lombok.ToString;
+import org.deeplearning4j.models.paragraphvectors.ParagraphVectors;
 import org.deeplearning4j.models.word2vec.VocabWord;
 
 import java.util.ArrayList;
@@ -71,4 +72,17 @@ public class LabelledDocument {
         labels.add(label);
     }
 
+    /**
+     * This method predicts label of the document.
+     * Computes a similarity wrt the mean of the
+     * representation of words in the document
+     *
+     * @param paragraphVectors@return the word distances for each label
+     */
+    public String predict(ParagraphVectors paragraphVectors) {
+        if (getReferencedContent() != null)
+            return predict(getReferencedContent());
+        else
+            return predict(getContent());
+    }
 }
diff --git a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/test/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectorsTest.java b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/test/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectorsTest.java
index bd3933e..5f60e10 100644
--- a/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/test/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectorsTest.java
+++ b/deeplearning4j/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/test/java/org/deeplearning4j/models/paragraphvectors/ParagraphVectorsTest.java
@@ -20,12 +20,9 @@ package org.deeplearning4j.models.paragraphvectors;
 import lombok.NonNull;
 import lombok.extern.slf4j.Slf4j;
 import lombok.val;
-import org.deeplearning4j.models.embeddings.learning.impl.elements.CBOW;
-import org.deeplearning4j.models.embeddings.reader.impl.FlatModelUtils;
 import org.deeplearning4j.models.sequencevectors.sequence.Sequence;
 import org.deeplearning4j.models.sequencevectors.transformers.impl.SentenceTransformer;
 import org.deeplearning4j.models.sequencevectors.transformers.impl.iterables.BasicTransformerIterator;
-import org.deeplearning4j.models.sequencevectors.transformers.impl.iterables.ParallelTransformerIterator;
 import org.junit.Rule;
 import org.junit.rules.TemporaryFolder;
 import org.nd4j.linalg.io.ClassPathResource;
@@ -53,7 +50,6 @@ import org.deeplearning4j.text.sentenceiterator.interoperability.SentenceIterato
 import org.deeplearning4j.text.tokenization.tokenizer.preprocessor.CommonPreprocessor;
 import org.deeplearning4j.text.tokenization.tokenizerfactory.DefaultTokenizerFactory;
 import org.deeplearning4j.text.tokenization.tokenizerfactory.TokenizerFactory;
-import org.junit.Before;
 import org.junit.Ignore;
 import org.junit.Test;
 import org.nd4j.linalg.api.ndarray.INDArray;
@@ -62,13 +58,10 @@ import org.nd4j.linalg.io.CollectionUtils;
 import org.nd4j.linalg.ops.transforms.Transforms;
 import org.nd4j.linalg.util.SerializationUtils;
 import org.nd4j.resources.Resources;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
 
 import java.io.File;
 import java.io.IOException;
 import java.util.*;
-import java.util.concurrent.atomic.AtomicLong;
 
 import static org.junit.Assert.*;
 
@@ -815,7 +808,7 @@ public class ParagraphVectorsTest {
             log.info("Similarity to [" + result + "] is [" + sim + "]");
         }
 
-        String topPrediction = paragraphVectors.predict(document);
+        String topPrediction = document.predict(paragraphVectors);
         assertEquals("Z"+document.getLabel(), topPrediction);
     }
 
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.java
index a259c7b..904427f 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/ComputationGraphConfiguration.java
@@ -276,7 +276,7 @@ public class ComputationGraphConfiguration implements Serializable, Cloneable {
                 }
 
                 if (weightInit != null) {
-                    final IWeightInit wi = WeightInit.valueOf(weightInit.asText()).getWeightInitFunction(dist);
+                    final IWeightInit wi = dist.getWeightInitFunction(WeightInit.valueOf(weightInit.asText()));
                     ((BaseLayer) layer).setWeightInitFn(wi);
                 }
 
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/MultiLayerConfiguration.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/MultiLayerConfiguration.java
index de33733..e295017 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/MultiLayerConfiguration.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/MultiLayerConfiguration.java
@@ -26,7 +26,6 @@ import org.deeplearning4j.nn.conf.layers.recurrent.LastTimeStep;
 import org.deeplearning4j.nn.conf.memory.LayerMemoryReport;
 import org.deeplearning4j.nn.conf.memory.MemoryReport;
 import org.deeplearning4j.nn.conf.memory.NetworkMemoryReport;
-import org.deeplearning4j.nn.layers.recurrent.LastTimeStepLayer;
 import org.deeplearning4j.nn.weights.IWeightInit;
 import org.deeplearning4j.nn.weights.WeightInit;
 import org.deeplearning4j.util.OutputLayerUtil;
@@ -326,7 +325,7 @@ public class MultiLayerConfiguration implements Serializable, Cloneable {
                     }
 
                     if (weightInit != null) {
-                        final IWeightInit wi = WeightInit.valueOf(weightInit.asText()).getWeightInitFunction(dist);
+                        final IWeightInit wi = dist.getWeightInitFunction(WeightInit.valueOf(weightInit.asText()));
                         ((BaseLayer) l).setWeightInitFn(wi);
                     }
                 }
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/NeuralNetConfiguration.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/NeuralNetConfiguration.java
index d8e77a5..40456e2 100755
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/NeuralNetConfiguration.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/NeuralNetConfiguration.java
@@ -42,6 +42,7 @@ import org.deeplearning4j.nn.conf.serde.legacyformat.LegacyPreprocessorDeseriali
 import org.deeplearning4j.nn.conf.serde.legacyformat.LegacyReconstructionDistributionDeserializer;
 import org.deeplearning4j.nn.conf.stepfunctions.StepFunction;
 import org.deeplearning4j.nn.conf.weightnoise.IWeightNoise;
+import org.deeplearning4j.nn.params.PretrainParamInitializer;
 import org.deeplearning4j.nn.weights.IWeightInit;
 import org.deeplearning4j.nn.weights.WeightInit;
 import org.deeplearning4j.nn.weights.WeightInitDistribution;
@@ -52,6 +53,7 @@ import org.nd4j.linalg.activations.Activation;
 import org.nd4j.linalg.activations.IActivation;
 import org.nd4j.linalg.activations.impl.ActivationSigmoid;
 import org.nd4j.linalg.api.buffer.DataType;
+import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.factory.Nd4j;
 import org.nd4j.linalg.learning.config.IUpdater;
 import org.nd4j.linalg.learning.config.Sgd;
@@ -151,6 +153,17 @@ public class NeuralNetConfiguration implements Serializable, Cloneable {
         variables.clear();
     }
 
+    public INDArray createVisibleBias(INDArray visibleBiasView,
+                                      boolean initializeParameters, PretrainParamInitializer pretrainParamInitializer) {
+        BasePretrainNetwork layerConf =
+                        (BasePretrainNetwork) getLayer();
+        if (initializeParameters) {
+            INDArray ret = Nd4j.valueArrayOf(new long[]{1, layerConf.getNIn()}, layerConf.getVisibleBiasInit());
+            visibleBiasView.assign(ret);
+        }
+        return visibleBiasView;
+    }
+
     /**
      * Fluent interface for building a list of configurations
      */
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/distribution/Distribution.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/distribution/Distribution.java
index ae1c5af..a526f82 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/distribution/Distribution.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/distribution/Distribution.java
@@ -17,6 +17,7 @@
 package org.deeplearning4j.nn.conf.distribution;
 
 import org.deeplearning4j.nn.conf.distribution.serde.LegacyDistributionHelper;
+import org.deeplearning4j.nn.weights.*;
 import org.nd4j.shade.jackson.annotation.JsonTypeInfo;
 
 import java.io.Serializable;
@@ -39,4 +40,58 @@ public abstract class Distribution implements Serializable, Cloneable {
             throw new RuntimeException(e);
         }
     }
+
+    /**
+     * Create an instance of the weight initialization function
+     *
+     *
+     * @param weightInit@return a new {@link IWeightInit} instance
+     */
+    public IWeightInit getWeightInitFunction(WeightInit weightInit) {
+        switch (weightInit) {
+            case ZERO:
+                return new WeightInitConstant(0.0);
+            case ONES:
+                return new WeightInitConstant(1.0);
+            case DISTRIBUTION:
+                return new WeightInitDistribution(this);
+            case SIGMOID_UNIFORM:
+                return new WeightInitSigmoidUniform();
+            case LECUN_NORMAL: //Fall through: these 3 are equivalent
+            case XAVIER_FAN_IN:
+            case NORMAL:
+                return new WeightInitNormal();
+            case UNIFORM:
+                return new WeightInitUniform();
+            case XAVIER:
+                return new WeightInitXavier();
+            case XAVIER_UNIFORM:
+                return new WeightInitXavierUniform();
+            case XAVIER_LEGACY:
+                return new WeightInitXavierLegacy();
+            case RELU:
+                return new WeightInitRelu();
+            case RELU_UNIFORM:
+                return new WeightInitReluUniform();
+            case IDENTITY:
+                return new WeightInitIdentity();
+            case LECUN_UNIFORM:
+                return new WeightInitLecunUniform();
+            case VAR_SCALING_NORMAL_FAN_IN:
+                return new WeightInitVarScalingNormalFanIn();
+            case VAR_SCALING_NORMAL_FAN_OUT:
+                return new WeightInitVarScalingNormalFanOut();
+            case VAR_SCALING_NORMAL_FAN_AVG:
+                return new WeightInitVarScalingNormalFanAvg();
+            case VAR_SCALING_UNIFORM_FAN_IN:
+                return new WeightInitVarScalingUniformFanIn();
+            case VAR_SCALING_UNIFORM_FAN_OUT:
+                return new WeightInitVarScalingUniformFanOut();
+            case VAR_SCALING_UNIFORM_FAN_AVG:
+                return new WeightInitVarScalingUniformFanAvg();
+
+            default:
+                throw new UnsupportedOperationException("Unknown or not supported weight initialization function: " + weightInit);
+        }
+    }
 }
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/DepthwiseConvolution2D.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/DepthwiseConvolution2D.java
index 03fec11..ffb5036 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/DepthwiseConvolution2D.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/DepthwiseConvolution2D.java
@@ -107,6 +107,11 @@ public class DepthwiseConvolution2D extends ConvolutionLayer {
         }
     }
 
+    public long numBiasParams(DepthwiseConvolutionParamInitializer depthwiseConvolutionParamInitializer) {
+        val nOut = getNOut();
+        return (hasBias() ? nOut : 0);
+    }
+
     @Getter
     @Setter
     public static class Builder extends BaseConvBuilder<Builder> {
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/Layer.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/Layer.java
index 0dcd121..035a274 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/Layer.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/Layer.java
@@ -30,6 +30,7 @@ import org.deeplearning4j.nn.conf.dropout.IDropout;
 import org.deeplearning4j.nn.conf.inputs.InputType;
 import org.deeplearning4j.nn.conf.memory.LayerMemoryReport;
 import org.deeplearning4j.nn.conf.serde.legacyformat.LegacyLayerDeserializerHelper;
+import org.deeplearning4j.nn.params.DefaultParamInitializer;
 import org.deeplearning4j.optimize.api.TrainingListener;
 import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.ndarray.INDArray;
@@ -232,6 +233,19 @@ public abstract class Layer implements TrainingConfig, Serializable, Cloneable {
      */
     public abstract LayerMemoryReport getMemoryReport(InputType inputType);
 
+    public boolean hasBias(DefaultParamInitializer defaultParamInitializer){
+        if(this instanceof BaseOutputLayer ) {
+            return ((BaseOutputLayer) this).hasBias();
+        } else if(this instanceof DenseLayer){
+            return ((DenseLayer) this).hasBias();
+        } else if(this instanceof EmbeddingLayer){
+            return ((EmbeddingLayer) this).hasBias();
+        }  else if(this instanceof EmbeddingSequenceLayer){
+            return ((EmbeddingSequenceLayer) this).hasBias();
+        }
+        return true;
+    }
+
     @SuppressWarnings("unchecked")
     @Getter
     @Setter
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LocallyConnected1D.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LocallyConnected1D.java
index 4787d10..b77bdf7 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LocallyConnected1D.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LocallyConnected1D.java
@@ -201,9 +201,9 @@ public class LocallyConnected1D extends SameDiffLayer {
         if (hasBias) {
             SDVariable b = paramTable.get(ConvolutionParamInitializer.BIAS_KEY);
             SDVariable biasAddedResult = sameDiff.nn().biasAdd(result, b);
-            return activation.asSameDiff("out", sameDiff, biasAddedResult);
+            return sameDiff.asSameDiff("out", biasAddedResult, activation);
         } else {
-            return activation.asSameDiff("out", sameDiff, result);
+            return sameDiff.asSameDiff("out", result, activation);
         }
 
     }
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LocallyConnected2D.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LocallyConnected2D.java
index 5426fda..598c7d9 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LocallyConnected2D.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/LocallyConnected2D.java
@@ -31,7 +31,6 @@ import org.deeplearning4j.util.ValidationUtils;
 import org.nd4j.autodiff.samediff.SDIndex;
 import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
-import org.nd4j.base.Preconditions;
 import org.nd4j.linalg.activations.Activation;
 import org.nd4j.linalg.api.memory.MemoryWorkspace;
 import org.nd4j.linalg.api.ndarray.INDArray;
@@ -212,9 +211,9 @@ public class LocallyConnected2D extends SameDiffLayer {
         if (hasBias) {
             SDVariable b = paramTable.get(ConvolutionParamInitializer.BIAS_KEY);
             SDVariable biasAddedResult = sameDiff.nn().biasAdd(permutedResult, b);
-            return activation.asSameDiff("out", sameDiff, biasAddedResult);
+            return sameDiff.asSameDiff("out", biasAddedResult, activation);
         } else {
-            return activation.asSameDiff("out", sameDiff, permutedResult);
+            return sameDiff.asSameDiff("out", permutedResult, activation);
         }
 
     }
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/SeparableConvolution2D.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/SeparableConvolution2D.java
index 181cc53..01d45a3 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/SeparableConvolution2D.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/SeparableConvolution2D.java
@@ -157,6 +157,11 @@ public class SeparableConvolution2D extends ConvolutionLayer {
                         nOut, layerIndex, getLayerName(), SeparableConvolution2DLayer.class);
     }
 
+    public long numBiasParams(SeparableConvolutionParamInitializer separableConvolutionParamInitializer) {
+        val nOut = getNOut();
+        return (hasBias() ? nOut : 0);
+    }
+
 
     @Getter
     @Setter
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/samediff/AbstractSameDiffLayer.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/samediff/AbstractSameDiffLayer.java
index 74d5f45..d4026fe 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/samediff/AbstractSameDiffLayer.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/layers/samediff/AbstractSameDiffLayer.java
@@ -16,10 +16,7 @@
 
 package org.deeplearning4j.nn.conf.layers.samediff;
 
-import lombok.Data;
-import lombok.EqualsAndHashCode;
-import lombok.Getter;
-import lombok.Setter;
+import lombok.*;
 import lombok.extern.slf4j.Slf4j;
 import org.deeplearning4j.nn.api.ParamInitializer;
 import org.deeplearning4j.nn.conf.GradientNormalization;
@@ -40,11 +37,11 @@ import org.nd4j.linalg.learning.regularization.L1Regularization;
 import org.nd4j.linalg.learning.regularization.L2Regularization;
 import org.nd4j.linalg.learning.regularization.Regularization;
 import org.nd4j.linalg.learning.regularization.WeightDecay;
+import org.nd4j.linalg.util.ArrayUtil;
 
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
+
+import static org.nd4j.linalg.indexing.NDArrayIndex.interval;
 
 @Slf4j
 @Data
@@ -205,6 +202,34 @@ public abstract class AbstractSameDiffLayer extends Layer {
         applyGlobalConfigToLayer(b);
     }
 
+    public Map<String,INDArray> subsetAndReshape(List<String> params, Map<String, long[]> paramShapes, INDArray view,
+                                                 SameDiffVertex sdv, SameDiffParamInitializer sameDiffParamInitializer){
+        Class<?> clazz = (this != null ? getClass() : sdv.getClass());
+        String layerName = (this != null ? getLayerName() : ""); //TODO
+
+        Map<String,INDArray> out = new LinkedHashMap<>();
+        int soFar = 0;
+        for(String s : params){
+            val sh = paramShapes.get(s);
+            val length = ArrayUtil.prodLong(sh);
+            if(length <= 0){
+                throw new IllegalStateException("Invalid array state for parameter \"" + s + "\" in layer " + layerName
+                        + " of type " + clazz.getSimpleName() + ": parameter length (" + length
+                        + ") must be > 0 - parameter array shape: " + Arrays.toString(sh));
+            }
+            INDArray sub = view.get(interval(0,0,true), interval(soFar, soFar + length));
+
+            if(!Arrays.equals(sub.shape(), sh)){
+                char order = (this != null ? paramReshapeOrder(s) : sdv.paramReshapeOrder(s));
+                sub = sub.reshape(order, sh);
+            }
+            out.put(s, sub);
+
+            soFar += length;
+        }
+        return out;
+    }
+
     @Getter
     @Setter
     public static abstract class Builder<T extends Builder<T>> extends Layer.Builder<T> {
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/serde/BaseNetConfigDeserializer.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/serde/BaseNetConfigDeserializer.java
index 5194da4..8d348c9 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/serde/BaseNetConfigDeserializer.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/serde/BaseNetConfigDeserializer.java
@@ -232,7 +232,7 @@ public abstract class BaseNetConfigDeserializer<T> extends StdDeserializer<T> im
                         String dist = on.get("dist").asText();
                         d = NeuralNetConfiguration.mapper().readValue(dist, Distribution.class);
                     }
-                    IWeightInit iwi = w.getWeightInitFunction(d);
+                    IWeightInit iwi = d.getWeightInitFunction(w);
                     baseLayer.setWeightInitFn(iwi);
                 } catch (Throwable t){
                     log.warn("Failed to infer weight initialization from legacy JSON format",t);
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/samediff/SameDiffGraphVertex.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/samediff/SameDiffGraphVertex.java
index 1e95b4b..595fdbb 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/samediff/SameDiffGraphVertex.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/layers/samediff/SameDiffGraphVertex.java
@@ -71,8 +71,8 @@ public class SameDiffGraphVertex extends BaseGraphVertex {
         super(graph, name, vertexIndex, null, null, dataType);
         this.config = config;
         SDVertexParams vp = config.getVertexParams();
-        paramTable = SameDiffParamInitializer.getInstance().subsetAndReshape(vp.getParameterKeys(),
-                vp.getParamShapes(), paramsView, null, config);
+        paramTable = null.subsetAndReshape(vp.getParameterKeys(),
+                vp.getParamShapes(), paramsView, config, SameDiffParamInitializer.getInstance());
         if(initParams){
             config.initializeParameters(paramTable);
         }
@@ -177,8 +177,8 @@ public class SameDiffGraphVertex extends BaseGraphVertex {
     @Override
     public void setBackpropGradientsViewArray(INDArray backpropGradientsViewArray) {
         SDVertexParams vp = config.getVertexParams();
-        gradTable = SameDiffParamInitializer.getInstance().subsetAndReshape(vp.getParameterKeys(),
-                vp.getParamShapes(), backpropGradientsViewArray, null, config);
+        gradTable = null.subsetAndReshape(vp.getParameterKeys(),
+                vp.getParamShapes(), backpropGradientsViewArray, config, SameDiffParamInitializer.getInstance());
     }
 
     @Override
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java
index 9294e8d..2918f04 100755
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/multilayer/MultiLayerNetwork.java
@@ -798,15 +798,6 @@ public class MultiLayerNetwork implements Serializable, Classifier, Layer, Neura
         }
     }
 
-    protected INDArray activationFromPrevLayer(int curr, INDArray input, boolean training, LayerWorkspaceMgr mgr) {
-        if (getLayerWiseConfigurations().getInputPreProcess(curr) != null) {
-            input = getLayerWiseConfigurations().getInputPreProcess(curr).preProcess(input, getInputMiniBatchSize(), mgr);
-        }
-
-        INDArray ret = layers[curr].activate(input, training, mgr);
-        return ret;
-    }
-
     /**
      * Calculate activation for few layers at once. Suitable for autoencoder partial activation.
      *
@@ -829,7 +820,7 @@ public class MultiLayerNetwork implements Serializable, Classifier, Layer, Neura
 
             INDArray res = input;
             for (int l = from; l <= to; l++) {
-                res = this.activationFromPrevLayer(l, res, false, mgr);
+                res = mgr.activationFromPrevLayer(l, res, false, this);
             }
             return res;
         } catch (OutOfMemoryError e){
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DefaultParamInitializer.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DefaultParamInitializer.java
index f309927..2201cef 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DefaultParamInitializer.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DefaultParamInitializer.java
@@ -53,7 +53,7 @@ public class DefaultParamInitializer implements ParamInitializer {
         FeedForwardLayer layerConf = (FeedForwardLayer) l;
         val nIn = layerConf.getNIn();
         val nOut = layerConf.getNOut();
-        return (nIn * nOut + (hasBias(l) ? nOut : 0) + (hasLayerNorm(l) ? nOut : 0)); //weights + bias + gain
+        return (nIn * nOut + (l.hasBias(this) ? nOut : 0) + (hasLayerNorm(l) ? nOut : 0)); //weights + bias + gain
     }
 
     @Override
@@ -74,7 +74,7 @@ public class DefaultParamInitializer implements ParamInitializer {
 
     @Override
     public List<String> biasKeys(Layer layer) {
-        if(hasBias(layer)){
+        if(layer.hasBias(this)){
             return Collections.singletonList(BIAS_KEY);
         } else {
             return Collections.emptyList();
@@ -116,7 +116,7 @@ public class DefaultParamInitializer implements ParamInitializer {
         conf.addVariable(WEIGHT_KEY);
 
         long offset = nWeightParams;
-        if(hasBias(layerConf)){
+        if(layerConf.hasBias(this)){
             INDArray biasView = paramsView.get(NDArrayIndex.interval(0,0,true),
                     NDArrayIndex.interval(offset, offset + nOut));
             params.put(BIAS_KEY, createBias(conf, biasView, initializeParams));
@@ -149,7 +149,7 @@ public class DefaultParamInitializer implements ParamInitializer {
         out.put(WEIGHT_KEY, weightGradientView);
 
         long offset = nWeightParams;
-        if(hasBias(layerConf)){
+        if(layerConf.hasBias(this)){
             INDArray biasView = gradientView.get(NDArrayIndex.interval(0,0,true),
                     NDArrayIndex.interval(offset, offset + nOut)); //Already a row vector
             out.put(BIAS_KEY, biasView);
@@ -220,19 +220,6 @@ public class DefaultParamInitializer implements ParamInitializer {
         }
     }
 
-    protected boolean hasBias(Layer layer){
-        if(layer instanceof BaseOutputLayer ) {
-            return ((BaseOutputLayer) layer).hasBias();
-        } else if(layer instanceof DenseLayer){
-            return ((DenseLayer)layer).hasBias();
-        } else if(layer instanceof EmbeddingLayer){
-            return ((EmbeddingLayer)layer).hasBias();
-        }  else if(layer instanceof EmbeddingSequenceLayer){
-            return ((EmbeddingSequenceLayer)layer).hasBias();
-        }
-        return true;
-    }
-
     protected boolean hasLayerNorm(Layer layer){
         if(layer instanceof DenseLayer){
             return ((DenseLayer) layer).hasLayerNorm();
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DepthwiseConvolutionParamInitializer.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DepthwiseConvolutionParamInitializer.java
index 220f591..a53cca7 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DepthwiseConvolutionParamInitializer.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/DepthwiseConvolutionParamInitializer.java
@@ -54,16 +54,11 @@ public class DepthwiseConvolutionParamInitializer implements ParamInitializer {
         DepthwiseConvolution2D layerConf = (DepthwiseConvolution2D) l;
 
         val depthWiseParams = numDepthWiseParams(layerConf);
-        val biasParams = numBiasParams(layerConf);
+        val biasParams = layerConf.numBiasParams(this);
 
         return depthWiseParams + biasParams;
     }
 
-    private long numBiasParams(DepthwiseConvolution2D layerConf) {
-        val nOut = layerConf.getNOut();
-        return (layerConf.hasBias() ? nOut : 0);
-    }
-
     /**
      * For each input feature we separately compute depthMultiplier many
      * output maps for the given kernel size
@@ -126,7 +121,7 @@ public class DepthwiseConvolutionParamInitializer implements ParamInitializer {
         DepthwiseConvolution2D layerConf = (DepthwiseConvolution2D) conf.getLayer();
 
         val depthWiseParams = numDepthWiseParams(layerConf);
-        val biasParams = numBiasParams(layerConf);
+        val biasParams = layerConf.numBiasParams(this);
 
         INDArray depthWiseWeightView = paramsView.get(
                 NDArrayIndex.interval(0,0,true), NDArrayIndex.interval(biasParams, biasParams + depthWiseParams));
@@ -156,7 +151,7 @@ public class DepthwiseConvolutionParamInitializer implements ParamInitializer {
         Map<String, INDArray> out = new LinkedHashMap<>();
 
         val depthWiseParams = numDepthWiseParams(layerConf);
-        val biasParams = numBiasParams(layerConf);
+        val biasParams = layerConf.numBiasParams(this);
 
         INDArray depthWiseWeightGradientView = gradientView.get(
                 NDArrayIndex.interval(0,0,true), NDArrayIndex.interval(biasParams, biasParams + depthWiseParams))
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/PretrainParamInitializer.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/PretrainParamInitializer.java
index 7391da9..cd9574d 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/PretrainParamInitializer.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/PretrainParamInitializer.java
@@ -19,7 +19,6 @@ package org.deeplearning4j.nn.params;
 import lombok.val;
 import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
 import org.nd4j.linalg.api.ndarray.INDArray;
-import org.nd4j.linalg.factory.Nd4j;
 import org.nd4j.linalg.indexing.NDArrayIndex;
 
 import java.util.Map;
@@ -59,23 +58,12 @@ public class PretrainParamInitializer extends DefaultParamInitializer {
 
         INDArray visibleBiasView = paramsView.get(NDArrayIndex.interval(0,0,true),
                         NDArrayIndex.interval(nWeightParams + nOut, nWeightParams + nOut + nIn));
-        params.put(VISIBLE_BIAS_KEY, createVisibleBias(conf, visibleBiasView, initializeParams));
+        params.put(VISIBLE_BIAS_KEY, conf.createVisibleBias(visibleBiasView, initializeParams, this));
         conf.addVariable(VISIBLE_BIAS_KEY);
 
         return params;
     }
 
-    protected INDArray createVisibleBias(NeuralNetConfiguration conf, INDArray visibleBiasView,
-                    boolean initializeParameters) {
-        org.deeplearning4j.nn.conf.layers.BasePretrainNetwork layerConf =
-                        (org.deeplearning4j.nn.conf.layers.BasePretrainNetwork) conf.getLayer();
-        if (initializeParameters) {
-            INDArray ret = Nd4j.valueArrayOf(new long[]{1, layerConf.getNIn()}, layerConf.getVisibleBiasInit());
-            visibleBiasView.assign(ret);
-        }
-        return visibleBiasView;
-    }
-
 
     @Override
     public Map<String, INDArray> getGradientsFromFlattened(NeuralNetConfiguration conf, INDArray gradientView) {
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/SameDiffParamInitializer.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/SameDiffParamInitializer.java
index ca9c10c..03a045a 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/SameDiffParamInitializer.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/SameDiffParamInitializer.java
@@ -22,19 +22,13 @@ import org.deeplearning4j.nn.api.ParamInitializer;
 import org.deeplearning4j.nn.conf.NeuralNetConfiguration;
 import org.deeplearning4j.nn.conf.layers.Layer;
 import org.deeplearning4j.nn.conf.layers.samediff.AbstractSameDiffLayer;
-import org.deeplearning4j.nn.conf.layers.samediff.SameDiffVertex;
-import org.deeplearning4j.nn.layers.samediff.SameDiffGraphVertex;
-import org.nd4j.base.Preconditions;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.util.ArrayUtil;
 
-import java.util.Arrays;
-import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
 import static org.nd4j.linalg.indexing.NDArrayIndex.interval;
-import static org.nd4j.linalg.indexing.NDArrayIndex.point;
 
 @Slf4j
 public class SameDiffParamInitializer implements ParamInitializer {
@@ -114,34 +108,7 @@ public class SameDiffParamInitializer implements ParamInitializer {
 
     private Map<String,INDArray> subsetAndReshape(List<String> params, Map<String,long[]> paramShapes, INDArray view,
                                                   AbstractSameDiffLayer sdl){
-        return subsetAndReshape(params, paramShapes, view, sdl, null);
+        return sdl.subsetAndReshape(params, paramShapes, view, null, this);
     }
 
-    public Map<String,INDArray> subsetAndReshape(List<String> params, Map<String,long[]> paramShapes, INDArray view,
-                                                 AbstractSameDiffLayer sdl, SameDiffVertex sdv){
-        Class<?> clazz = (sdl != null ? sdl.getClass() : sdv.getClass());
-        String layerName = (sdl != null ? sdl.getLayerName() : ""); //TODO
-
-        Map<String,INDArray> out = new LinkedHashMap<>();
-        int soFar = 0;
-        for(String s : params){
-            val sh = paramShapes.get(s);
-            val length = ArrayUtil.prodLong(sh);
-            if(length <= 0){
-                throw new IllegalStateException("Invalid array state for parameter \"" + s + "\" in layer " + layerName
-                        + " of type " + clazz.getSimpleName() + ": parameter length (" + length
-                        + ") must be > 0 - parameter array shape: " + Arrays.toString(sh));
-            }
-            INDArray sub = view.get(interval(0,0,true), interval(soFar, soFar + length));
-
-            if(!Arrays.equals(sub.shape(), sh)){
-                char order = (sdl != null ? sdl.paramReshapeOrder(s) : sdv.paramReshapeOrder(s));
-                sub = sub.reshape(order, sh);
-            }
-            out.put(s, sub);
-
-            soFar += length;
-        }
-        return out;
-    }
 }
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/SeparableConvolutionParamInitializer.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/SeparableConvolutionParamInitializer.java
index 796bf29..57523af 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/SeparableConvolutionParamInitializer.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/params/SeparableConvolutionParamInitializer.java
@@ -56,16 +56,11 @@ public class SeparableConvolutionParamInitializer implements ParamInitializer {
 
         val depthWiseParams = numDepthWiseParams(layerConf);
         val pointWiseParams = numPointWiseParams(layerConf);
-        val biasParams = numBiasParams(layerConf);
+        val biasParams = layerConf.numBiasParams(this);
 
         return depthWiseParams + pointWiseParams + biasParams;
     }
 
-    private long numBiasParams(SeparableConvolution2D layerConf) {
-        val nOut = layerConf.getNOut();
-        return (layerConf.hasBias() ? nOut : 0);
-    }
-
     /**
      * For each input feature we separately compute depthMultiplier many
      * output maps for the given kernel size
@@ -143,7 +138,7 @@ public class SeparableConvolutionParamInitializer implements ParamInitializer {
         SeparableConvolution2D layerConf = (SeparableConvolution2D) conf.getLayer();
 
         val depthWiseParams = numDepthWiseParams(layerConf);
-        val biasParams = numBiasParams(layerConf);
+        val biasParams = layerConf.numBiasParams(this);
 
         INDArray depthWiseWeightView = paramsView.get(
                 NDArrayIndex.interval(0,0,true), NDArrayIndex.interval(biasParams, biasParams + depthWiseParams));
@@ -178,7 +173,7 @@ public class SeparableConvolutionParamInitializer implements ParamInitializer {
         Map<String, INDArray> out = new LinkedHashMap<>();
 
         val depthWiseParams = numDepthWiseParams(layerConf);
-        val biasParams = numBiasParams(layerConf);
+        val biasParams = layerConf.numBiasParams(this);
 
         INDArray depthWiseWeightGradientView = gradientView.get(
                 NDArrayIndex.interval(0,0,true), NDArrayIndex.interval(biasParams, biasParams + depthWiseParams))
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/transferlearning/TransferLearning.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/transferlearning/TransferLearning.java
index 673010a..cb52c8f 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/transferlearning/TransferLearning.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/transferlearning/TransferLearning.java
@@ -252,7 +252,7 @@ public class TransferLearning {
          * @return Builder
          */
         public Builder nInReplace(int layerNum, int nIn, WeightInit scheme, Distribution dist) {
-            return nInReplace(layerNum, nIn, scheme.getWeightInitFunction(dist));
+            return nInReplace(layerNum, nIn, dist.getWeightInitFunction(scheme));
         }
 
         /**
@@ -718,7 +718,7 @@ public class TransferLearning {
          * @return GraphBuilder
          */
         public GraphBuilder nInReplace(String layerName, int nIn, WeightInit scheme, Distribution dist) {
-            return nInReplace(layerName, nIn, scheme.getWeightInitFunction(dist));
+            return nInReplace(layerName, nIn, dist.getWeightInitFunction(scheme));
         }
 
         /**
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/weights/WeightInit.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/weights/WeightInit.java
index 435e148..a9ea04f 100755
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/weights/WeightInit.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/weights/WeightInit.java
@@ -16,8 +16,6 @@
 
 package org.deeplearning4j.nn.weights;
 
-import org.deeplearning4j.nn.conf.distribution.Distribution;
-
 /**
  * Weight initialization scheme
  * <p>
@@ -78,60 +76,7 @@ public enum WeightInit {
      * @return a new {@link IWeightInit} instance
      */
     public IWeightInit getWeightInitFunction() {
-        return getWeightInitFunction(null);
+        return null.getWeightInitFunction(this);
     }
 
-    /**
-     * Create an instance of the weight initialization function
-     *
-     * @param distribution Distribution of the weights (Only used in case DISTRIBUTION)
-     * @return a new {@link IWeightInit} instance
-     */
-    public IWeightInit getWeightInitFunction(Distribution distribution) {
-        switch (this) {
-            case ZERO:
-                return new WeightInitConstant(0.0);
-            case ONES:
-                return new WeightInitConstant(1.0);
-            case DISTRIBUTION:
-                return new WeightInitDistribution(distribution);
-            case SIGMOID_UNIFORM:
-                return new WeightInitSigmoidUniform();
-            case LECUN_NORMAL: //Fall through: these 3 are equivalent
-            case XAVIER_FAN_IN:
-            case NORMAL:
-                return new WeightInitNormal();
-            case UNIFORM:
-                return new WeightInitUniform();
-            case XAVIER:
-                return new WeightInitXavier();
-            case XAVIER_UNIFORM:
-                return new WeightInitXavierUniform();
-            case XAVIER_LEGACY:
-                return new WeightInitXavierLegacy();
-            case RELU:
-                return new WeightInitRelu();
-            case RELU_UNIFORM:
-                return new WeightInitReluUniform();
-            case IDENTITY:
-                return new WeightInitIdentity();
-            case LECUN_UNIFORM:
-                return new WeightInitLecunUniform();
-            case VAR_SCALING_NORMAL_FAN_IN:
-                return new WeightInitVarScalingNormalFanIn();
-            case VAR_SCALING_NORMAL_FAN_OUT:
-                return new WeightInitVarScalingNormalFanOut();
-            case VAR_SCALING_NORMAL_FAN_AVG:
-                return new WeightInitVarScalingNormalFanAvg();
-            case VAR_SCALING_UNIFORM_FAN_IN:
-                return new WeightInitVarScalingUniformFanIn();
-            case VAR_SCALING_UNIFORM_FAN_OUT:
-                return new WeightInitVarScalingUniformFanOut();
-            case VAR_SCALING_UNIFORM_FAN_AVG:
-                return new WeightInitVarScalingUniformFanAvg();
-
-            default:
-                throw new UnsupportedOperationException("Unknown or not supported weight initialization function: " + this);
-        }
-    }
 }
diff --git a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/workspace/LayerWorkspaceMgr.java b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/workspace/LayerWorkspaceMgr.java
index e40fdcd..854822c 100644
--- a/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/workspace/LayerWorkspaceMgr.java
+++ b/deeplearning4j/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/workspace/LayerWorkspaceMgr.java
@@ -21,6 +21,7 @@ import lombok.Getter;
 import lombok.NonNull;
 import lombok.Setter;
 import org.bytedeco.javacpp.Pointer;
+import org.deeplearning4j.nn.multilayer.MultiLayerNetwork;
 import org.nd4j.linalg.api.memory.conf.WorkspaceConfiguration;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.workspace.BaseWorkspaceMgr;
@@ -136,6 +137,15 @@ public class LayerWorkspaceMgr extends BaseWorkspaceMgr<ArrayType> {
         return NO_WS_IMMUTABLE;
     }
 
+    public INDArray activationFromPrevLayer(int curr, INDArray input, boolean training, MultiLayerNetwork multiLayerNetwork) {
+        if (multiLayerNetwork.getLayerWiseConfigurations().getInputPreProcess(curr) != null) {
+            input = multiLayerNetwork.getLayerWiseConfigurations().getInputPreProcess(curr).preProcess(input, multiLayerNetwork.getInputMiniBatchSize(), this);
+        }
+
+        INDArray ret = multiLayerNetwork.getLayers()[curr].activate(input, training, this);
+        return ret;
+    }
+
     public static class Builder {
 
         private LayerWorkspaceMgr mgr;
diff --git a/deeplearning4j/deeplearning4j-scaleout/spark/dl4j-spark-nlp/src/main/java/org/deeplearning4j/spark/models/embeddings/word2vec/SentenceBatch.java b/deeplearning4j/deeplearning4j-scaleout/spark/dl4j-spark-nlp/src/main/java/org/deeplearning4j/spark/models/embeddings/word2vec/SentenceBatch.java
index 0d97f9b..1d607ce 100644
--- a/deeplearning4j/deeplearning4j-scaleout/spark/dl4j-spark-nlp/src/main/java/org/deeplearning4j/spark/models/embeddings/word2vec/SentenceBatch.java
+++ b/deeplearning4j/deeplearning4j-scaleout/spark/dl4j-spark-nlp/src/main/java/org/deeplearning4j/spark/models/embeddings/word2vec/SentenceBatch.java
@@ -60,7 +60,7 @@ public class SentenceBatch implements Function<Word2VecFuncCall, Word2VecChange>
                 VocabWord vocabWord = sentence.get(i);
                 if (vocabWord != null && vocabWord.getWord().endsWith("STOP")) {
                     nextRandom.set(nextRandom.get() * 25214903917L + 11);
-                    skipGram(param, i, sentence, (int) nextRandom.get() % param.getWindow(), alpha, changed);
+                    param.skipGram(i, sentence, (int) nextRandom.get() % param.getWindow(), alpha, changed, this);
                 }
             }
         }
@@ -68,34 +68,6 @@ public class SentenceBatch implements Function<Word2VecFuncCall, Word2VecChange>
 
 
     /**
-     * Train via skip gram
-     * @param i the current word
-     * @param sentence the sentence to train on
-     * @param b
-     * @param alpha the learning rate
-     */
-    public void skipGram(Word2VecParam param, int i, List<VocabWord> sentence, int b, double alpha,
-                    List<Triple<Integer, Integer, Integer>> changed) {
-
-        final VocabWord word = sentence.get(i);
-        int window = param.getWindow();
-        if (word != null && !sentence.isEmpty()) {
-            int end = window * 2 + 1 - b;
-            for (int a = b; a < end; a++) {
-                if (a != window) {
-                    int c = i - window + a;
-                    if (c >= 0 && c < sentence.size()) {
-                        VocabWord lastWord = sentence.get(c);
-                        iterateSample(param, word, lastWord, alpha, changed);
-                    }
-                }
-            }
-        }
-    }
-
-
-
-    /**
      * Iterate on the given 2 vocab words
      *
      * @param w1 the first word to iterate on
diff --git a/deeplearning4j/deeplearning4j-scaleout/spark/dl4j-spark-nlp/src/main/java/org/deeplearning4j/spark/models/embeddings/word2vec/Word2VecParam.java b/deeplearning4j/deeplearning4j-scaleout/spark/dl4j-spark-nlp/src/main/java/org/deeplearning4j/spark/models/embeddings/word2vec/Word2VecParam.java
index 9a88a9e..4eb856e 100644
--- a/deeplearning4j/deeplearning4j-scaleout/spark/dl4j-spark-nlp/src/main/java/org/deeplearning4j/spark/models/embeddings/word2vec/Word2VecParam.java
+++ b/deeplearning4j/deeplearning4j-scaleout/spark/dl4j-spark-nlp/src/main/java/org/deeplearning4j/spark/models/embeddings/word2vec/Word2VecParam.java
@@ -18,11 +18,14 @@ package org.deeplearning4j.spark.models.embeddings.word2vec;
 
 import org.apache.spark.broadcast.Broadcast;
 import org.deeplearning4j.models.embeddings.inmemory.InMemoryLookupTable;
+import org.deeplearning4j.models.word2vec.VocabWord;
 import org.nd4j.linalg.api.ndarray.INDArray;
+import org.nd4j.linalg.primitives.Triple;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
 import java.io.Serializable;
+import java.util.List;
 import java.util.concurrent.atomic.AtomicLong;
 
 /**
@@ -203,6 +206,34 @@ public class Word2VecParam implements Serializable {
         this.vectorLength = vectorLength;
     }
 
+    /**
+     * Train via skip gram
+     * @param i the current word
+     * @param sentence the sentence to train on
+     * @param b
+     * @param alpha the learning rate
+     * @param changed
+     * @param sentenceBatch
+     */
+    public void skipGram(int i, List<VocabWord> sentence, int b, double alpha,
+                         List<Triple<Integer, Integer, Integer>> changed, SentenceBatch sentenceBatch) {
+
+        final VocabWord word = sentence.get(i);
+        int window = getWindow();
+        if (word != null && !sentence.isEmpty()) {
+            int end = window * 2 + 1 - b;
+            for (int a = b; a < end; a++) {
+                if (a != window) {
+                    int c = i - window + a;
+                    if (c >= 0 && c < sentence.size()) {
+                        VocabWord lastWord = sentence.get(c);
+                        sentenceBatch.iterateSample(this, word, lastWord, alpha, changed);
+                    }
+                }
+            }
+        }
+    }
+
     public static class Builder {
         private boolean useAdaGrad = true;
         private double negative = 0;
diff --git a/deeplearning4j/deeplearning4j-ui-parent/deeplearning4j-play/src/main/java/org/deeplearning4j/ui/i18n/DefaultI18N.java b/deeplearning4j/deeplearning4j-ui-parent/deeplearning4j-play/src/main/java/org/deeplearning4j/ui/i18n/DefaultI18N.java
index 27691ee..37e5595 100644
--- a/deeplearning4j/deeplearning4j-ui-parent/deeplearning4j-play/src/main/java/org/deeplearning4j/ui/i18n/DefaultI18N.java
+++ b/deeplearning4j/deeplearning4j-ui-parent/deeplearning4j-play/src/main/java/org/deeplearning4j/ui/i18n/DefaultI18N.java
@@ -17,13 +17,9 @@
 package org.deeplearning4j.ui.i18n;
 
 import lombok.extern.slf4j.Slf4j;
-import org.apache.commons.io.IOUtils;
 import org.deeplearning4j.ui.api.I18N;
 import org.deeplearning4j.ui.api.UIModule;
 
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.charset.Charset;
 import java.util.Map;
 import java.util.HashMap;
 import java.util.List;
@@ -115,7 +111,7 @@ public class DefaultI18N implements I18N {
                     String langCode = path.substring(idxLast + 1).toLowerCase();
                     Map<String, String> map = messagesByLanguage.computeIfAbsent(langCode, k -> new HashMap<>());
 
-                    parseFile(r, map);
+                    r.parseFile(map, this);
                 } catch (Throwable t){
                     log.warn("Error parsing UI I18N content file; skipping: {}", r.getResource(), t);
                     languageLoadingException = t;
@@ -144,31 +140,6 @@ public class DefaultI18N implements I18N {
         return noData;
     }
 
-    private void parseFile(I18NResource r, Map<String,String> results){
-        List<String> lines;
-        try (InputStream is = r.getInputStream()){
-            lines = IOUtils.readLines(is, Charset.forName("UTF-8"));
-        } catch (IOException e){
-            log.debug("Error parsing UI I18N content file; skipping: {}", r.getResource(), e.getMessage());
-            return;
-        }
-
-        int count = 0;
-        for (String line : lines) {
-            if (!line.matches(".+=.*")) {
-                log.debug("Invalid line in I18N file: {}, \"{}\"", r.getResource(), line);
-                continue;
-            }
-            int idx = line.indexOf('=');
-            String key = line.substring(0, idx);
-            String value = line.substring(Math.min(idx + 1, line.length()));
-            results.put(key, value);
-            count++;
-        }
-
-        log.trace("Loaded {} messages from file {}", count, r.getResource());
-    }
-
     @Override
     public String getMessage(String key) {
         return getMessage(currentLanguage, key);
diff --git a/deeplearning4j/deeplearning4j-ui-parent/deeplearning4j-play/src/main/java/org/deeplearning4j/ui/i18n/I18NResource.java b/deeplearning4j/deeplearning4j-ui-parent/deeplearning4j-play/src/main/java/org/deeplearning4j/ui/i18n/I18NResource.java
index 6ce67ee..0ee604d 100644
--- a/deeplearning4j/deeplearning4j-ui-parent/deeplearning4j-play/src/main/java/org/deeplearning4j/ui/i18n/I18NResource.java
+++ b/deeplearning4j/deeplearning4j-ui-parent/deeplearning4j-play/src/main/java/org/deeplearning4j/ui/i18n/I18NResource.java
@@ -18,10 +18,14 @@ package org.deeplearning4j.ui.i18n;
 
 import lombok.AllArgsConstructor;
 import lombok.Data;
+import org.apache.commons.io.IOUtils;
 import org.nd4j.linalg.io.ClassPathResource;
 
 import java.io.IOException;
 import java.io.InputStream;
+import java.nio.charset.Charset;
+import java.util.List;
+import java.util.Map;
 
 @AllArgsConstructor
 @Data
@@ -34,4 +38,28 @@ public class I18NResource {
     }
 
 
+    public void parseFile(Map<String, String> results, DefaultI18N defaultI18N){
+        List<String> lines;
+        try (InputStream is = getInputStream()){
+            lines = IOUtils.readLines(is, Charset.forName("UTF-8"));
+        } catch (IOException e){
+            log.debug("Error parsing UI I18N content file; skipping: {}", getResource(), e.getMessage());
+            return;
+        }
+
+        int count = 0;
+        for (String line : lines) {
+            if (!line.matches(".+=.*")) {
+                log.debug("Invalid line in I18N file: {}, \"{}\"", getResource(), line);
+                continue;
+            }
+            int idx = line.indexOf('=');
+            String key = line.substring(0, idx);
+            String value = line.substring(Math.min(idx + 1, line.length()));
+            results.put(key, value);
+            count++;
+        }
+
+        log.trace("Loaded {} messages from file {}", count, getResource());
+    }
 }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/functions/DifferentialFunction.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/functions/DifferentialFunction.java
index a3f4079..a47c259 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/functions/DifferentialFunction.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/functions/DifferentialFunction.java
@@ -16,6 +16,8 @@
 
 package org.nd4j.autodiff.functions;
 
+import com.google.common.primitives.Floats;
+import com.google.common.primitives.Ints;
 import com.rits.cloning.Cloner;
 import lombok.Data;
 import lombok.Getter;
@@ -30,6 +32,7 @@ import org.nd4j.graph.DataType;
 import org.nd4j.imports.converters.DifferentialFunctionClassHolder;
 import org.nd4j.imports.descriptors.properties.AttributeAdapter;
 import org.nd4j.imports.descriptors.properties.PropertyMapping;
+import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.api.ops.Op;
 import org.nd4j.linalg.api.shape.LongShapeDescriptor;
@@ -852,4 +855,202 @@ public abstract class DifferentialFunction {
 
     public int getNumOutputs(){return -1;}
 
+    /**
+     * Init a function's attributes
+     * @param mappedTfName the tensorflow name to pick (sometimes ops have multiple names
+     * @param attributesForNode the attributes for the node
+     * @param node
+     * @param graph
+     * @param tfGraphMapper
+     */
+    public void initFunctionFromProperties(String mappedTfName, Map<String, AttrValue> attributesForNode, NodeDef node, GraphDef graph, TFGraphMapper tfGraphMapper) {
+        val properties = mappingsForFunction();
+        val tfProperties = properties.get(mappedTfName);
+        val fields = DifferentialFunctionClassHolder.getInstance().getFieldsForFunction(this);
+        val attributeAdapters = attributeAdaptersForFunction();
+
+        // if there's no properties announced for this function - just return
+        if (tfProperties == null)
+            return;
+
+        //Can't execute in just any order: sometimes there are dependencies between attribute mappings
+        //For example, conv2d strides depend on data format -> need to map data format before mapping strides
+        //Solution: map nodes without adapters before nodes with adapters. This doesn't guarantee we'll always be
+        // mapping in the right order (for example, we might have adapter(x) depends on adapter(y)) but it should catch most cases
+        Map<String,PropertyMapping> map;
+        if(attributeAdapters == null || !attributeAdapters.containsKey(mappedTfName)) {
+            map = tfProperties;
+        } else {
+            map = new LinkedHashMap<>();
+            for (Map.Entry<String, PropertyMapping> e : tfProperties.entrySet()) {
+                if (!attributeAdapters.get(mappedTfName).containsKey(e.getKey())) {
+                    //No adapter for this attribute
+                    map.put(e.getKey(), e.getValue());
+                }
+            }
+            for (Map.Entry<String, PropertyMapping> e : tfProperties.entrySet()) {
+                if (!map.containsKey(e.getKey())) {
+                    //Not added on first pass -> must have attribute mapper
+                    map.put(e.getKey(), e.getValue());
+                }
+            }
+        }
+
+        for(Map.Entry<String,PropertyMapping> entry : map.entrySet()){
+            val tfAttrName = entry.getValue().getTfAttrName();
+            val currentField = fields.get(entry.getKey());
+
+            AttributeAdapter adapter = null;
+            if(attributeAdapters != null && !attributeAdapters.isEmpty()) {
+                val mappers = attributeAdapters.get(mappedTfName);
+                val adapterFor = mappers.get(entry.getKey());
+                adapter = adapterFor;
+            }
+
+
+            if(tfAttrName != null) {
+                if(currentField == null) {
+                    continue;
+                }
+
+                if(attributesForNode.containsKey(tfAttrName)) {
+                    val attr = attributesForNode.get(tfAttrName);
+                    switch (attr.getValueCase()) {
+                        case B:
+                            if (adapter != null) {
+                                adapter.mapAttributeFor(attr.getB(), currentField, this);
+                            }
+                            break;
+                        case F: break;
+                        case FUNC: break;
+                        case S:
+                            val setString = attr.getS().toStringUtf8();
+                            if(adapter != null) {
+                                adapter.mapAttributeFor(setString,currentField, this);
+                            }
+                            else
+                                setValueFor(currentField,setString);
+                            break;
+                        case I:
+                            val setInt = (int) attr.getI();
+                            if(adapter != null) {
+                                adapter.mapAttributeFor(setInt,currentField, this);
+                            }
+                            else
+                                setValueFor(currentField,setInt);
+                            break;
+                        case SHAPE:
+                            val shape = attr.getShape().getDimList();
+                            int[] dimsToSet = new int[shape.size()];
+                            for(int i = 0; i < dimsToSet.length; i++) {
+                                dimsToSet[i] = (int) shape.get(i).getSize();
+                            }
+
+                            if(adapter != null) {
+                                adapter.mapAttributeFor(dimsToSet,currentField, this);
+                            }
+
+                            else
+                                setValueFor(currentField,dimsToSet);
+                            break;
+                        case VALUE_NOT_SET:break;
+                        case PLACEHOLDER: break;
+                        case LIST:
+                            val setList = attr.getList();
+                            if(!setList.getIList().isEmpty()) {
+                                val intList = Ints.toArray(setList.getIList());
+                                if(adapter != null) {
+                                    adapter.mapAttributeFor(intList,currentField, this);
+                                }
+                                else
+                                    setValueFor(currentField,intList);
+                            }
+                            else if(!setList.getBList().isEmpty()) {
+                                break;
+                            }
+                            else if(!setList.getFList().isEmpty()) {
+                                val floats = Floats.toArray(setList.getFList());
+                                if(adapter != null) {
+                                    adapter.mapAttributeFor(floats,currentField, this);
+                                }
+
+                                else
+                                    setValueFor(currentField,floats);
+                                break;
+                            }
+                            else if(!setList.getFuncList().isEmpty()) {
+                                break;
+                            }
+                            else if(!setList.getTensorList().isEmpty()) {
+                                break;
+                            }
+                            break;
+                        case TENSOR:
+                            val tensorToGet = TFGraphMapper.getInstance().mapTensorProto(attr.getTensor());
+                            if(adapter != null) {
+                                adapter.mapAttributeFor(tensorToGet,currentField, this);
+                            }
+                            else
+                                setValueFor(currentField,tensorToGet);
+                            break;
+                        case TYPE:
+                            if (adapter != null) {
+                                adapter.mapAttributeFor(attr.getType(), currentField, this);
+                            }
+                            break;
+                    }
+                }
+            }
+
+            else if(entry.getValue().getTfInputPosition() != null) {
+
+
+                int position = entry.getValue().getTfInputPosition();
+                if(position < 0) {
+                    position += node.getInputCount();
+                }
+
+                val inputFromNode = TFGraphMapper.getInstance().getNodeWithNameFromGraph(graph,node.getInput(position));
+                INDArray tensor = inputFromNode != null ? TFGraphMapper.getInstance().getNDArrayFromTensor("value",inputFromNode,graph) : null;
+                if(tensor == null) {
+                    tensor = getSameDiff().getArrForVarName(tfGraphMapper.getNodeName(node.getInput(position)));
+                }
+
+
+                if(tensor != null) {
+                    //use adapter instead of direct mapping just like above
+                    if(adapter != null) {
+                        adapter.mapAttributeFor(tensor,currentField, this);
+                    }
+                    else {
+                        if(currentField.getType().equals(int[].class)) {
+                            setValueFor(currentField,tensor.data().asInt());
+                        }
+                        else if(currentField.getType().equals(double[].class)) {
+                            setValueFor(currentField,tensor.data().asDouble());
+
+                        }
+                        else if(currentField.getType().equals(float[].class)) {
+                            setValueFor(currentField,tensor.data().asFloat());
+
+                        }
+                        else if(currentField.getType().equals(INDArray.class)) {
+                            setValueFor(currentField,tensor);
+                        }
+                        else if(currentField.getType().equals(int.class)) {
+                            setValueFor(currentField,tensor.getInt(0));
+                        }
+                        else if(currentField.getType().equals(double.class)) {
+                            setValueFor(currentField,tensor.getDouble(0));
+                        }
+                        else if(currentField.getType().equals(float.class)) {
+                            setValueFor(currentField,tensor.getFloat(0));
+                        }
+                    }
+                } else {
+                    getSameDiff().addPropertyToResolve(this,entry.getKey());
+                }
+            }
+        }
+    }
 }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/functions/DifferentialFunctionFactory.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/functions/DifferentialFunctionFactory.java
index 7dfc89c..4589253 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/functions/DifferentialFunctionFactory.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/functions/DifferentialFunctionFactory.java
@@ -94,7 +94,6 @@ import org.nd4j.linalg.api.ops.random.custom.RandomBernoulli;
 import org.nd4j.linalg.api.ops.random.custom.RandomExponential;
 import org.nd4j.linalg.api.ops.random.custom.RandomNormal;
 import org.nd4j.linalg.api.ops.random.impl.*;
-import org.nd4j.linalg.api.ops.random.impl.Linspace;
 import org.nd4j.linalg.api.shape.Shape;
 import org.nd4j.linalg.indexing.conditions.Condition;
 import org.nd4j.linalg.util.ArrayUtil;
@@ -247,23 +246,6 @@ public class DifferentialFunctionFactory {
     }
 
     /**
-     * Local response normalization operation.
-     *
-     * @param input     the inputs to lrn
-     * @param lrnConfig the configuration
-     * @return
-     */
-    public SDVariable localResponseNormalization(SDVariable input, LocalResponseNormalizationConfig lrnConfig) {
-        LocalResponseNormalization lrn = LocalResponseNormalization.builder()
-                .inputFunctions(new SDVariable[]{input})
-                .sameDiff(sameDiff())
-                .config(lrnConfig)
-                .build();
-
-        return lrn.outputVariable();
-    }
-
-    /**
      * Conv1d operation.
      *
      * @param input        the inputs to conv1d
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SDVariable.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SDVariable.java
index 65d371f..8af1f84 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SDVariable.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SDVariable.java
@@ -16,6 +16,8 @@
 
 package org.nd4j.autodiff.samediff;
 
+import com.google.common.primitives.Ints;
+import com.google.common.primitives.Longs;
 import lombok.*;
 import lombok.extern.slf4j.Slf4j;
 import onnx.OnnxProto3;
@@ -27,6 +29,7 @@ import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.blas.params.MMulTranspose;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.api.ops.Op;
+import org.nd4j.linalg.api.ops.impl.reduce.TensorMmul;
 import org.nd4j.linalg.api.ops.impl.transforms.pairwise.arithmetic.*;
 import org.nd4j.linalg.api.shape.LongShapeDescriptor;
 import org.nd4j.linalg.exception.ND4JIllegalStateException;
@@ -1953,5 +1956,83 @@ public class SDVariable extends DifferentialFunction implements Serializable {
         }
         return x;
     }
-   
+
+    public SDVariable doTensorMmul(SDVariable b,
+                                   int[][] axes, TensorMmul tensorMmul) {
+
+        int validationLength = Math.min(axes[0].length, axes[1].length);
+        for (int i = 0; i < validationLength; i++) {
+            if (getShape()[axes[0][i]] != b.getShape()[axes[1][i]])
+                throw new IllegalArgumentException("Size of the given axes at each dimension must be the same size.");
+            if (axes[0][i] < 0)
+                axes[0][i] += getShape().length;
+            if (axes[1][i] < 0)
+                axes[1][i] += b.getShape().length;
+
+        }
+
+        List<Integer> listA = new ArrayList<>();
+        for (int i = 0; i < getShape().length; i++) {
+            if (!Ints.contains(axes[0], i))
+                listA.add(i);
+        }
+
+        int[] newAxesA = Ints.concat(Ints.toArray(listA), axes[0]);
+
+
+        List<Integer> listB = new ArrayList<>();
+        for (int i = 0; i < b.getShape().length; i++) {
+            if (!Ints.contains(axes[1], i))
+                listB.add(i);
+        }
+
+        int[] newAxesB = Ints.concat(axes[1], Ints.toArray(listB));
+
+        int n2 = 1;
+        int aLength = Math.min(getShape().length, axes[0].length);
+        for (int i = 0; i < aLength; i++) {
+            n2 *= getShape()[axes[0][i]];
+        }
+
+        //if listA and listB are empty these do not initialize.
+        //so initializing with {1} which will then get overridden if not empty
+        long[] newShapeA = {-1, n2};
+        long[] oldShapeA;
+        if (listA.size() == 0) {
+            oldShapeA = new long[] {1};
+        } else {
+            oldShapeA = Longs.toArray(listA);
+            for (int i = 0; i < oldShapeA.length; i++)
+                oldShapeA[i] = getShape()[(int) oldShapeA[i]];
+        }
+
+        int n3 = 1;
+        int bNax = Math.min(b.getShape().length, axes[1].length);
+        for (int i = 0; i < bNax; i++) {
+            n3 *= b.getShape()[axes[1][i]];
+        }
+
+
+        int[] newShapeB = {n3, -1};
+        long[] oldShapeB;
+        if (listB.size() == 0) {
+            oldShapeB = new long[] {1};
+        } else {
+            oldShapeB = Longs.toArray(listB);
+            for (int i = 0; i < oldShapeB.length; i++)
+                oldShapeB[i] = b.getShape()[(int) oldShapeB[i]];
+        }
+
+
+        SDVariable at = tensorMmul.f()
+                .reshape(tensorMmul.f().permute
+                        (this,newAxesA),newShapeA);
+        SDVariable bt = tensorMmul.f()
+                .reshape(tensorMmul.f()
+                        .permute(b,newAxesB),newShapeB);
+
+        SDVariable ret = tensorMmul.f().mmul(at,bt);
+        long[] aPlusB = Longs.concat(oldShapeA, oldShapeB);
+        return tensorMmul.f().reshape(ret, aPlusB);
+    }
 }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java
index 82a770d..6c234fe 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/SameDiff.java
@@ -40,6 +40,7 @@ import org.nd4j.base.Preconditions;
 import org.nd4j.evaluation.IEvaluation;
 import org.nd4j.graph.*;
 import org.nd4j.jackson.objectmapper.holder.ObjectMapperHolder;
+import org.nd4j.linalg.activations.Activation;
 import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.buffer.factory.DataBufferFactory;
 import org.nd4j.linalg.api.memory.MemoryWorkspace;
@@ -4977,4 +4978,48 @@ public class SameDiff extends SDBaseOps {
     }
 
 
+    /**
+     * Get the Activation as a SameDiff variable
+     *
+     * @param variableName Variable name
+     * @param input        Input variable to apply the activation function to
+     * @param activation
+     * @return SDVariable: output after applying the activation function
+     */
+    public SDVariable asSameDiff(String variableName, SDVariable input, Activation activation) {
+        switch (activation) {
+            case CUBE:
+                return math().pow(variableName, input, 3.0);
+            case ELU:
+                return nn().elu(variableName, input);
+            case HARDTANH:
+                return nn().hardTanh(variableName, input);
+            case IDENTITY:
+                return identity(variableName, input);
+            case LEAKYRELU:
+                return nn().leakyRelu(variableName, input, 0.0);
+            case RELU:
+                return nn().relu(variableName, input, 0.0);
+            case SIGMOID:
+                return nn().sigmoid(variableName, input);
+            case SOFTMAX:
+                return nn().softmax(variableName, input);
+            case SOFTPLUS:
+                return nn().softplus(variableName, input);
+            case SOFTSIGN:
+                return nn().softsign(variableName, input);
+            case TANH:
+                return math().tanh(variableName, input);
+            case GELU:
+                return nn().gelu(variableName, input);
+            case HARDSIGMOID:
+            case RATIONALTANH:
+            case RRELU:
+            case RECTIFIEDTANH:
+            case SELU:
+            case SWISH:
+            default:
+                throw new UnsupportedOperationException("Activation function not yet supported: " + activation);
+        }
+    }
 }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/ops/SDCNN.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/ops/SDCNN.java
index 8a203c9..8921334 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/ops/SDCNN.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/autodiff/samediff/ops/SDCNN.java
@@ -587,7 +587,7 @@ public class SDCNN extends SDOps {
     public SDVariable localResponseNormalization(String name, SDVariable input,
                                                  LocalResponseNormalizationConfig lrnConfig) {
         validateFloatingPoint("local response normalization", input);
-        SDVariable ret = f().localResponseNormalization(input, lrnConfig);
+        SDVariable ret = lrnConfig.localResponseNormalization(input, f());
         return updateVariableNameAndReference(ret, name);
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/classification/ROC.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/classification/ROC.java
index 93b2cd6..633026b 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/classification/ROC.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/classification/ROC.java
@@ -39,6 +39,7 @@ import org.nd4j.shade.jackson.annotation.JsonIgnoreProperties;
 import org.nd4j.shade.jackson.annotation.JsonTypeInfo;
 import org.nd4j.shade.jackson.databind.annotation.JsonSerialize;
 
+import java.io.IOException;
 import java.io.Serializable;
 import java.util.Arrays;
 import java.util.LinkedHashMap;
@@ -516,6 +517,32 @@ public class ROC extends BaseEvaluation<ROC> {
         return prCurve;
     }
 
+    @Override
+    public void serialize(JsonGenerator jsonGenerator, SerializerProvider serializerProvider, ROCSerializer rocSerializer)
+                    throws IOException {
+        if (isExact()) {
+            //For exact ROC implementation: force AUC and AUPRC calculation, so result can be stored in JSON, such
+            //that we have them once deserialized.
+            //Due to potentially huge size, exact mode doesn't store the original predictions in JSON
+            calculateAUC();
+            calculateAUCPR();
+        }
+        jsonGenerator.writeNumberField("thresholdSteps", getThresholdSteps());
+        jsonGenerator.writeNumberField("countActualPositive", getCountActualPositive());
+        jsonGenerator.writeNumberField("countActualNegative", getCountActualNegative());
+        jsonGenerator.writeObjectField("counts", getCounts());
+        jsonGenerator.writeNumberField("auc", calculateAUC());
+        jsonGenerator.writeNumberField("auprc", calculateAUCPR());
+        if (isExact()) {
+            //Store ROC and PR curves only for exact mode... they are redundant + can be calculated again for thresholded mode
+            jsonGenerator.writeObjectField("rocCurve", getRocCurve());
+            jsonGenerator.writeObjectField("prCurve", getPrecisionRecallCurve());
+        }
+        jsonGenerator.writeBooleanField("isExact", isExact());
+        jsonGenerator.writeNumberField("exampleCount", getExampleCount());
+        jsonGenerator.writeBooleanField("rocRemoveRedundantPts", isRocRemoveRedundantPts());
+    }
+
     @AllArgsConstructor
     @Data
     @NoArgsConstructor
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/serde/ROCArraySerializer.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/serde/ROCArraySerializer.java
index 6687963..e0ace2f 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/serde/ROCArraySerializer.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/serde/ROCArraySerializer.java
@@ -39,7 +39,7 @@ public class ROCArraySerializer extends JsonSerializer<ROC[]> {
         for (ROC r : rocs) {
             jsonGenerator.writeStartObject();
             jsonGenerator.writeStringField("@class", ROC.class.getName());
-            serializer.serialize(r, jsonGenerator, serializerProvider);
+            r.serialize(jsonGenerator, serializerProvider, serializer);
             jsonGenerator.writeEndObject();
         }
         jsonGenerator.writeEndArray();
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/serde/ROCSerializer.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/serde/ROCSerializer.java
index 2364075..50cb442 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/serde/ROCSerializer.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/evaluation/serde/ROCSerializer.java
@@ -33,37 +33,12 @@ import java.io.IOException;
  * @author Alex Black
  */
 public class ROCSerializer extends JsonSerializer<ROC> {
-    @Override
-    public void serialize(ROC roc, JsonGenerator jsonGenerator, SerializerProvider serializerProvider)
-                    throws IOException {
-        if (roc.isExact()) {
-            //For exact ROC implementation: force AUC and AUPRC calculation, so result can be stored in JSON, such
-            //that we have them once deserialized.
-            //Due to potentially huge size, exact mode doesn't store the original predictions in JSON
-            roc.calculateAUC();
-            roc.calculateAUCPR();
-        }
-        jsonGenerator.writeNumberField("thresholdSteps", roc.getThresholdSteps());
-        jsonGenerator.writeNumberField("countActualPositive", roc.getCountActualPositive());
-        jsonGenerator.writeNumberField("countActualNegative", roc.getCountActualNegative());
-        jsonGenerator.writeObjectField("counts", roc.getCounts());
-        jsonGenerator.writeNumberField("auc", roc.calculateAUC());
-        jsonGenerator.writeNumberField("auprc", roc.calculateAUCPR());
-        if (roc.isExact()) {
-            //Store ROC and PR curves only for exact mode... they are redundant + can be calculated again for thresholded mode
-            jsonGenerator.writeObjectField("rocCurve", roc.getRocCurve());
-            jsonGenerator.writeObjectField("prCurve", roc.getPrecisionRecallCurve());
-        }
-        jsonGenerator.writeBooleanField("isExact", roc.isExact());
-        jsonGenerator.writeNumberField("exampleCount", roc.getExampleCount());
-        jsonGenerator.writeBooleanField("rocRemoveRedundantPts", roc.isRocRemoveRedundantPts());
-    }
 
     @Override
     public void serializeWithType(ROC value, JsonGenerator gen, SerializerProvider serializers, TypeSerializer typeSer)
                     throws IOException {
         typeSer.writeTypePrefixForObject(value, gen);
-        serialize(value, gen, serializers);
+        value.serialize(gen, serializers, this);
         typeSer.writeTypeSuffixForObject(value, gen);
     }
 }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/imports/graphmapper/tf/TFGraphMapper.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/imports/graphmapper/tf/TFGraphMapper.java
index a273d18..95a433e 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/imports/graphmapper/tf/TFGraphMapper.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/imports/graphmapper/tf/TFGraphMapper.java
@@ -17,7 +17,6 @@
 package org.nd4j.imports.graphmapper.tf;
 
 import com.github.os72.protobuf351.Message;
-import com.google.common.primitives.Floats;
 import com.google.common.primitives.Ints;
 import lombok.extern.slf4j.Slf4j;
 import lombok.val;
@@ -29,7 +28,6 @@ import org.nd4j.autodiff.samediff.internal.SameDiffOp;
 import org.nd4j.autodiff.samediff.internal.Variable;
 import org.nd4j.base.Preconditions;
 import org.nd4j.imports.converters.DifferentialFunctionClassHolder;
-import org.nd4j.imports.descriptors.properties.AttributeAdapter;
 import org.nd4j.imports.descriptors.properties.PropertyMapping;
 import org.nd4j.imports.descriptors.tensorflow.TensorflowDescriptorParser;
 import org.nd4j.imports.graphmapper.BaseGraphMapper;
@@ -674,206 +672,7 @@ public class TFGraphMapper extends BaseGraphMapper<GraphDef,NodeDef,AttrValue,No
      * @param graph
      */
     public void initFunctionFromProperties(DifferentialFunction on, Map<String, AttrValue> attributesForNode, NodeDef node, GraphDef graph) {
-        initFunctionFromProperties(on.tensorflowName(),on,attributesForNode,node,graph);
-    }
-
-    /**
-     * Init a function's attributes
-     * @param mappedTfName the tensorflow name to pick (sometimes ops have multiple names
-     * @param on the function to map
-     * @param attributesForNode the attributes for the node
-     * @param node
-     * @param graph
-     */
-    public void initFunctionFromProperties(String mappedTfName, DifferentialFunction on, Map<String, AttrValue> attributesForNode, NodeDef node, GraphDef graph) {
-        val properties = on.mappingsForFunction();
-        val tfProperties = properties.get(mappedTfName);
-        val fields = DifferentialFunctionClassHolder.getInstance().getFieldsForFunction(on);
-        val attributeAdapters = on.attributeAdaptersForFunction();
-
-        // if there's no properties announced for this function - just return
-        if (tfProperties == null)
-            return;
-
-        //Can't execute in just any order: sometimes there are dependencies between attribute mappings
-        //For example, conv2d strides depend on data format -> need to map data format before mapping strides
-        //Solution: map nodes without adapters before nodes with adapters. This doesn't guarantee we'll always be
-        // mapping in the right order (for example, we might have adapter(x) depends on adapter(y)) but it should catch most cases
-        Map<String,PropertyMapping> map;
-        if(attributeAdapters == null || !attributeAdapters.containsKey(mappedTfName)) {
-            map = tfProperties;
-        } else {
-            map = new LinkedHashMap<>();
-            for (Map.Entry<String, PropertyMapping> e : tfProperties.entrySet()) {
-                if (!attributeAdapters.get(mappedTfName).containsKey(e.getKey())) {
-                    //No adapter for this attribute
-                    map.put(e.getKey(), e.getValue());
-                }
-            }
-            for (Map.Entry<String, PropertyMapping> e : tfProperties.entrySet()) {
-                if (!map.containsKey(e.getKey())) {
-                    //Not added on first pass -> must have attribute mapper
-                    map.put(e.getKey(), e.getValue());
-                }
-            }
-        }
-
-        for(Map.Entry<String,PropertyMapping> entry : map.entrySet()){
-            val tfAttrName = entry.getValue().getTfAttrName();
-            val currentField = fields.get(entry.getKey());
-
-            AttributeAdapter adapter = null;
-            if(attributeAdapters != null && !attributeAdapters.isEmpty()) {
-                val mappers = attributeAdapters.get(mappedTfName);
-                val adapterFor = mappers.get(entry.getKey());
-                adapter = adapterFor;
-            }
-
-
-            if(tfAttrName != null) {
-                if(currentField == null) {
-                    continue;
-                }
-
-                if(attributesForNode.containsKey(tfAttrName)) {
-                    val attr = attributesForNode.get(tfAttrName);
-                    switch (attr.getValueCase()) {
-                        case B:
-                            if (adapter != null) {
-                                adapter.mapAttributeFor(attr.getB(), currentField, on);
-                            }
-                            break;
-                        case F: break;
-                        case FUNC: break;
-                        case S:
-                            val setString = attr.getS().toStringUtf8();
-                            if(adapter != null) {
-                                adapter.mapAttributeFor(setString,currentField,on);
-                            }
-                            else
-                                on.setValueFor(currentField,setString);
-                            break;
-                        case I:
-                            val setInt = (int) attr.getI();
-                            if(adapter != null) {
-                                adapter.mapAttributeFor(setInt,currentField,on);
-                            }
-                            else
-                                on.setValueFor(currentField,setInt);
-                            break;
-                        case SHAPE:
-                            val shape = attr.getShape().getDimList();
-                            int[] dimsToSet = new int[shape.size()];
-                            for(int i = 0; i < dimsToSet.length; i++) {
-                                dimsToSet[i] = (int) shape.get(i).getSize();
-                            }
-
-                            if(adapter != null) {
-                                adapter.mapAttributeFor(dimsToSet,currentField,on);
-                            }
-
-                            else
-                                on.setValueFor(currentField,dimsToSet);
-                            break;
-                        case VALUE_NOT_SET:break;
-                        case PLACEHOLDER: break;
-                        case LIST:
-                            val setList = attr.getList();
-                            if(!setList.getIList().isEmpty()) {
-                                val intList = Ints.toArray(setList.getIList());
-                                if(adapter != null) {
-                                    adapter.mapAttributeFor(intList,currentField,on);
-                                }
-                                else
-                                    on.setValueFor(currentField,intList);
-                            }
-                            else if(!setList.getBList().isEmpty()) {
-                                break;
-                            }
-                            else if(!setList.getFList().isEmpty()) {
-                                val floats = Floats.toArray(setList.getFList());
-                                if(adapter != null) {
-                                    adapter.mapAttributeFor(floats,currentField,on);
-                                }
-
-                                else
-                                    on.setValueFor(currentField,floats);
-                                break;
-                            }
-                            else if(!setList.getFuncList().isEmpty()) {
-                                break;
-                            }
-                            else if(!setList.getTensorList().isEmpty()) {
-                                break;
-                            }
-                            break;
-                        case TENSOR:
-                            val tensorToGet = TFGraphMapper.getInstance().mapTensorProto(attr.getTensor());
-                            if(adapter != null) {
-                                adapter.mapAttributeFor(tensorToGet,currentField,on);
-                            }
-                            else
-                                on.setValueFor(currentField,tensorToGet);
-                            break;
-                        case TYPE:
-                            if (adapter != null) {
-                                adapter.mapAttributeFor(attr.getType(), currentField, on);
-                            }
-                            break;
-                    }
-                }
-            }
-
-            else if(entry.getValue().getTfInputPosition() != null) {
-
-
-                int position = entry.getValue().getTfInputPosition();
-                if(position < 0) {
-                    position += node.getInputCount();
-                }
-
-                val inputFromNode = TFGraphMapper.getInstance().getNodeWithNameFromGraph(graph,node.getInput(position));
-                INDArray tensor = inputFromNode != null ? TFGraphMapper.getInstance().getNDArrayFromTensor("value",inputFromNode,graph) : null;
-                if(tensor == null) {
-                    tensor = on.getSameDiff().getArrForVarName(getNodeName(node.getInput(position)));
-                }
-
-
-                if(tensor != null) {
-                    //use adapter instead of direct mapping just like above
-                    if(adapter != null) {
-                        adapter.mapAttributeFor(tensor,currentField,on);
-                    }
-                    else {
-                        if(currentField.getType().equals(int[].class)) {
-                            on.setValueFor(currentField,tensor.data().asInt());
-                        }
-                        else if(currentField.getType().equals(double[].class)) {
-                            on.setValueFor(currentField,tensor.data().asDouble());
-
-                        }
-                        else if(currentField.getType().equals(float[].class)) {
-                            on.setValueFor(currentField,tensor.data().asFloat());
-
-                        }
-                        else if(currentField.getType().equals(INDArray.class)) {
-                            on.setValueFor(currentField,tensor);
-                        }
-                        else if(currentField.getType().equals(int.class)) {
-                            on.setValueFor(currentField,tensor.getInt(0));
-                        }
-                        else if(currentField.getType().equals(double.class)) {
-                            on.setValueFor(currentField,tensor.getDouble(0));
-                        }
-                        else if(currentField.getType().equals(float.class)) {
-                            on.setValueFor(currentField,tensor.getFloat(0));
-                        }
-                    }
-                } else {
-                    on.getSameDiff().addPropertyToResolve(on,entry.getKey());
-                }
-            }
-        }
+        on.initFunctionFromProperties(on.tensorflowName(), attributesForNode,node,graph, this);
     }
 
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/activations/Activation.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/activations/Activation.java
index 53c1531..5026ab8 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/activations/Activation.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/activations/Activation.java
@@ -114,52 +114,7 @@ public enum Activation {
      * @see #asSameDiff(SameDiff, SDVariable)
      */
     public SDVariable asSameDiff(SameDiff sd, SDVariable input) {
-        return asSameDiff(null, sd, input);
-    }
-
-    /**
-     * Get the Activation as a SameDiff variable
-     *
-     * @param variableName Variable name
-     * @param sd           SameDiff instance
-     * @param input        Input variable to apply the activation function to
-     * @return SDVariable: output after applying the activation function
-     */
-    public SDVariable asSameDiff(String variableName, SameDiff sd, SDVariable input) {
-        switch (this) {
-            case CUBE:
-                return sd.math().pow(variableName, input, 3.0);
-            case ELU:
-                return sd.nn().elu(variableName, input);
-            case HARDTANH:
-                return sd.nn().hardTanh(variableName, input);
-            case IDENTITY:
-                return sd.identity(variableName, input);
-            case LEAKYRELU:
-                return sd.nn().leakyRelu(variableName, input, 0.0);
-            case RELU:
-                return sd.nn().relu(variableName, input, 0.0);
-            case SIGMOID:
-                return sd.nn().sigmoid(variableName, input);
-            case SOFTMAX:
-                return sd.nn().softmax(variableName, input);
-            case SOFTPLUS:
-                return sd.nn().softplus(variableName, input);
-            case SOFTSIGN:
-                return sd.nn().softsign(variableName, input);
-            case TANH:
-                return sd.math().tanh(variableName, input);
-            case GELU:
-                return sd.nn().gelu(variableName, input);
-            case HARDSIGMOID:
-            case RATIONALTANH:
-            case RRELU:
-            case RECTIFIEDTANH:
-            case SELU:
-            case SWISH:
-            default:
-                throw new UnsupportedOperationException("Activation function not yet supported: " + this);
-        }
+        return sd.asSameDiff(null, input, this);
     }
 
     /**
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/controlflow/Select.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/controlflow/Select.java
index 2a3403a..1314790 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/controlflow/Select.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/controlflow/Select.java
@@ -55,7 +55,7 @@ public class Select extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/controlflow/compat/BaseCompatOp.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/controlflow/compat/BaseCompatOp.java
index 4c0fb2e..4e62ecd 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/controlflow/compat/BaseCompatOp.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/controlflow/compat/BaseCompatOp.java
@@ -53,7 +53,7 @@ public abstract class BaseCompatOp extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode,nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode,nodeDef, graph, TFGraphMapper.getInstance());
     }
 
     @Override
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/image/ResizeBilinear.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/image/ResizeBilinear.java
index a5008eb..2a56189 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/image/ResizeBilinear.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/image/ResizeBilinear.java
@@ -51,7 +51,7 @@ public class ResizeBilinear extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         this.alignCorners = attributesForNode.get("align_corners").getB();
         addArgs();
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/image/ResizeNearestNeighbor.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/image/ResizeNearestNeighbor.java
index fa005e5..08a8f0e 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/image/ResizeNearestNeighbor.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/image/ResizeNearestNeighbor.java
@@ -24,7 +24,6 @@ import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
 
-import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -46,7 +45,7 @@ public class ResizeNearestNeighbor extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
     }
 
     @Override
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/BatchNorm.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/BatchNorm.java
index c03dc91..c002b57 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/BatchNorm.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/BatchNorm.java
@@ -26,7 +26,6 @@ import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.autodiff.samediff.internal.SameDiffOp;
 import org.nd4j.base.Preconditions;
-import org.nd4j.imports.descriptors.properties.PropertyMapping;
 import org.nd4j.imports.graphmapper.onnx.OnnxGraphMapper;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
 import org.nd4j.linalg.api.buffer.DataType;
@@ -41,7 +40,6 @@ import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
 
-import java.lang.reflect.Field;
 import java.util.*;
 
 
@@ -106,7 +104,7 @@ public class BatchNorm extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         //Switch order: TF uses [input, gamma, beta, mean, variance]; libnd4j expects [input, mean, variance, gamma, beta]
         SameDiffOp op = initWith.getOps().get(this.getOwnName());
         List<String> list = op.getInputsToOp();
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv1D.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv1D.java
index 1f19e4d..f65ac2b 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv1D.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv1D.java
@@ -22,7 +22,6 @@ import lombok.NoArgsConstructor;
 import lombok.extern.slf4j.Slf4j;
 import lombok.val;
 import onnx.OnnxProto3;
-import org.nd4j.autodiff.functions.DifferentialFunction;
 import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.base.Preconditions;
@@ -40,7 +39,6 @@ import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.api.ops.DynamicCustomOp;
 import org.nd4j.linalg.api.ops.impl.layers.convolution.config.Conv1DConfig;
-import org.nd4j.linalg.api.ops.impl.layers.convolution.config.Conv2DConfig;
 import org.nd4j.linalg.util.ArrayUtil;
 import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
@@ -115,7 +113,7 @@ public class Conv1D extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv2D.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv2D.java
index c32d8a3..a5ad22e 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv2D.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv2D.java
@@ -112,7 +112,7 @@ public class Conv2D extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv3D.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv3D.java
index 1fe73be..7c36862 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv3D.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/Conv3D.java
@@ -247,7 +247,7 @@ public class Conv3D extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DeConv2DTF.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DeConv2DTF.java
index 34b6905..999b4bb 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DeConv2DTF.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DeConv2DTF.java
@@ -219,7 +219,7 @@ public class DeConv2DTF extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DepthToSpace.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DepthToSpace.java
index 6715f74..50372a7 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DepthToSpace.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DepthToSpace.java
@@ -77,7 +77,7 @@ public class DepthToSpace extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         boolean isNHWC = dataFormat.equals("NHWC");
         addIArgument(blockSize, isNHWC ? 1 : 0);
     }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DepthwiseConv2D.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DepthwiseConv2D.java
index dcad3b8..78a29da 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DepthwiseConv2D.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/DepthwiseConv2D.java
@@ -126,7 +126,7 @@ public class DepthwiseConv2D extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
 
         /*
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/SpaceToDepth.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/SpaceToDepth.java
index 90bdcdb..2a7f8e0 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/SpaceToDepth.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/SpaceToDepth.java
@@ -75,7 +75,7 @@ public class SpaceToDepth extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         boolean isNHWC = dataFormat == null ? true : dataFormat.equals("NHWC");
         addIArgument(blockSize, isNHWC ? 1 : 0);
     }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/config/LocalResponseNormalizationConfig.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/config/LocalResponseNormalizationConfig.java
index 5099003..3c6b761 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/config/LocalResponseNormalizationConfig.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/layers/convolution/config/LocalResponseNormalizationConfig.java
@@ -18,6 +18,9 @@ package org.nd4j.linalg.api.ops.impl.layers.convolution.config;
 
 import lombok.Builder;
 import lombok.Data;
+import org.nd4j.autodiff.functions.DifferentialFunctionFactory;
+import org.nd4j.autodiff.samediff.SDVariable;
+import org.nd4j.linalg.api.ops.impl.layers.convolution.LocalResponseNormalization;
 
 import java.util.LinkedHashMap;
 import java.util.Map;
@@ -38,4 +41,20 @@ public class LocalResponseNormalizationConfig extends BaseConvolutionConfig {
         return ret;
     }
 
+    /**
+     * Local response normalization operation.
+     *
+     * @param input     the inputs to lrn
+     * @param differentialFunctionFactory
+     * @return
+     */
+    public SDVariable localResponseNormalization(SDVariable input, DifferentialFunctionFactory differentialFunctionFactory) {
+        LocalResponseNormalization lrn = LocalResponseNormalization.builder()
+                .inputFunctions(new SDVariable[]{input})
+                .sameDiff(differentialFunctionFactory.sameDiff())
+                .config(this)
+                .build();
+
+        return lrn.outputVariable();
+    }
 }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/loss/SoftmaxCrossEntropyLoss.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/loss/SoftmaxCrossEntropyLoss.java
index 3b12187..1b87c69 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/loss/SoftmaxCrossEntropyLoss.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/loss/SoftmaxCrossEntropyLoss.java
@@ -22,8 +22,6 @@ import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.imports.NoOpNameFoundException;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
-import org.nd4j.linalg.api.ops.DynamicCustomOp;
-import org.nd4j.linalg.api.ops.Op;
 import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
@@ -64,7 +62,7 @@ public class SoftmaxCrossEntropyLoss extends BaseLoss {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/loss/SparseSoftmaxCrossEntropyLossWithLogits.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/loss/SparseSoftmaxCrossEntropyLossWithLogits.java
index 9c385c4..cfa183b 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/loss/SparseSoftmaxCrossEntropyLossWithLogits.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/loss/SparseSoftmaxCrossEntropyLossWithLogits.java
@@ -17,13 +17,11 @@
 package org.nd4j.linalg.api.ops.impl.loss;
 
 import lombok.NoArgsConstructor;
-import lombok.val;
 import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.autodiff.samediff.internal.SameDiffOp;
 import org.nd4j.base.Preconditions;
 import org.nd4j.imports.NoOpNameFoundException;
-import org.nd4j.imports.descriptors.properties.PropertyMapping;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
 import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.ops.DynamicCustomOp;
@@ -55,7 +53,7 @@ public class SparseSoftmaxCrossEntropyLossWithLogits extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         //Switch order: TF uses [logits, labels]; libnd4j expects [labels, logits]
         SameDiffOp op = initWith.getOps().get(this.getOwnName());
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/Moments.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/Moments.java
index 80b7676..d4c673c 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/Moments.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/Moments.java
@@ -64,7 +64,7 @@ public class Moments extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/NormalizeMoments.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/NormalizeMoments.java
index 09f4ac2..970029e 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/NormalizeMoments.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/NormalizeMoments.java
@@ -60,7 +60,7 @@ public class NormalizeMoments extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/TensorMmul.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/TensorMmul.java
index 586a3b8..49a43f8 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/TensorMmul.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/reduce/TensorMmul.java
@@ -16,8 +16,6 @@
 
 package org.nd4j.linalg.api.ops.impl.reduce;
 
-import com.google.common.primitives.Ints;
-import com.google.common.primitives.Longs;
 import lombok.NoArgsConstructor;
 import lombok.val;
 import onnx.OnnxProto3;
@@ -123,101 +121,19 @@ public class TensorMmul extends DynamicCustomOp {
 
         //tensor matrix multiply gradient wrt second variable
         int[] firstPerm = argsort(combine(deletedAxes[0],keep(argsort(sumAxes[1]),sumAxes[0])));
-        SDVariable firstResult = doTensorMmul(i_v1.get(0), rarg(), firstAxes);
+        SDVariable firstResult = i_v1.get(0).doTensorMmul(rarg(), firstAxes, this);
         SDVariable permuted = f().permute(firstResult,firstPerm);
         ret.add(permuted);
 
         //tensor matrix multiply gradient wrt first variable
         int[] secondPerm = argsort(combine(keep(argsort(sumAxes[0]),sumAxes[1]),deletedAxes[1]));
-        SDVariable secondResult = doTensorMmul(i_v1.get(0), larg(), secondAxes);
+        SDVariable secondResult = i_v1.get(0).doTensorMmul(larg(), secondAxes, this);
         SDVariable secondPermuted = f().permute(secondResult,secondPerm);
         ret.add(secondPermuted);
         return ret;
     }
 
 
-
-    private SDVariable doTensorMmul(SDVariable a,
-                                    SDVariable b,
-                                    int[][] axes) {
-
-        int validationLength = Math.min(axes[0].length, axes[1].length);
-        for (int i = 0; i < validationLength; i++) {
-            if (a.getShape()[axes[0][i]] != b.getShape()[axes[1][i]])
-                throw new IllegalArgumentException("Size of the given axes at each dimension must be the same size.");
-            if (axes[0][i] < 0)
-                axes[0][i] += a.getShape().length;
-            if (axes[1][i] < 0)
-                axes[1][i] += b.getShape().length;
-
-        }
-
-        List<Integer> listA = new ArrayList<>();
-        for (int i = 0; i < a.getShape().length; i++) {
-            if (!Ints.contains(axes[0], i))
-                listA.add(i);
-        }
-
-        int[] newAxesA = Ints.concat(Ints.toArray(listA), axes[0]);
-
-
-        List<Integer> listB = new ArrayList<>();
-        for (int i = 0; i < b.getShape().length; i++) {
-            if (!Ints.contains(axes[1], i))
-                listB.add(i);
-        }
-
-        int[] newAxesB = Ints.concat(axes[1], Ints.toArray(listB));
-
-        int n2 = 1;
-        int aLength = Math.min(a.getShape().length, axes[0].length);
-        for (int i = 0; i < aLength; i++) {
-            n2 *= a.getShape()[axes[0][i]];
-        }
-
-        //if listA and listB are empty these do not initialize.
-        //so initializing with {1} which will then get overridden if not empty
-        long[] newShapeA = {-1, n2};
-        long[] oldShapeA;
-        if (listA.size() == 0) {
-            oldShapeA = new long[] {1};
-        } else {
-            oldShapeA = Longs.toArray(listA);
-            for (int i = 0; i < oldShapeA.length; i++)
-                oldShapeA[i] = a.getShape()[(int) oldShapeA[i]];
-        }
-
-        int n3 = 1;
-        int bNax = Math.min(b.getShape().length, axes[1].length);
-        for (int i = 0; i < bNax; i++) {
-            n3 *= b.getShape()[axes[1][i]];
-        }
-
-
-        int[] newShapeB = {n3, -1};
-        long[] oldShapeB;
-        if (listB.size() == 0) {
-            oldShapeB = new long[] {1};
-        } else {
-            oldShapeB = Longs.toArray(listB);
-            for (int i = 0; i < oldShapeB.length; i++)
-                oldShapeB[i] = b.getShape()[(int) oldShapeB[i]];
-        }
-
-
-        SDVariable at = f()
-                .reshape(f().permute
-                        (a,newAxesA),newShapeA);
-        SDVariable bt = f()
-                .reshape(f()
-                        .permute(b,newAxesB),newShapeB);
-
-        SDVariable ret = f().mmul(at,bt);
-        long[] aPlusB = Longs.concat(oldShapeA, oldShapeB);
-        return f().reshape(ret, aPlusB);
-    }
-
-
     public TensorMmul(INDArray x, INDArray y, int[][] axes) {
         super(null,new INDArray[]{x, y},null);
         this.axes = axes;
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterAdd.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterAdd.java
index ad19598..629c6aa 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterAdd.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterAdd.java
@@ -16,7 +16,6 @@
 
 package org.nd4j.linalg.api.ops.impl.scatter;
 
-import lombok.val;
 import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.base.Preconditions;
@@ -63,7 +62,7 @@ public class ScatterAdd extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterDiv.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterDiv.java
index ea7ef3d..309dd60 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterDiv.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterDiv.java
@@ -86,7 +86,7 @@ public class ScatterDiv extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMax.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMax.java
index 33f8db9..e91a591 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMax.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMax.java
@@ -60,7 +60,7 @@ public class ScatterMax extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMin.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMin.java
index 00322b2..de4d0cf 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMin.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMin.java
@@ -60,7 +60,7 @@ public class ScatterMin extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMul.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMul.java
index 1db4263..cad9a1b 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMul.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterMul.java
@@ -62,7 +62,7 @@ public class ScatterMul extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNd.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNd.java
index a589fa1..9145433 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNd.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNd.java
@@ -27,7 +27,6 @@ import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
 
-import java.util.ArrayList;
 import java.util.Collections;
 import java.util.List;
 import java.util.Map;
@@ -67,7 +66,7 @@ public class ScatterNd extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdAdd.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdAdd.java
index 7dd2b94..30331d4 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdAdd.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdAdd.java
@@ -66,7 +66,7 @@ public class ScatterNdAdd extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdSub.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdSub.java
index 42c539f..7965770 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdSub.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdSub.java
@@ -66,7 +66,7 @@ public class ScatterNdSub extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdUpdate.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdUpdate.java
index aeb3c98..52ee337 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdUpdate.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterNdUpdate.java
@@ -66,7 +66,7 @@ public class ScatterNdUpdate extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterSub.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterSub.java
index 375d5bc..26e8459 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterSub.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterSub.java
@@ -79,7 +79,7 @@ public class ScatterSub extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterUpdate.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterUpdate.java
index ccfc541..53027ca 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterUpdate.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/scatter/ScatterUpdate.java
@@ -73,7 +73,7 @@ public class ScatterUpdate extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         if (nodeDef.containsAttr("use_locking")) {
             if (nodeDef.getAttrOrThrow("use_locking").getB() == true) {
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Gather.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Gather.java
index a8bc012..f348f49 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Gather.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Gather.java
@@ -28,7 +28,6 @@ import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.api.ops.DynamicCustomOp;
 import org.nd4j.linalg.factory.Nd4j;
-import org.nd4j.linalg.util.ArrayUtil;
 import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
@@ -73,7 +72,7 @@ public class Gather extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/OneHot.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/OneHot.java
index 2d5dcc6..094420d 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/OneHot.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/OneHot.java
@@ -26,7 +26,6 @@ import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
 import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.api.ops.DynamicCustomOp;
-import org.nd4j.linalg.factory.Nd4j;
 import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
@@ -88,7 +87,7 @@ public class OneHot extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
         if(attributesForNode.containsKey("T")) {
             outputType = TFGraphMapper.convertType(attributesForNode.get("T").getType());
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/ParallelStack.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/ParallelStack.java
index 8d7dcf6..6a00ba3 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/ParallelStack.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/ParallelStack.java
@@ -16,7 +16,6 @@
 
 package org.nd4j.linalg.api.ops.impl.shape;
 
-import lombok.val;
 import onnx.OnnxProto3;
 import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
@@ -64,7 +63,7 @@ public class ParallelStack extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
     }
 
     @Override
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Repeat.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Repeat.java
index 14d67d9..d82f959 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Repeat.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Repeat.java
@@ -21,7 +21,6 @@ import lombok.val;
 import onnx.OnnxProto3;
 import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
-import org.nd4j.base.Preconditions;
 import org.nd4j.imports.descriptors.properties.PropertyMapping;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
 import org.nd4j.linalg.api.buffer.DataType;
@@ -101,7 +100,7 @@ public class Repeat extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addIArgument(jaxis);
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/SequenceMask.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/SequenceMask.java
index 5faa826..a98fa56 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/SequenceMask.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/SequenceMask.java
@@ -71,7 +71,7 @@ public class SequenceMask extends DynamicCustomOp {
             // No 2nd input
             this.is_static_maxlen = true;
         }
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         if (is_static_maxlen) {
             addIArgument(this.maxLen);
         }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Stack.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Stack.java
index 44cb053..457fa7e 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Stack.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/shape/Stack.java
@@ -88,7 +88,7 @@ public class Stack extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/Cholesky.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/Cholesky.java
index 4d321d9..1d9b140 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/Cholesky.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/Cholesky.java
@@ -24,7 +24,6 @@ import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
 
-import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
 
@@ -46,7 +45,7 @@ public class Cholesky extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
     }
 
     @Override
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/NthElement.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/NthElement.java
index 958df55..31fc448 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/NthElement.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/NthElement.java
@@ -47,7 +47,7 @@ public class NthElement extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
 
         this.reverse = attributesForNode.get("reverse").getB();
         addArgs();
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/CumProd.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/CumProd.java
index d1d0176..e76ff64 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/CumProd.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/CumProd.java
@@ -120,7 +120,7 @@ public class CumProd extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/CumSum.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/CumSum.java
index 2b62b73..f400bbb 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/CumSum.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/CumSum.java
@@ -122,7 +122,7 @@ public class CumSum extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/Dilation2D.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/Dilation2D.java
index 79e7931..556ce08 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/Dilation2D.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/Dilation2D.java
@@ -27,7 +27,6 @@ import org.nd4j.imports.descriptors.properties.adapters.*;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.api.ops.DynamicCustomOp;
-import org.nd4j.linalg.util.ArrayUtil;
 import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
@@ -90,7 +89,7 @@ public class Dilation2D extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode,nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode,nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/DynamicPartition.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/DynamicPartition.java
index 06d52f7..f60eb52 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/DynamicPartition.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/DynamicPartition.java
@@ -17,7 +17,6 @@
 package org.nd4j.linalg.api.ops.impl.transforms.custom;
 
 import lombok.val;
-import org.apache.commons.lang3.ArrayUtils;
 import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.imports.descriptors.properties.PropertyMapping;
@@ -74,7 +73,7 @@ public class DynamicPartition extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/MirrorPad.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/MirrorPad.java
index 1e84fa3..dd7d758 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/MirrorPad.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/MirrorPad.java
@@ -22,7 +22,6 @@ import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.base.Preconditions;
 import org.nd4j.imports.descriptors.properties.AttributeAdapter;
 import org.nd4j.imports.descriptors.properties.PropertyMapping;
-import org.nd4j.imports.descriptors.properties.adapters.StringEqualsAdapter;
 import org.nd4j.imports.descriptors.properties.adapters.StringNotEqualsAdapter;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
 import org.nd4j.linalg.api.buffer.DataType;
@@ -43,7 +42,7 @@ public class MirrorPad extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         iArguments.add(isSymmetric ? 1L : 0L);
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/ParallelConcat.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/ParallelConcat.java
index e54a9dc..c0ea120 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/ParallelConcat.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/ParallelConcat.java
@@ -17,12 +17,10 @@
 package org.nd4j.linalg.api.ops.impl.transforms.custom;
 
 import lombok.extern.slf4j.Slf4j;
-import lombok.val;
 import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.base.Preconditions;
 import org.nd4j.imports.descriptors.properties.AttributeAdapter;
 import org.nd4j.imports.descriptors.properties.PropertyMapping;
-import org.nd4j.imports.descriptors.properties.adapters.StringNotEqualsAdapter;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
 import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.ops.DynamicCustomOp;
@@ -42,7 +40,7 @@ public class ParallelConcat extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         // We might want to import everything here? i.e. shape in advance?
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/ReverseSequence.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/ReverseSequence.java
index 0906451..f5a8af1 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/ReverseSequence.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/custom/ReverseSequence.java
@@ -19,7 +19,6 @@ package org.nd4j.linalg.api.ops.impl.transforms.custom;
 import lombok.val;
 import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
-import org.nd4j.base.Preconditions;
 import org.nd4j.imports.NoOpNameFoundException;
 import org.nd4j.imports.descriptors.properties.PropertyMapping;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
@@ -75,7 +74,7 @@ public class ReverseSequence extends DynamicCustomOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArguments();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/dtype/Cast.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/dtype/Cast.java
index 01408ea..01f414d 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/dtype/Cast.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/api/ops/impl/transforms/dtype/Cast.java
@@ -22,21 +22,17 @@ import org.nd4j.autodiff.samediff.SDVariable;
 import org.nd4j.autodiff.samediff.SameDiff;
 import org.nd4j.autodiff.samediff.serde.FlatBuffersMapper;
 import org.nd4j.base.Preconditions;
-import org.nd4j.imports.NoOpNameFoundException;
 import org.nd4j.imports.converters.DifferentialFunctionClassHolder;
 import org.nd4j.imports.descriptors.properties.AttributeAdapter;
 import org.nd4j.imports.descriptors.properties.PropertyMapping;
 import org.nd4j.imports.descriptors.properties.adapters.DataTypeAdapter;
-import org.nd4j.imports.descriptors.properties.adapters.IntArrayIntIndexAdpater;
 import org.nd4j.imports.graphmapper.tf.TFGraphMapper;
-import org.nd4j.linalg.api.buffer.DataBuffer;
 import org.nd4j.linalg.api.buffer.DataType;
 import org.nd4j.linalg.api.ops.impl.transforms.BaseDynamicTransformOp;
 import org.nd4j.linalg.api.shape.LongShapeDescriptor;
 import org.nd4j.linalg.api.shape.Shape;
 import org.nd4j.linalg.api.shape.options.ArrayOptionsHelper;
 import org.nd4j.linalg.api.shape.options.ArrayType;
-import org.nd4j.linalg.exception.ND4JIllegalStateException;
 import org.tensorflow.framework.AttrValue;
 import org.tensorflow.framework.GraphDef;
 import org.tensorflow.framework.NodeDef;
@@ -84,7 +80,7 @@ public class Cast extends BaseDynamicTransformOp {
 
     @Override
     public void initFromTensorFlow(NodeDef nodeDef, SameDiff initWith, Map<String, AttrValue> attributesForNode, GraphDef graph) {
-        TFGraphMapper.getInstance().initFunctionFromProperties(nodeDef.getOp(), this, attributesForNode, nodeDef, graph);
+        initFunctionFromProperties(nodeDef.getOp(), attributesForNode, nodeDef, graph, TFGraphMapper.getInstance());
         addArgs();
     }
 
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/dataset/DataSet.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/dataset/DataSet.java
index d72a71c..7f5eca7 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/dataset/DataSet.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/dataset/DataSet.java
@@ -24,6 +24,7 @@ import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.dataset.api.DataSetUtil;
 import org.nd4j.linalg.dataset.api.MultiDataSet;
 import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
+import org.nd4j.linalg.dataset.api.iterator.StandardScaler;
 import org.nd4j.linalg.dataset.api.preprocessor.NormalizerStandardize;
 import org.nd4j.linalg.factory.Nd4j;
 import org.nd4j.linalg.indexing.BooleanIndexing;
@@ -1378,4 +1379,12 @@ public class DataSet implements org.nd4j.linalg.dataset.api.DataSet {
     }
 
 
+    /**
+     * Transform the data
+     * @param standardScaler
+     */
+    public void transform(StandardScaler standardScaler) {
+        setFeatures(getFeatures().subRowVector(standardScaler.getMean()));
+        setFeatures(getFeatures().divRowVector(standardScaler.getStd()));
+    }
 }
diff --git a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/dataset/api/iterator/StandardScaler.java b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/dataset/api/iterator/StandardScaler.java
index 4014d76..5c850d1 100644
--- a/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/dataset/api/iterator/StandardScaler.java
+++ b/nd4j/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/dataset/api/iterator/StandardScaler.java
@@ -120,15 +120,6 @@ public class StandardScaler {
         Nd4j.saveBinary(this.std, std);
     }
 
-    /**
-     * Transform the data
-     * @param dataSet the dataset to transform
-     */
-    public void transform(DataSet dataSet) {
-        dataSet.setFeatures(dataSet.getFeatures().subRowVector(mean));
-        dataSet.setFeatures(dataSet.getFeatures().divRowVector(std));
-    }
-
 
     public INDArray getMean() {
         return mean;
diff --git a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/BasicContextPool.java b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/BasicContextPool.java
index ca29053..c74d4f6 100644
--- a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/BasicContextPool.java
+++ b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/BasicContextPool.java
@@ -17,12 +17,10 @@
 package org.nd4j.jita.allocator.context.impl;
 
 import lombok.extern.slf4j.Slf4j;
-import lombok.val;
 import org.apache.commons.lang3.RandomUtils;
 import org.bytedeco.javacpp.Pointer;
 import org.nd4j.jita.allocator.context.ContextPack;
 import org.nd4j.jita.allocator.context.ContextPool;
-import org.nd4j.jita.allocator.pointers.CudaPointer;
 import org.nd4j.jita.allocator.pointers.cuda.CUcontext;
 import org.nd4j.jita.allocator.pointers.cuda.cublasHandle_t;
 import org.nd4j.jita.allocator.pointers.cuda.cudaStream_t;
@@ -114,7 +112,7 @@ public class BasicContextPool implements ContextPool {
                     log.debug("Creating new context...");
                     CudaContext context = createNewStream(deviceId);
 
-                    getDeviceBuffers(context, deviceId);
+                    context.getDeviceBuffers(deviceId, this);
 
                     if (contextsForDevices.get(deviceId).size() == 0) {
                         // if we have no contexts created - it's just awesome time to attach cuBLAS handle here
@@ -283,46 +281,6 @@ public class BasicContextPool implements ContextPool {
         return cuPool.get(deviceId);
     }
 
-    /**
-     * This method is used to allocate
-     * @param context
-     * @param deviceId
-     */
-    protected void getDeviceBuffers(CudaContext context, int deviceId) {
-        NativeOps nativeOps = NativeOpsHolder.getInstance().getDeviceNativeOps(); //((CudaExecutioner) Nd4j.getExecutioner()).getNativeOps();
-
-        // we hardcode sizeOf to sizeOf(double)
-        int sizeOf = 8;
-
-        val reductionPointer = nativeOps.mallocDevice(16384 * sizeOf, deviceId, 0);
-        if (reductionPointer == null)
-            throw new IllegalStateException("Can't allocate [DEVICE] reduction buffer memory!");
-
-        nativeOps.memsetAsync(reductionPointer, 0, 16384 * sizeOf, 0, context.getOldStream());
-
-        context.syncOldStream();
-
-        val allocationPointer = nativeOps.mallocDevice(16384 * sizeOf, deviceId, 0);
-        if (allocationPointer == null)
-            throw new IllegalStateException("Can't allocate [DEVICE] allocation buffer memory!");
-
-        val scalarPointer = nativeOps.mallocHost(sizeOf, 0);
-        if (scalarPointer == null)
-            throw new IllegalStateException("Can't allocate [HOST] scalar buffer memory!");
-
-        context.setBufferScalar(scalarPointer);
-        context.setBufferAllocation(allocationPointer);
-        context.setBufferReduction(reductionPointer);
-
-        val specialPointer = nativeOps.mallocDevice(16384 * sizeOf, deviceId, 0);
-        if (specialPointer == null)
-            throw new IllegalStateException("Can't allocate [DEVICE] special buffer memory!");
-
-        nativeOps.memsetAsync(specialPointer, 0, 16384 * sizeOf, 0, context.getOldStream());
-
-        context.setBufferSpecial(specialPointer);
-    }
-
     public ContextPack acquireContextPackForDevice(Integer deviceId) {
         return new ContextPack(acquireContextForDevice(deviceId));
     }
diff --git a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/LimitedContextPool.java b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/LimitedContextPool.java
index 20f8adc..0cbbb54 100644
--- a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/LimitedContextPool.java
+++ b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/LimitedContextPool.java
@@ -16,18 +16,13 @@
 
 package org.nd4j.jita.allocator.context.impl;
 
-import lombok.NonNull;
 import lombok.extern.slf4j.Slf4j;
 import lombok.val;
 import lombok.var;
 import org.apache.commons.lang3.RandomUtils;
 import org.nd4j.jita.allocator.context.ContextPack;
 import org.nd4j.jita.allocator.garbage.DeallocatableThread;
-import org.nd4j.jita.allocator.garbage.GarbageResourceReference;
 import org.nd4j.jita.allocator.impl.AtomicAllocator;
-import org.nd4j.jita.allocator.pointers.CudaPointer;
-import org.nd4j.jita.allocator.pointers.cuda.cublasHandle_t;
-import org.nd4j.jita.allocator.pointers.cuda.cusolverDnHandle_t;
 import org.nd4j.jita.conf.CudaEnvironment;
 import org.nd4j.linalg.api.memory.Deallocatable;
 import org.nd4j.linalg.factory.Nd4j;
@@ -44,7 +39,6 @@ import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.LinkedBlockingQueue;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.locks.LockSupport;
 
 /**
  * @author raver119@gmail.com
@@ -87,7 +81,7 @@ public class LimitedContextPool extends BasicContextPool {
         for (int cnt = 0; cnt < numResources; cnt++) {
             val context = createNewStream(device);
             context.initOldStream();
-            getDeviceBuffers(context, device);
+            context.getDeviceBuffers(device, LimitedContextPool.this);
             context.setHandle(handle);
 
             context.syncOldStream();
@@ -116,7 +110,7 @@ public class LimitedContextPool extends BasicContextPool {
             for (int cnt = 0; cnt < numResources; cnt++) {
                 val context = createNewStream(device);
                 context.initOldStream();
-                getDeviceBuffers(context, device);
+                context.getDeviceBuffers(device, LimitedContextPool.this);
                 context.setHandle(handle);
                 context.setSolverHandle(solverHandle);
 
diff --git a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/PackedContextPool.java b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/PackedContextPool.java
index 575bb61..311f100 100644
--- a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/PackedContextPool.java
+++ b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/context/impl/PackedContextPool.java
@@ -56,7 +56,7 @@ public class PackedContextPool extends BasicContextPool implements ContextPool {
                 for (int c = 0; c < LANES_PER_THREAD; c++) {
                     CudaContext context = createNewStream(deviceId);
 
-                    getDeviceBuffers(context, deviceId);
+                    context.getDeviceBuffers(deviceId, PackedContextPool.this);
 
                     if (cublasPool.get(deviceId) == null) {
                         // if we have no contexts created - it's just awesome time to attach cuBLAS handle here
diff --git a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/pointers/cuda/cudaEvent_t.java b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/pointers/cuda/cudaEvent_t.java
index 1650e08..3582956 100644
--- a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/pointers/cuda/cudaEvent_t.java
+++ b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/pointers/cuda/cudaEvent_t.java
@@ -70,11 +70,4 @@ public class cudaEvent_t extends CudaPointer {
         }
     }
 
-    public void register(cudaStream_t stream) {
-        if (!isDestroyed()) {
-            int res = NativeOpsHolder.getInstance().getDeviceNativeOps().registerEvent(this, stream);
-            if (res == 0)
-                throw new ND4JException("CUDA exception happened. Terminating. Last op: [" + Nd4j.getExecutioner().getLastOp() +"]");
-        }
-    }
 }
diff --git a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/pointers/cuda/cudaStream_t.java b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/pointers/cuda/cudaStream_t.java
index b18ceb2..6064af1 100644
--- a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/pointers/cuda/cudaStream_t.java
+++ b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/jita/allocator/pointers/cuda/cudaStream_t.java
@@ -41,4 +41,12 @@ public class cudaStream_t extends CudaPointer {
 
         return res;
     }
+
+    public void register(cudaEvent_t cudaEvent_t) {
+        if (!org.nd4j.jita.allocator.pointers.cuda.cudaEvent_t.isDestroyed()) {
+            int res = NativeOpsHolder.getInstance().getDeviceNativeOps().registerEvent(cudaEvent_t, this);
+            if (res == 0)
+                throw new ND4JException("CUDA exception happened. Terminating. Last op: [" + Nd4j.getExecutioner().getLastOp() +"]");
+        }
+    }
 }
diff --git a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/linalg/jcublas/context/CudaContext.java b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/linalg/jcublas/context/CudaContext.java
index f1ecfc9..1489744 100644
--- a/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/linalg/jcublas/context/CudaContext.java
+++ b/nd4j/nd4j-backends/nd4j-backend-impls/nd4j-cuda/src/main/java/org/nd4j/linalg/jcublas/context/CudaContext.java
@@ -17,7 +17,9 @@
 package org.nd4j.linalg.jcublas.context;
 
 import lombok.Data;
+import lombok.val;
 import org.bytedeco.javacpp.Pointer;
+import org.nd4j.jita.allocator.context.impl.BasicContextPool;
 import org.nd4j.jita.allocator.garbage.GarbageResourceReference;
 import org.nd4j.jita.allocator.impl.AtomicAllocator;
 import org.nd4j.jita.allocator.pointers.cuda.cublasHandle_t;
@@ -253,4 +255,43 @@ public class CudaContext {
         return context;
     }
 
+    /**
+     * This method is used to allocate
+     * @param deviceId
+     * @param basicContextPool
+     */
+    public void getDeviceBuffers(int deviceId, BasicContextPool basicContextPool) {
+        NativeOps nativeOps = NativeOpsHolder.getInstance().getDeviceNativeOps(); //((CudaExecutioner) Nd4j.getExecutioner()).getNativeOps();
+
+        // we hardcode sizeOf to sizeOf(double)
+        int sizeOf = 8;
+
+        val reductionPointer = nativeOps.mallocDevice(16384 * sizeOf, deviceId, 0);
+        if (reductionPointer == null)
+            throw new IllegalStateException("Can't allocate [DEVICE] reduction buffer memory!");
+
+        nativeOps.memsetAsync(reductionPointer, 0, 16384 * sizeOf, 0, getOldStream());
+
+        syncOldStream();
+
+        val allocationPointer = nativeOps.mallocDevice(16384 * sizeOf, deviceId, 0);
+        if (allocationPointer == null)
+            throw new IllegalStateException("Can't allocate [DEVICE] allocation buffer memory!");
+
+        val scalarPointer = nativeOps.mallocHost(sizeOf, 0);
+        if (scalarPointer == null)
+            throw new IllegalStateException("Can't allocate [HOST] scalar buffer memory!");
+
+        setBufferScalar(scalarPointer);
+        setBufferAllocation(allocationPointer);
+        setBufferReduction(reductionPointer);
+
+        val specialPointer = nativeOps.mallocDevice(16384 * sizeOf, deviceId, 0);
+        if (specialPointer == null)
+            throw new IllegalStateException("Can't allocate [DEVICE] special buffer memory!");
+
+        nativeOps.memsetAsync(specialPointer, 0, 16384 * sizeOf, 0, getOldStream());
+
+        setBufferSpecial(specialPointer);
+    }
 }
diff --git a/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/BaseDataBuffer.java b/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/BaseDataBuffer.java
index 6bb7931..31c4b8d 100644
--- a/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/BaseDataBuffer.java
+++ b/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/BaseDataBuffer.java
@@ -23,7 +23,6 @@ import org.bytedeco.javacpp.*;
 import org.bytedeco.javacpp.indexer.*;
 import org.nd4j.config.ND4JSystemProperties;
 import org.nd4j.linalg.api.buffer.util.AllocUtil;
-import org.nd4j.linalg.api.buffer.util.DataTypeUtil;
 import org.nd4j.linalg.api.memory.MemoryWorkspace;
 import org.nd4j.linalg.primitives.AtomicBoolean;
 import org.nd4j.linalg.primitives.AtomicDouble;
@@ -31,13 +30,11 @@ import org.nd4j.linalg.primitives.Triple;
 import org.nd4j.linalg.util.ArrayUtil;
 
 import java.io.*;
-import java.lang.ref.WeakReference;
 import java.nio.*;
 import java.nio.DoubleBuffer;
 import java.nio.FloatBuffer;
 import java.nio.IntBuffer;
 import java.nio.LongBuffer;
-import java.util.ArrayList;
 import java.util.Collection;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicLong;
@@ -1422,24 +1419,6 @@ public abstract class BaseDataBuffer implements DataBuffer {
         }
     }
 
-    public void putByDestinationType(long i, Number element, DataType globalType) {
-        if (globalType == DataType.INT || type == DataType.INT) {
-            int anElement = element.intValue();
-            put(i, anElement);
-        } else if (globalType == DataType.LONG || type == DataType.LONG) {
-            long anElement = element.longValue();
-            put(i, anElement);
-        } else if (globalType == DataType.FLOAT || globalType == DataType.HALF) {
-            float anElement = element.floatValue();
-            put(i, anElement);
-        } else if (globalType == DataType.DOUBLE) {
-            double anElement = element.doubleValue();
-            put(i, anElement);
-        } else {
-            throw new IllegalStateException("Unknown type: " + globalType);
-        }
-    }
-
     @Override
     public void put(long i, float element) {
         switch (dataType()) {
@@ -1930,12 +1909,12 @@ public abstract class BaseDataBuffer implements DataBuffer {
                 AtomicDouble aDbl = new AtomicDouble();
                 for (long i = 0; i < length(); i++) {
                     aDbl.set(s.readDouble());
-                    putByDestinationType(i, aDbl, thisType);
+                    thisType.putByDestinationType(i, aDbl, this);
                 }
             } else if (sourceType == DataType.FLOAT) {
                 //TODO no AtomicFloat to use here?
                 for (long i = 0; i < length(); i++) {
-                    putByDestinationType(i, s.readFloat(), thisType);
+                    thisType.putByDestinationType(i, s.readFloat(), this);
                 }
             } else if (sourceType == DataType.COMPRESSED) {
                 String compressionAlgorithm = s.readUTF();
@@ -1956,19 +1935,19 @@ public abstract class BaseDataBuffer implements DataBuffer {
                 AtomicInteger aInt = new AtomicInteger();
                 for (long i = 0; i < length(); i++) {
                     aInt.set(s.readShort());
-                    putByDestinationType(i, HalfIndexer.toFloat(aInt.get()), thisType);
+                    thisType.putByDestinationType(i, HalfIndexer.toFloat(aInt.get()), this);
                 }
             } else if (sourceType == DataType.LONG) {
                 AtomicLong aLong = new AtomicLong();
                 for (long i = 0; i < length(); i++) {
                     aLong.set(s.readLong());
-                    putByDestinationType(i, aLong, thisType);
+                    thisType.putByDestinationType(i, aLong, this);
                 }
             } else if (sourceType == DataType.INT ){
                 AtomicInteger aInt = new AtomicInteger();
                 for (long i = 0; i < length(); i++) {
                     aInt.set(s.readInt());
-                    putByDestinationType(i, aInt, thisType);
+                    thisType.putByDestinationType(i, aInt, this);
                 }
             } else {
                 throw new UnsupportedOperationException("Cannot read type: " + sourceType + " to " + thisType);
diff --git a/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/DataType.java b/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/DataType.java
index 8c27d15..cd384ec 100644
--- a/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/DataType.java
+++ b/nd4j/nd4j-buffer/src/main/java/org/nd4j/linalg/api/buffer/DataType.java
@@ -82,4 +82,22 @@ public enum DataType {
                 throw new IllegalStateException("Unknown datatype or no ND4J equivalent datatype exists: " + numpyDtypeName);
         }
     }
+
+    public void putByDestinationType(long i, Number element, BaseDataBuffer baseDataBuffer) {
+        if (this == INT || baseDataBuffer.dataType() == INT) {
+            int anElement = element.intValue();
+            baseDataBuffer.put(i, anElement);
+        } else if (this == LONG || baseDataBuffer.dataType() == LONG) {
+            long anElement = element.longValue();
+            baseDataBuffer.put(i, anElement);
+        } else if (this == FLOAT || this == HALF) {
+            float anElement = element.floatValue();
+            baseDataBuffer.put(i, anElement);
+        } else if (this == DOUBLE) {
+            double anElement = element.doubleValue();
+            baseDataBuffer.put(i, anElement);
+        } else {
+            throw new IllegalStateException("Unknown type: " + this);
+        }
+    }
 }
diff --git a/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/src/main/java/org/nd4j/parameterserver/distributed/VoidParameterServer.java b/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/src/main/java/org/nd4j/parameterserver/distributed/VoidParameterServer.java
index 751fa79..18774b5 100644
--- a/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/src/main/java/org/nd4j/parameterserver/distributed/VoidParameterServer.java
+++ b/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/src/main/java/org/nd4j/parameterserver/distributed/VoidParameterServer.java
@@ -19,7 +19,6 @@ package org.nd4j.parameterserver.distributed;
 import lombok.Getter;
 import lombok.NonNull;
 import lombok.extern.slf4j.Slf4j;
-import org.nd4j.config.ND4JEnvironmentVars;
 import org.nd4j.linalg.primitives.Pair;
 import org.nd4j.linalg.api.ndarray.INDArray;
 import org.nd4j.linalg.exception.ND4JIllegalStateException;
@@ -35,16 +34,13 @@ import org.nd4j.parameterserver.distributed.messages.*;
 import org.nd4j.parameterserver.distributed.messages.requests.*;
 import org.nd4j.parameterserver.distributed.training.TrainingDriver;
 import org.nd4j.parameterserver.distributed.training.impl.SkipGramTrainer;
-import org.nd4j.parameterserver.distributed.transport.MulticastTransport;
 import org.nd4j.parameterserver.distributed.transport.RoutedTransport;
 import org.nd4j.parameterserver.distributed.transport.Transport;
-import org.nd4j.parameterserver.distributed.util.NetworkOrganizer;
 
 import java.net.InterfaceAddress;
 import java.net.NetworkInterface;
 import java.util.*;
 import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.ThreadPoolExecutor;
 import java.util.concurrent.atomic.AtomicBoolean;
@@ -219,7 +215,7 @@ public class VoidParameterServer {
                                     && voidConfiguration.getShardAddresses().get(0).contains("127.0.0.1")) {
                         pair = Pair.create(NodeRole.SHARD, voidConfiguration.getShardAddresses().get(0));
                     } else {
-                        pair = getRole(voidConfiguration, getLocalAddresses());
+                        pair = voidConfiguration.getRole(getLocalAddresses(), this);
                     }
                     nodeRole = pair.getFirst();
 
@@ -328,52 +324,6 @@ public class VoidParameterServer {
     }
 
     /**
-     * This method checks for designated role, according to local IP addresses and configuration passed into method
-     *
-     * @param voidConfiguration
-     * @param localIPs
-     * @return
-     */
-    protected Pair<NodeRole, String> getRole(@NonNull VoidConfiguration voidConfiguration,
-                    @NonNull Collection<String> localIPs) {
-        NodeRole result = NodeRole.CLIENT;
-
-        for (String ip : voidConfiguration.getShardAddresses()) {
-            String cleansed = ip.replaceAll(":.*", "");
-            if (localIPs.contains(cleansed))
-                return Pair.create(NodeRole.SHARD, ip);
-        }
-
-        if (voidConfiguration.getBackupAddresses() != null)
-            for (String ip : voidConfiguration.getBackupAddresses()) {
-                String cleansed = ip.replaceAll(":.*", "");
-                if (localIPs.contains(cleansed))
-                    return Pair.create(NodeRole.BACKUP, ip);
-            }
-
-
-        String sparkIp = null;
-
-
-        if (sparkIp == null && voidConfiguration.getNetworkMask() != null) {
-            NetworkOrganizer organizer = new NetworkOrganizer(voidConfiguration.getNetworkMask());
-            sparkIp = organizer.getMatchingAddress();
-        }
-
-        // last resort here...
-        if (sparkIp == null)
-            sparkIp = System.getenv(ND4JEnvironmentVars.DL4J_VOID_IP);
-
-
-        log.info("Got [{}] as sparkIp", sparkIp);
-        if (sparkIp == null)
-            throw new ND4JIllegalStateException("Can't get IP address for UDP communcation");
-
-        // local IP from pair is used for shard only, so we don't care
-        return Pair.create(result, sparkIp + ":" + voidConfiguration.getUnicastControllerPort());
-    }
-
-    /**
      * This method initiates shutdown sequence for this instance.
      *
      * PLEASE NOTE: This method is blocking for first caller only
diff --git a/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/src/main/java/org/nd4j/parameterserver/distributed/conf/VoidConfiguration.java b/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/src/main/java/org/nd4j/parameterserver/distributed/conf/VoidConfiguration.java
index 535c67b..2e83cc2 100644
--- a/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/src/main/java/org/nd4j/parameterserver/distributed/conf/VoidConfiguration.java
+++ b/nd4j/nd4j-parameter-server-parent/nd4j-parameter-server-node/src/main/java/org/nd4j/parameterserver/distributed/conf/VoidConfiguration.java
@@ -18,17 +18,22 @@ package org.nd4j.parameterserver.distributed.conf;
 
 import lombok.*;
 import lombok.extern.slf4j.Slf4j;
+import org.nd4j.config.ND4JEnvironmentVars;
 import org.nd4j.linalg.exception.ND4JIllegalStateException;
+import org.nd4j.linalg.primitives.Pair;
+import org.nd4j.parameterserver.distributed.VoidParameterServer;
 import org.nd4j.parameterserver.distributed.enums.ExecutionMode;
 import org.nd4j.parameterserver.distributed.enums.FaultToleranceStrategy;
 import org.nd4j.parameterserver.distributed.enums.NodeRole;
 import org.nd4j.parameterserver.distributed.enums.TransportType;
+import org.nd4j.parameterserver.distributed.util.NetworkOrganizer;
 import org.nd4j.parameterserver.distributed.v2.enums.MeshBuildMode;
 import org.nd4j.parameterserver.distributed.v2.transport.PortSupplier;
 import org.nd4j.parameterserver.distributed.v2.transport.impl.StaticPortSupplier;
 
 import java.io.Serializable;
 import java.util.ArrayList;
+import java.util.Collection;
 import java.util.List;
 
 /**
@@ -300,6 +305,51 @@ public class VoidConfiguration implements Serializable {
         this.executionMode = executionMode;
     }
 
+    /**
+     * This method checks for designated role, according to local IP addresses and configuration passed into method
+     *
+     * @param localIPs
+     * @param voidParameterServer
+     * @return
+     */
+    public Pair<NodeRole, String> getRole(@NonNull Collection<String> localIPs, VoidParameterServer voidParameterServer) {
+        NodeRole result = NodeRole.CLIENT;
+
+        for (String ip : getShardAddresses()) {
+            String cleansed = ip.replaceAll(":.*", "");
+            if (localIPs.contains(cleansed))
+                return Pair.create(NodeRole.SHARD, ip);
+        }
+
+        if (getBackupAddresses() != null)
+            for (String ip : getBackupAddresses()) {
+                String cleansed = ip.replaceAll(":.*", "");
+                if (localIPs.contains(cleansed))
+                    return Pair.create(NodeRole.BACKUP, ip);
+            }
+
+
+        String sparkIp = null;
+
+
+        if (sparkIp == null && getNetworkMask() != null) {
+            NetworkOrganizer organizer = new NetworkOrganizer(getNetworkMask());
+            sparkIp = organizer.getMatchingAddress();
+        }
+
+        // last resort here...
+        if (sparkIp == null)
+            sparkIp = System.getenv(ND4JEnvironmentVars.DL4J_VOID_IP);
+
+
+        log.info("Got [{}] as sparkIp", sparkIp);
+        if (sparkIp == null)
+            throw new ND4JIllegalStateException("Can't get IP address for UDP communcation");
+
+        // local IP from pair is used for shard only, so we don't care
+        return Pair.create(result, sparkIp + ":" + getUnicastControllerPort());
+    }
+
     //Partial implementation to hideh/exclude unicastControllerPort from builder - users should use portSupplier method instead
     // in the builder - otherwise, users might think they can set it via this method (instead, it gets overriden by whatever
     // is provided by the port supplier)
